{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Viku/GitHub/ReCoT/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    AutoProcessor,\n",
    "    CLIPVisionModel,\n",
    "    AdamW, \n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoModel,\n",
    "    AutoModelForSeq2SeqLM\n",
    ")\n",
    "import json\n",
    "import logging\n",
    "import faiss\n",
    "from PIL import Image\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed: int = 11):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MISTRAL_API_KEY\"]=\"jJAuJZkjVcy2ynUhan375sHNviHiBeJU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config class to hold hyperparameters\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Base Model\n",
    "        self.model_name = \"bert-base-uncased\"  # can be replaced with any suitable LLM\n",
    "        self.tokenizer_name = \"bert-base-uncased\"\n",
    "        \n",
    "        # Vision Model\n",
    "        self.vision_model_name = \"openai/clip-vit-base-patch32\"\n",
    "        \n",
    "        # ScienceQA Dataset\n",
    "        self.file_path = \"/Users/Viku/Datasets/ScienceQA\"\n",
    "        self.train_path = \"/Users/Viku/Datasets/ScienceQA/train/train.json\"\n",
    "        self.val_path = \"/Users/Viku/Datasets/ScienceQA/val/val.json\"\n",
    "        self.max_seq_length = 512\n",
    "        self.batch_size = 4\n",
    "        \n",
    "        # Training\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 0.01\n",
    "        self.epochs = 3\n",
    "        self.warmup_steps = 100\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.gradient_accumulation_steps = 8\n",
    "        \n",
    "        # RL Training\n",
    "        self.ppo_epochs = 4\n",
    "        self.reward_scale = 0.01\n",
    "        self.clip_param = 0.2\n",
    "        self.value_loss_coef = 0.5\n",
    "        self.entropy_coef = 0.01\n",
    "        \n",
    "        # Transformer Refiner\n",
    "        self.refiner_model_name = \"bert-base-uncased\"  # Can be smaller than main model\n",
    "        self.refiner_learning_rate = 2e-5\n",
    "        self.refiner_weight_decay = 0.01\n",
    "        self.refiner_batch_size = 16\n",
    "        self.refiner_epochs = 2\n",
    "        self.refiner_max_seq_length = 256  # Can be shorter than main model\n",
    "        \n",
    "        # Retrieval\n",
    "        self.retrieval_top_k = 3\n",
    "        self.embedding_dim = 768\n",
    "        \n",
    "        # Reflection\n",
    "        self.reflection_threshold = 0.7\n",
    "        \n",
    "        # Paths\n",
    "        self.output_dir = \"outputs/\"\n",
    "        self.checkpoint_dir = \"checkpoints/\"\n",
    "        self.exemplar_path = \"exemplars.json\"\n",
    "        \n",
    "        # Device\n",
    "        os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "        self.device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "\n",
    "        # Self Training\n",
    "        self.max_answer_length = 64\n",
    "        self.rl_updates = 1000\n",
    "        self.self_training_iterations = 3\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 21:23:15,039 - __main__ - INFO - Initializing BERT model: bert-base-uncased\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "2025-03-10 21:23:17,851 - __main__ - INFO - BERT model initialized successfully\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertLMHeadModel, AutoImageProcessor, BertConfig\n",
    "\n",
    "# Define the initialize_bert function\n",
    "def initialize_bert():\n",
    "    logger.info(f\"Initializing BERT model: {config.model_name}\")\n",
    "    \n",
    "    # First get the original config\n",
    "    bert_config = BertConfig.from_pretrained(config.model_name)\n",
    "    \n",
    "    # Set is_decoder=True\n",
    "    bert_config.is_decoder = True\n",
    "    \n",
    "    # Initialize tokenizer normally\n",
    "    tokenizer = BertTokenizer.from_pretrained(config.tokenizer_name)\n",
    "    \n",
    "    # Initialize model with this modified config\n",
    "    model = BertLMHeadModel.from_pretrained(\n",
    "        config.model_name, \n",
    "        config=bert_config,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    \n",
    "    vision_processor = AutoImageProcessor.from_pretrained(config.vision_model_name)\n",
    "    return model, tokenizer, vision_processor\n",
    "\n",
    "# Now call the function to initialize the models\n",
    "bert_model, bert_tokenizer, vision_processor = initialize_bert()\n",
    "logger.info(f\"BERT model initialized successfully\")\n",
    "\n",
    "# After initializing the model\n",
    "model_path = os.path.join(config.checkpoint_dir, \"bert_decoder\")\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "bert_model.save_pretrained(model_path)\n",
    "bert_tokenizer.save_pretrained(model_path)\n",
    "\n",
    "# Then reload it\n",
    "bert_model = BertLMHeadModel.from_pretrained(model_path)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScienceQADataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, vision_processor, config, is_train=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vision_processor = vision_processor\n",
    "        self.config = config\n",
    "        self.is_train = is_train\n",
    "        self.base_dir = os.path.dirname(file_path)  # Get directory containing the JSON file\n",
    "        self.data = self.load_and_preprocess_data(file_path)\n",
    "        \n",
    "    def load_and_preprocess_data(self, file_path):\n",
    "        logger.info(f\"Loading ScienceQA data from {file_path}\")\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        processed_data = []\n",
    "        for item in data:\n",
    "            # Extract fields specific to ScienceQA\n",
    "            question = item.get('question', '')\n",
    "            context = item.get('context', '')\n",
    "            choices = item.get('choices', [])\n",
    "            answer = item.get('answer', '')\n",
    "            explanation = item.get('explanation', '')\n",
    "            question_id = item.get('id', '')  # Get question ID for image path\n",
    "            \n",
    "            # Format choices as text\n",
    "            choices_text = \"\"\n",
    "            for i, choice in enumerate(choices):\n",
    "                choices_text += f\"({chr(65+i)}) {choice} \"\n",
    "            \n",
    "            # Combine context and question\n",
    "            full_question = f\"Context: {context}\\nQuestion: {question}\\nChoices: {choices_text}\"\n",
    "            \n",
    "            # Split explanation into reasoning steps\n",
    "            steps = self.extract_reasoning_steps(explanation)\n",
    "            \n",
    "            # Process image if available\n",
    "            visual_features = None\n",
    "            if 'image' in item and item['image']:\n",
    "                # Construct image path based on question ID in train/val folder structure\n",
    "                # Assuming question_id corresponds to the folder name\n",
    "                image_folder = os.path.join(self.base_dir, str(question_id))\n",
    "                image_files = [f for f in os.listdir(image_folder) if f.endswith('.png')] if os.path.exists(image_folder) else []\n",
    "                \n",
    "                if image_files:\n",
    "                    image_path = os.path.join(image_folder, image_files[0])\n",
    "                    try:\n",
    "                        image = Image.open(image_path).convert('RGB')\n",
    "                        visual_features = self.process_image(image)\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error processing image {image_path}: {e}\")\n",
    "            \n",
    "            # Tokenize question\n",
    "            question_tokens = self.tokenizer.encode(\n",
    "                \"Let's think step by step! \" + full_question, \n",
    "                add_special_tokens=True,\n",
    "                truncation=True,\n",
    "                max_length=self.config.max_seq_length // 2\n",
    "            )\n",
    "            \n",
    "            # Tokenize each step separately\n",
    "            steps_tokens = []\n",
    "            for step in steps:\n",
    "                step_tokens = self.tokenizer.encode(\n",
    "                    step,\n",
    "                    add_special_tokens=False,\n",
    "                    truncation=True,\n",
    "                    max_length=self.config.max_seq_length // (2 * max(1, len(steps)))\n",
    "                )\n",
    "                steps_tokens.append(step_tokens)\n",
    "            \n",
    "            # Tokenize answer\n",
    "            answer_tokens = self.tokenizer.encode(\n",
    "                f\"Therefore, the answer is {answer}\",\n",
    "                add_special_tokens=False,\n",
    "                truncation=True,\n",
    "                max_length=self.config.max_seq_length // 4\n",
    "            )\n",
    "            \n",
    "            processed_data.append({\n",
    "                'question': full_question,\n",
    "                'question_tokens': question_tokens,\n",
    "                'steps': steps,\n",
    "                'steps_tokens': steps_tokens,\n",
    "                'answer': answer,\n",
    "                'answer_tokens': answer_tokens,\n",
    "                'visual_features': visual_features,\n",
    "                'has_image': visual_features is not None\n",
    "            })\n",
    "        \n",
    "        logger.info(f\"Processed {len(processed_data)} ScienceQA examples\")\n",
    "        return processed_data\n",
    "    \n",
    "    def debug_label_issues(self):\n",
    "        \"\"\"Print detailed debugging info for label issues\"\"\"\n",
    "        # Get a few samples\n",
    "        for idx in range(3):\n",
    "            sample = self.__getitem__(idx)\n",
    "            \n",
    "            # Get data\n",
    "            input_ids = sample['input_ids'].tolist()\n",
    "            attention_mask = sample['attention_mask'].tolist()\n",
    "            labels = sample['labels'].tolist()\n",
    "            \n",
    "            # Count stats\n",
    "            total = len(labels)\n",
    "            non_ignored = sum(1 for l in labels if l != -100)\n",
    "            padded = attention_mask.count(0)\n",
    "            non_padded = attention_mask.count(1)\n",
    "            \n",
    "            print(f\"\\n=== SAMPLE {idx} ===\")\n",
    "            print(f\"Total length: {total}\")\n",
    "            print(f\"Non-padded tokens: {non_padded}\")\n",
    "            print(f\"Padded tokens: {padded}\")\n",
    "            print(f\"Non-ignored labels: {non_ignored}\")\n",
    "            print(f\"Non-ignored percentage: {non_ignored/total*100:.2f}%\")\n",
    "            print(f\"Non-ignored / Non-padded ratio: {non_ignored/max(1,non_padded)*100:.2f}%\")\n",
    "            \n",
    "            # Show a sample of the tokens\n",
    "            print(\"\\nSample tokens (first 10):\")\n",
    "            for i in range(min(10, len(input_ids))):\n",
    "                input_token = self.tokenizer.decode([input_ids[i]])\n",
    "                label_val = labels[i]\n",
    "                label_token = self.tokenizer.decode([label_val]) if label_val != -100 else \"IGNORED\"\n",
    "                mask = attention_mask[i]\n",
    "                \n",
    "                print(f\"Pos {i}: Input='{input_token}' | Label='{label_token}' | Mask={mask}\")\n",
    "            \n",
    "            # Print stats on where ignored labels are\n",
    "            print(\"\\nIgnored label positions:\")\n",
    "            ignored_positions = [i for i, l in enumerate(labels) if l == -100]\n",
    "            if len(ignored_positions) > 20:\n",
    "                print(f\"{ignored_positions[:10]} ... {ignored_positions[-10:]}\")\n",
    "            else:\n",
    "                print(ignored_positions)\n",
    "    \n",
    "    def process_image(self, image):\n",
    "        \"\"\"Process image using CLIP vision encoder\"\"\"\n",
    "        print(\"Processing Image\")\n",
    "        inputs = self.vision_processor(images=image, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            vision_model = CLIPVisionModel.from_pretrained(self.config.vision_model_name)\n",
    "            vision_model.to(self.config.device)\n",
    "            outputs = vision_model(**{k: v.to(self.config.device) for k, v in inputs.items()})\n",
    "            visual_features = outputs.pooler_output.cpu().numpy()\n",
    "        return visual_features[0]  # Return the feature vector\n",
    "    \n",
    "    def extract_reasoning_steps(self, explanation):\n",
    "        \"\"\"Extract reasoning steps from explanation\"\"\"\n",
    "        # Method 1: Split by numbered steps if present\n",
    "        numbered_pattern = re.compile(r'\\d+\\.\\s+')\n",
    "        if numbered_pattern.search(explanation):\n",
    "            steps = [step.strip() for step in numbered_pattern.split(explanation) if step.strip()]\n",
    "            if steps and not steps[0][0].isdigit():  # Remove introduction if it doesn't start with a number\n",
    "                steps = steps[1:]\n",
    "            return steps or [explanation]\n",
    "        \n",
    "        # Method 2: Split by sentences assuming each sentence is a step\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', explanation)\n",
    "        if len(sentences) > 1:\n",
    "            return [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        # Default: Treat the whole explanation as one step\n",
    "        return [explanation]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Combine question, steps, and answer tokens for input\n",
    "        input_tokens = item['question_tokens'].copy()\n",
    "        for step_tokens in item['steps_tokens']:\n",
    "            input_tokens.extend(step_tokens)\n",
    "        input_tokens.extend(item['answer_tokens'])\n",
    "        \n",
    "        # Ensure we stay within the max sequence length\n",
    "        if len(input_tokens) > self.config.max_seq_length:\n",
    "            input_tokens = input_tokens[:self.config.max_seq_length]\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = [1] * len(input_tokens)\n",
    "        padding_length = self.config.max_seq_length - len(input_tokens)\n",
    "        input_tokens.extend([self.tokenizer.pad_token_id] * padding_length)\n",
    "        attention_mask.extend([0] * padding_length)\n",
    "        \n",
    "        # Create shifted labels for causal language modeling\n",
    "        labels = input_tokens.copy()\n",
    "        labels = [-100] + labels[:-1]  # Shift right by one position\n",
    "        \n",
    "        # Make sure padding tokens are ignored in loss\n",
    "        for i in range(len(attention_mask)):\n",
    "            if attention_mask[i] == 0:\n",
    "                labels[i] = -100\n",
    "        \n",
    "        # IMPROVED: Focus on reasoning steps and answer but keep more labels\n",
    "        # The current approach masks too many tokens\n",
    "        question_length = len(item['question_tokens'])\n",
    "        \n",
    "        # Instead of masking middle of question, keep most tokens but selectively mask\n",
    "        # only a portion of them to maintain context while focusing training\n",
    "        if question_length > 20:  # Only if question is long enough\n",
    "            # Mask every other token in the question part after the first few tokens\n",
    "            # This preserves context while reducing focus on the question\n",
    "            for i in range(6, question_length, 2):  # Start after a few tokens, skip every other one\n",
    "                if i < self.config.max_seq_length:\n",
    "                    labels[i] = -100\n",
    "        \n",
    "        # Keep all reasoning steps and answer tokens for training\n",
    "        # They're the most important parts we want the model to learn\n",
    "        \n",
    "        # Calculate percentage of non-ignored labels for monitoring\n",
    "        non_ignored = sum(1 for l in labels if l != -100)\n",
    "        total = len(labels)\n",
    "        non_ignored_percentage = (non_ignored / total) * 100\n",
    "        \n",
    "        # Print for a few samples during debugging\n",
    "        if idx < 3 and self.is_train:\n",
    "            print(f\"Sample {idx}: Non-ignored labels: {non_ignored}/{total} ({non_ignored_percentage:.2f}%)\")\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_tokens, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long),\n",
    "            'question': item['question'],\n",
    "            'steps': item['steps'],\n",
    "            'answer': item['answer'],\n",
    "            'visual_features': torch.tensor(item['visual_features'], dtype=torch.float) if item['visual_features'] is not None else torch.zeros(768),\n",
    "            'has_image': item['has_image']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Base LLM & Chain-of-Thought Generator\n",
    "\n",
    "class ChainOfThoughtGenerator(nn.Module):\n",
    "    def __init__(self, config, model=None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # First, initialize the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n",
    "        \n",
    "        # Then check for special tokens\n",
    "        special_tokens = {\"pad_token\": \"[PAD]\"} if self.tokenizer.pad_token is None else {}\n",
    "        \n",
    "        # Now initialize the model\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "        else:\n",
    "            # Initialize model from config\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(config.model_name)\n",
    "        \n",
    "        # Add special tokens if needed\n",
    "        if special_tokens:\n",
    "            self.tokenizer.add_special_tokens(special_tokens)\n",
    "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "        # The rest of the code remains the same\n",
    "        # Vision encoder for multimodal inputs\n",
    "        self.vision_processor = AutoProcessor.from_pretrained(config.vision_model_name)\n",
    "        self.vision_model = CLIPVisionModel.from_pretrained(config.vision_model_name)\n",
    "        \n",
    "        # Vision-language integration layer\n",
    "        self.vision_projection = nn.Linear(\n",
    "            self.vision_model.config.hidden_size,\n",
    "            self.model.config.hidden_size\n",
    "        )\n",
    "        \n",
    "        # Move models to device\n",
    "        self.model.to(config.device)\n",
    "        self.vision_model.to(config.device)\n",
    "        self.vision_projection.to(config.device)\n",
    "\n",
    "        print(\"ChainOfThoughtGenerator initialized with:\")\n",
    "        print(f\"  Tokenizer: {config.tokenizer_name}\")\n",
    "        print(f\"  Model: {config.model_name}\")\n",
    "        print(f\"  Vision Model: {config.vision_model_name}\")\n",
    "        print(f\"  Device: {config.device}\")\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        \"\"\"Encode image using vision model\"\"\"\n",
    "        print(\"Encoding image...\")\n",
    "        vision_inputs = self.vision_processor(images=image, return_tensors=\"pt\")\n",
    "        vision_inputs = {k: v.to(self.config.device) for k, v in vision_inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            vision_outputs = self.vision_model(**vision_inputs)\n",
    "            image_features = vision_outputs.pooler_output\n",
    "            projected_features = self.vision_projection(image_features)\n",
    "        \n",
    "        print(\"Image encoding completed.\")\n",
    "        return projected_features\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None, visual_features=None):\n",
    "        # \"\"\"Forward pass with optional visual features\"\"\"\n",
    "        # print(\"Forward pass started.\")\n",
    "        # print(f\"  Input IDs: {input_ids.shape}\")\n",
    "        # print(f\"  Attention Mask: {attention_mask.shape}\")\n",
    "        # if labels is not None:\n",
    "        #     print(f\"  Labels: {labels.shape}\")\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        # print(\"Model forward pass completed.\")\n",
    "\n",
    "        # If visual features are available, enhance the hidden states\n",
    "        if visual_features is not None:\n",
    "            # print(\"Processing visual features...\")\n",
    "            if hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:\n",
    "                projected_visual = self.vision_projection(visual_features)\n",
    "                last_hidden = outputs.hidden_states[-1]\n",
    "                enhanced_hidden = last_hidden + projected_visual.unsqueeze(1)\n",
    "                # print(\"Visual features integrated into hidden states.\")\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def generate_step_by_step(self, question, image=None, retrieved_context=None, num_steps=5, max_length=512):\n",
    "        \"\"\"Generate a chain-of-thought reasoning process for a given question\"\"\"\n",
    "\n",
    "        context_str = \"\"\n",
    "        if retrieved_context:\n",
    "            context_str = \"Relevant information:\\n\" + retrieved_context + \"\\n\\n\"\n",
    "        \n",
    "        # Improved prompt with better structure and guidance\n",
    "        prompt = f\"\"\"Question: {question}\n",
    "\n",
    "    To solve this problem, I'll use my knowledge of science to think step by step.\n",
    "\n",
    "    {retrieved_context if retrieved_context else \"\"}\n",
    "\n",
    "    Let me reason through this carefully:\"\"\"\n",
    "\n",
    "        # For more controlled step generation\n",
    "        steps = []\n",
    "        current_prompt = prompt\n",
    "        for i in range(num_steps):\n",
    "            step_prompt = current_prompt\n",
    "            if i > 0:\n",
    "                step_prompt += f\"\\nStep {i+1}:\"\n",
    "            \n",
    "            inputs = self.tokenizer(step_prompt, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(self.config.device) for k, v in inputs.items()}\n",
    "            \n",
    "            step_output = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=len(inputs[\"input_ids\"][0]) + 50,  # Generate just the next step\n",
    "                temperature=0.4,\n",
    "                do_sample=True\n",
    "            )\n",
    "            \n",
    "            step_text = self.tokenizer.decode(step_output[0][len(inputs[\"input_ids\"][0]):], skip_special_tokens=True)\n",
    "            current_prompt += step_text\n",
    "            steps.append(step_text.strip())\n",
    "            \n",
    "            # Check if this step contains an answer conclusion\n",
    "            if any(indicator in step_text for indicator in [\"Therefore\", \"Thus\", \"So the answer\", \"The answer is\"]):\n",
    "                break\n",
    "        \n",
    "        print(f\"Tokenized input: {inputs}\")\n",
    "\n",
    "        # Process image if provided\n",
    "        visual_embedding = None\n",
    "        if image is not None:\n",
    "            print(\"Processing image for reasoning...\")\n",
    "            visual_embedding = self.encode_image(image)\n",
    "\n",
    "        # Generate reasoning steps and answer\n",
    "        with torch.no_grad():\n",
    "            print(\"Generating response from model...\")\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.3,  # Lower temperature for more focused reasoning\n",
    "                do_sample=True,\n",
    "                top_p=0.9,        # Nucleus sampling for better quality\n",
    "                no_repeat_ngram_size=2,  # Prevent repetitive text\n",
    "                num_beams=3       # Use beam search for more coherent outputs\n",
    "            )\n",
    "        \n",
    "        # Decode the generated text\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Generated text: {generated_text}\")\n",
    "\n",
    "        # More robust parsing approach\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        reasoning_part = generated_text[len(prompt):].strip()\n",
    "\n",
    "        # Look for various answer indicators\n",
    "        answer_indicators = [\n",
    "            \"\\nTherefore, the answer is\", \n",
    "            \"\\nSo the answer is\", \n",
    "            \"\\nThus, the answer is\",\n",
    "            \"\\nThe answer is\",\n",
    "            \"\\nIn conclusion,\"\n",
    "        ]\n",
    "\n",
    "        answer = \"\"\n",
    "        steps = []\n",
    "\n",
    "        for indicator in answer_indicators:\n",
    "            if indicator in reasoning_part:\n",
    "                parts = reasoning_part.split(indicator, 1)\n",
    "                reasoning = parts[0].strip()\n",
    "                answer = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "                \n",
    "                # Extract steps - handle numbered or bullet points\n",
    "                step_lines = reasoning.split(\"\\n\")\n",
    "                for line in step_lines:\n",
    "                    clean_line = re.sub(r\"^\\d+\\.\\s*\", \"\", line.strip())  # Remove numbering\n",
    "                    if clean_line:\n",
    "                        steps.append(clean_line)\n",
    "                break\n",
    "        else:\n",
    "            # If no indicator found, try to extract steps anyway\n",
    "            step_lines = reasoning_part.split(\"\\n\")\n",
    "            for line in step_lines:\n",
    "                if line.strip():\n",
    "                    steps.append(line.strip())\n",
    "\n",
    "        print(\"Generated reasoning steps:\")\n",
    "        for i, step in enumerate(steps):\n",
    "            print(f\"  Step {i+1}: {step}\")\n",
    "\n",
    "        print(f\"Final answer: {answer.strip()}\")\n",
    "\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"steps\": steps,\n",
    "            \"answer\": answer.strip(),\n",
    "            \"full_text\": generated_text\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Reflection Module\n",
    "\n",
    "class ReflectionModule(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Use a smaller model for efficiency\n",
    "        self.encoder = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "        # Scoring layers\n",
    "        self.coherence_scorer = nn.Linear(self.encoder.config.hidden_size, 1)\n",
    "        self.language_scorer = nn.Linear(self.encoder.config.hidden_size, 1)\n",
    "        self.progress_scorer = nn.Linear(self.encoder.config.hidden_size, 1)\n",
    "        \n",
    "        # Tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "        # Move to device\n",
    "        self.encoder.to(config.device)\n",
    "        self.coherence_scorer.to(config.device)\n",
    "        self.language_scorer.to(config.device)\n",
    "        self.progress_scorer.to(config.device)\n",
    "    \n",
    "    def forward(self, question, steps, previous_steps=None):\n",
    "        \"\"\"\n",
    "        Evaluate the quality of reasoning steps\n",
    "        \n",
    "        Args:\n",
    "            question: The original question\n",
    "            steps: List of reasoning steps to evaluate\n",
    "            previous_steps: Optional previous steps for context\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of scores for each step and overall\n",
    "        \"\"\"\n",
    "        all_scores = []\n",
    "        \n",
    "        # Process each step\n",
    "        for i, step in enumerate(steps):\n",
    "            # Create context from question and previous steps\n",
    "            context = question\n",
    "            if previous_steps:\n",
    "                context += \" \" + \" \".join(previous_steps)\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = self.tokenizer(\n",
    "                context, \n",
    "                step, \n",
    "                truncation=True, \n",
    "                padding=True, \n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            inputs = {k: v.to(self.config.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Get embeddings\n",
    "            with torch.no_grad():\n",
    "                outputs = self.encoder(**inputs)\n",
    "                pooled_output = outputs.last_hidden_state[:, 0]  # Use [CLS] token\n",
    "            \n",
    "            # Calculate scores\n",
    "            coherence_score = torch.sigmoid(self.coherence_scorer(pooled_output)).item()\n",
    "            language_score = torch.sigmoid(self.language_scorer(pooled_output)).item()\n",
    "            progress_score = torch.sigmoid(self.progress_scorer(pooled_output)).item()\n",
    "            \n",
    "            # Calculate a combined score\n",
    "            combined_score = (coherence_score + language_score + progress_score) / 3\n",
    "            \n",
    "            all_scores.append({\n",
    "                'step': i+1,\n",
    "                'coherence': coherence_score,\n",
    "                'language': language_score,\n",
    "                'progress': progress_score,\n",
    "                'combined': combined_score\n",
    "            })\n",
    "            \n",
    "            # Update previous steps for next iteration\n",
    "            previous_steps = (previous_steps or []) + [step]\n",
    "        \n",
    "        # Calculate overall score\n",
    "        overall_score = sum(s['combined'] for s in all_scores) / len(all_scores) if all_scores else 0\n",
    "        \n",
    "        return {\n",
    "            'step_scores': all_scores,\n",
    "            'overall_score': overall_score\n",
    "        }\n",
    "    \n",
    "    def evaluate_reasoning(self, question, steps, answer=None):\n",
    "        \"\"\"Evaluate the overall reasoning process\"\"\"\n",
    "        step_scores = self.forward(question, steps)\n",
    "        \n",
    "        # Check if reasoning meets the threshold\n",
    "        meets_threshold = step_scores['overall_score'] >= self.config.reflection_threshold\n",
    "        \n",
    "        return {\n",
    "            'scores': step_scores,\n",
    "            'meets_threshold': meets_threshold,\n",
    "            'feedback': self.generate_feedback(step_scores) if not meets_threshold else None\n",
    "        }\n",
    "    \n",
    "    def generate_feedback(self, scores):\n",
    "        \"\"\"Generate feedback based on scores\"\"\"\n",
    "        feedback = []\n",
    "        \n",
    "        for step_score in scores['step_scores']:\n",
    "            step_num = step_score['step']\n",
    "            if step_score['coherence'] < 0.6:\n",
    "                feedback.append(f\"Step {step_num} lacks coherence with the context.\")\n",
    "            if step_score['language'] < 0.6:\n",
    "                feedback.append(f\"Step {step_num} has language issues.\")\n",
    "            if step_score['progress'] < 0.6:\n",
    "                feedback.append(f\"Step {step_num} doesn't make sufficient progress toward the answer.\")\n",
    "        \n",
    "        if not feedback:\n",
    "            feedback = [\"The reasoning needs improvement, but specific issues weren't identified.\"]\n",
    "        \n",
    "        return feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Retrieval Module\n",
    "\n",
    "class RetrievalModule:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.encoder = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        \n",
    "        # Move to device\n",
    "        self.encoder.to(config.device)\n",
    "        \n",
    "        # Load exemplars\n",
    "        self.exemplars = self.load_exemplars()\n",
    "        \n",
    "        # Build index for fast retrieval\n",
    "        self.index = self.build_index()\n",
    "    \n",
    "    def load_exemplars(self):\n",
    "        \"\"\"Load exemplar reasoning sequences\"\"\"\n",
    "        if not os.path.exists(self.config.exemplar_path):\n",
    "            logger.warning(f\"Exemplar file {self.config.exemplar_path} not found, using empty exemplars\")\n",
    "            return []\n",
    "        \n",
    "        with open(self.config.exemplar_path, 'r') as f:\n",
    "            exemplars = json.load(f)\n",
    "        \n",
    "        # Pre-compute embeddings for each exemplar\n",
    "        for exemplar in exemplars:\n",
    "            exemplar['embedding'] = self.encode_text(exemplar['question']).cpu().numpy()\n",
    "        \n",
    "        logger.info(f\"Loaded {len(exemplars)} exemplars\")\n",
    "        return exemplars\n",
    "    \n",
    "    def build_index(self):\n",
    "        \"\"\"Build FAISS index for fast retrieval\"\"\"\n",
    "        if not self.exemplars:\n",
    "            return None\n",
    "        \n",
    "        # Extract embeddings\n",
    "        embeddings = np.array([ex['embedding'] for ex in self.exemplars]).astype('float32')\n",
    "        \n",
    "        # Build index\n",
    "        index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        index.add(embeddings)\n",
    "        \n",
    "        return index\n",
    "    \n",
    "    def encode_text(self, text):\n",
    "        \"\"\"Encode text using the sentence transformer\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            text, \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {k: v.to(self.config.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.encoder(**inputs)\n",
    "            # Use mean pooling\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            mask = attention_mask.unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
    "            masked_embeddings = outputs.last_hidden_state * mask\n",
    "            summed = torch.sum(masked_embeddings, 1)\n",
    "            counts = torch.clamp(torch.sum(mask, 1), min=1e-9)\n",
    "            mean_pooled = summed / counts\n",
    "        \n",
    "        return mean_pooled[0]  # Return the embedding vector\n",
    "    \n",
    "    def retrieve_similar_examples(self, question, k=None):\n",
    "        \"\"\"Retrieve similar exemplars for a given question\"\"\"\n",
    "        if k is None:\n",
    "            k = self.config.retrieval_top_k\n",
    "        \n",
    "        if not self.exemplars or self.index is None:\n",
    "            return []\n",
    "        \n",
    "        # Encode the query\n",
    "        query_embedding = self.encode_text(question).cpu().numpy().reshape(1, -1).astype('float32')\n",
    "        \n",
    "        # Search for similar examples\n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        \n",
    "        # Get the exemplars\n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if idx < len(self.exemplars):\n",
    "                exemplar = self.exemplars[idx].copy()\n",
    "                exemplar['similarity'] = float(1.0 / (1.0 + distances[0][i]))  # Convert distance to similarity\n",
    "                exemplar.pop('embedding', None)  # Remove embedding from result\n",
    "                results.append(exemplar)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_demonstration_prompt(self, question, k=None):\n",
    "        \"\"\"Get a few-shot demonstration prompt based on retrieved examples\"\"\"\n",
    "        examples = self.retrieve_similar_examples(question, k)\n",
    "        \n",
    "        if not examples:\n",
    "            return f\"Let's think step by step! {question}\"\n",
    "        \n",
    "        prompt = \"I'll solve some similar problems step by step, then answer your question.\\n\\n\"\n",
    "        \n",
    "        # Add examples\n",
    "        for i, example in enumerate(examples):\n",
    "            prompt += f\"Example {i+1}:\\n\"\n",
    "            prompt += f\"Question: {example['question']}\\n\"\n",
    "            prompt += \"Reasoning:\\n\"\n",
    "            for j, step in enumerate(example['steps']):\n",
    "                prompt += f\"{j+1}. {step}\\n\"\n",
    "            prompt += f\"Answer: {example['answer']}\\n\\n\"\n",
    "        \n",
    "        # Add the current question\n",
    "        prompt += f\"Now, let's solve your question step by step!\\n{question}\\n\"\n",
    "        \n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Transformer module\n",
    "\n",
    "class SimpleTransformerRefiner:\n",
    "    \"\"\"A lightweight transformer-based model for refining reasoning steps\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Check for cached model path\n",
    "        cached_model_path = os.path.join(config.checkpoint_dir, \"cached_t5\")\n",
    "        \n",
    "        if os.path.exists(cached_model_path):\n",
    "            print(f\"Loading model from cache: {cached_model_path}\")\n",
    "            self.model = AutoModelForSeq2SeqLM.from_pretrained(cached_model_path)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(cached_model_path)\n",
    "        else:\n",
    "            # Try loading with retries and fallback\n",
    "            max_retries = 3\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    # Primary choice\n",
    "                    print(f\"Downloading t5-base (attempt {attempt+1}/{max_retries})...\")\n",
    "                    self.model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")  \n",
    "                    self.tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "                    \n",
    "                    # Cache the model\n",
    "                    os.makedirs(cached_model_path, exist_ok=True)\n",
    "                    self.model.save_pretrained(cached_model_path)\n",
    "                    self.tokenizer.save_pretrained(cached_model_path)\n",
    "                    print(f\"Model cached to: {cached_model_path}\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"Attempt {attempt+1}/{max_retries} failed: {str(e)}\")\n",
    "                    if attempt == max_retries - 1:\n",
    "                        # Fallback to smaller model\n",
    "                        print(\"Falling back to t5-small due to download issues\")\n",
    "                        self.model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "                        self.tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "                        \n",
    "                        # Cache the fallback model\n",
    "                        os.makedirs(cached_model_path, exist_ok=True)\n",
    "                        self.model.save_pretrained(cached_model_path)\n",
    "                        self.tokenizer.save_pretrained(cached_model_path)\n",
    "        \n",
    "        # Move model to device\n",
    "        self.model.to(config.device)\n",
    "        \n",
    "    def train(self, train_examples, epochs=3, batch_size=8):\n",
    "        \"\"\"Train the refiner model on examples with better handling\"\"\"\n",
    "        if not train_examples or len(train_examples) == 0:\n",
    "            print(\"Warning: No training examples provided, skipping training\")\n",
    "            return\n",
    "            \n",
    "        # Prepare optimizer with learning rate warmup\n",
    "        optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.refiner_learning_rate or 2e-5,\n",
    "            weight_decay=self.config.refiner_weight_decay or 0.01\n",
    "        )\n",
    "        \n",
    "        # Prepare improved data\n",
    "        train_inputs = []\n",
    "        train_targets = []\n",
    "        \n",
    "        print(f\"Processing {len(train_examples)} training examples...\")\n",
    "        for example in train_examples:\n",
    "            # Format context from question and previous steps\n",
    "            context = f\"Question: {example['context']} Original step: {example['step']}\"\n",
    "            \n",
    "            # Create improved version\n",
    "            improved_step = self._create_improved_step(example['step'])\n",
    "            \n",
    "            # Only add if there's a meaningful difference\n",
    "            if improved_step != example['step']:\n",
    "                train_inputs.append(f\"refine: {context}\")\n",
    "                train_targets.append(improved_step)\n",
    "        \n",
    "        if len(train_inputs) == 0:\n",
    "            print(\"Warning: No meaningful improvements found in training data\")\n",
    "            return\n",
    "            \n",
    "        print(f\"Created {len(train_inputs)} training examples with meaningful improvements\")\n",
    "        \n",
    "        # Convert to dataset\n",
    "        dataset = self._prepare_dataset(train_inputs, train_targets)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Training loop with better progress tracking\n",
    "        self.model.train()\n",
    "        total_steps = len(dataloader) * epochs\n",
    "        \n",
    "        # Create scheduler with warmup\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=int(0.1 * total_steps),\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        # Main training loop\n",
    "        print(f\"Starting training for {epochs} epochs...\")\n",
    "        best_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            # Training with progress bar\n",
    "            pbar = tqdm(dataloader, desc=f\"Training refiner (Epoch {epoch+1}/{epochs})\")\n",
    "            for batch in pbar:\n",
    "                # Skip empty batches\n",
    "                if any(k not in batch for k in [\"input_ids\", \"attention_mask\", \"labels\"]):\n",
    "                    continue\n",
    "                    \n",
    "                # Get batch\n",
    "                input_ids = batch[\"input_ids\"].to(self.config.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.config.device)\n",
    "                labels = batch[\"labels\"].to(self.config.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Update metrics\n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "                \n",
    "            # Log epoch stats\n",
    "            avg_loss = epoch_loss / len(dataloader)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}: Average Loss = {avg_loss:.4f}\")\n",
    "            \n",
    "            # Save if best\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                print(f\"New best loss: {best_loss:.4f}\")\n",
    "    \n",
    "    def _create_improved_step(self, original_step):\n",
    "        \"\"\"Create a more meaningful improved version of the reasoning step\"\"\"\n",
    "        # Initialize with original\n",
    "        improved = original_step\n",
    "\n",
    "        # 1. Replace vague language with precise language\n",
    "        vague_terms = {\n",
    "            \"I think\": \"We can determine that\",\n",
    "            \"maybe\": \"therefore\",\n",
    "            \"probably\": \"based on the evidence\",\n",
    "            \"might be\": \"is\",\n",
    "            \"could be\": \"is determined to be\",\n",
    "            \"seems like\": \"is evidently\",\n",
    "            \"it appears\": \"it is clear that\",\n",
    "        }\n",
    "        \n",
    "        for vague, precise in vague_terms.items():\n",
    "            if vague in improved.lower():\n",
    "                improved = improved.replace(vague, precise)\n",
    "        \n",
    "        # 2. Add logical connectors if missing\n",
    "        if not any(word in improved.lower() for word in \n",
    "                [\"therefore\", \"thus\", \"because\", \"since\", \"consequently\"]):\n",
    "            # Add logical connectors based on sentence structure\n",
    "            if improved.startswith((\"The\", \"This\", \"It\", \"We\")):\n",
    "                improved = f\"Therefore, {improved}\"\n",
    "            else:\n",
    "                improved = f\"This means that {improved}\"\n",
    "        \n",
    "        # 3. Expand abbreviated reasoning\n",
    "        if len(improved.split()) < 10:\n",
    "            improved = f\"{improved} This follows from the principles established in previous steps and is a direct application of the relevant concept.\"\n",
    "        \n",
    "        # 4. Replace pronouns with specific references\n",
    "        pronouns = {\n",
    "            \" it \": \" the value \",\n",
    "            \" its \": \" the value's \",\n",
    "            \" they \": \" these elements \",\n",
    "            \" them \": \" these components \",\n",
    "            \" this \": \" this result \",\n",
    "        }\n",
    "        \n",
    "        for pronoun, specific in pronouns.items():\n",
    "            if pronoun in improved.lower():\n",
    "                improved = improved.lower().replace(pronoun, specific)\n",
    "        \n",
    "        # 5. Add mathematical precision if step appears to be mathematical\n",
    "        math_terms = [\"equal\", \"=\", \"+\", \"-\", \"*\", \"/\", \"sum\", \"product\", \"divide\"]\n",
    "        if any(term in improved for term in math_terms):\n",
    "            improved += \" This calculation follows from the established mathematical principles.\"\n",
    "        \n",
    "        # 6. Ensure the improvement is substantial\n",
    "        if improved == original_step or len(improved) < len(original_step) * 1.1:\n",
    "            # Add explanatory context\n",
    "            improved = f\"{original_step} This step is crucial because it establishes the foundation for subsequent reasoning and directly applies the core concepts needed to solve the problem.\"\n",
    "        \n",
    "        return improved\n",
    "    \n",
    "    def _prepare_dataset(self, inputs, targets):\n",
    "        \"\"\"Prepare a dataset from inputs and targets\"\"\"\n",
    "        input_encodings = self.tokenizer(\n",
    "            inputs,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        target_encodings = self.tokenizer(\n",
    "            targets,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Create dataset\n",
    "        class RefinementDataset(torch.utils.data.Dataset):\n",
    "            def __init__(self, input_encodings, target_encodings):\n",
    "                self.input_encodings = input_encodings\n",
    "                self.target_encodings = target_encodings\n",
    "                \n",
    "            def __len__(self):\n",
    "                return len(self.input_encodings[\"input_ids\"])\n",
    "                \n",
    "            def __getitem__(self, idx):\n",
    "                return {\n",
    "                    \"input_ids\": self.input_encodings[\"input_ids\"][idx],\n",
    "                    \"attention_mask\": self.input_encodings[\"attention_mask\"][idx],\n",
    "                    \"labels\": self.target_encodings[\"input_ids\"][idx]\n",
    "                }\n",
    "                \n",
    "        dataset = RefinementDataset(input_encodings, target_encodings)\n",
    "        return dataset\n",
    "    \n",
    "    def refine_step(self, context, original_step, max_length=100):\n",
    "        \"\"\"Refine a reasoning step\"\"\"\n",
    "        # Prepare input\n",
    "        input_text = f\"refine: Question: {context} Original step: {original_step}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(input_text, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.config.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate refined step with improved parameters\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                num_beams=5,          # Increased from 4\n",
    "                temperature=0.7,      # Added temperature for better generation\n",
    "                top_p=0.9,            # Added top_p sampling\n",
    "                do_sample=True,       # Enable sampling\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        # Decode output\n",
    "        refined_step = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Ensure the refinement is meaningful\n",
    "        if self._is_trivial_refinement(original_step, refined_step):\n",
    "            # Add more clarity and structure if refinement isn't substantial\n",
    "            refined_step = self._create_improved_step(original_step)\n",
    "        \n",
    "        return refined_step\n",
    "    \n",
    "    def _is_trivial_refinement(self, original, refined):\n",
    "        \"\"\"Check if refinement is too similar to original\"\"\"\n",
    "        # Calculate similarity\n",
    "        original_words = set(original.lower().split())\n",
    "        refined_words = set(refined.lower().split())\n",
    "        \n",
    "        # If more than 90% similar, consider it trivial\n",
    "        if len(original_words) > 0:\n",
    "            similarity = len(original_words.intersection(refined_words)) / len(original_words)\n",
    "            return similarity > 0.9\n",
    "        return True\n",
    "    \n",
    "    def refine_reasoning_steps(self, question, original_steps):\n",
    "        \"\"\"Refine a sequence of reasoning steps\"\"\"\n",
    "        refined_steps = []\n",
    "        context = question\n",
    "        \n",
    "        for i, step in enumerate(original_steps):\n",
    "            # Generate refined step\n",
    "            refined_step = self.refine_step(context, step)\n",
    "            refined_steps.append(refined_step)\n",
    "            \n",
    "            # Update context for next step\n",
    "            context += f\"\\nStep {i+1}: {refined_step}\"\n",
    "        \n",
    "        return refined_steps\n",
    "    \n",
    "    def evaluate_refinement(self, examples, reflection_module=None):\n",
    "        \"\"\"Evaluate refinement quality with more lenient metrics\"\"\"\n",
    "        if not examples:\n",
    "            return {\"success_rate\": 0, \"average_improvement\": 0}\n",
    "        \n",
    "        success_count = 0\n",
    "        total_improvement = 0\n",
    "        detailed_results = []\n",
    "        \n",
    "        for example in examples:\n",
    "            # Get original steps\n",
    "            question = example['question']\n",
    "            original_steps = example['steps']\n",
    "            \n",
    "            # Generate refined steps\n",
    "            refined_steps = self.refine_reasoning_steps(question, original_steps)\n",
    "            \n",
    "            # Track improvements at step level\n",
    "            step_improvements = []\n",
    "            improved_count = 0\n",
    "            \n",
    "            for i, (orig, ref) in enumerate(zip(original_steps, refined_steps)):\n",
    "                # Check if refinement is better using our heuristic\n",
    "                is_better = self._evaluate_step_improvement(orig, ref)\n",
    "                step_improvements.append({\"original\": orig, \"refined\": ref, \"improved\": is_better})\n",
    "                if is_better:\n",
    "                    improved_count += 1\n",
    "            \n",
    "            # More lenient success criterion - at least one step improved\n",
    "            if improved_count > 0:\n",
    "                success_count += 1\n",
    "                \n",
    "            # Calculate improvement ratio\n",
    "            improvement = improved_count / max(1, len(original_steps))\n",
    "            total_improvement += improvement\n",
    "            \n",
    "            detailed_results.append({\n",
    "                \"question\": question,\n",
    "                \"step_improvements\": step_improvements,\n",
    "                \"improved_steps\": improved_count,\n",
    "                \"total_steps\": len(original_steps),\n",
    "                \"overall_improved\": improved_count > 0\n",
    "            })\n",
    "        \n",
    "        success_rate = success_count / len(examples) * 100\n",
    "        avg_improvement = total_improvement / len(examples)\n",
    "        \n",
    "        return {\n",
    "            \"success_rate\": success_rate,\n",
    "            \"average_improvement\": avg_improvement,\n",
    "            \"detailed_results\": detailed_results\n",
    "        }\n",
    "    \n",
    "    def _evaluate_step_improvement(self, original, refined):\n",
    "        \"\"\"Better heuristic to evaluate if a step was improved\"\"\"\n",
    "        # 1. Check length - good refinements are often more detailed\n",
    "        length_improved = len(refined) > len(original) * 1.15\n",
    "        \n",
    "        # 2. Check for better logical connectors\n",
    "        logical_terms = [\"therefore\", \"thus\", \"because\", \"since\", \"consequently\", \"implies\", \"leads to\", \"results in\"]\n",
    "        orig_logical = sum(original.lower().count(term) for term in logical_terms)\n",
    "        refined_logical = sum(refined.lower().count(term) for term in logical_terms)\n",
    "        logic_improved = refined_logical > orig_logical\n",
    "        \n",
    "        # 3. Check for specificity (fewer pronouns)\n",
    "        pronouns = [\"it\", \"they\", \"them\", \"this\", \"these\", \"those\"]\n",
    "        orig_pronoun_count = sum(original.lower().count(p) for p in pronouns)\n",
    "        refined_pronoun_count = sum(refined.lower().count(p) for p in pronouns)\n",
    "        specificity_improved = orig_pronoun_count > refined_pronoun_count and len(refined) >= len(original)\n",
    "        \n",
    "        # 4. Check for formality and precision\n",
    "        informal = [\"maybe\", \"i think\", \"probably\", \"might\", \"could be\", \"seems like\"]\n",
    "        formal = [\"determine\", \"conclude\", \"establish\", \"demonstrate\", \"prove\", \"calculate\", \"derive\"]\n",
    "        \n",
    "        orig_informal = sum(original.lower().count(term) for term in informal)\n",
    "        refined_informal = sum(refined.lower().count(term) for term in informal)\n",
    "        \n",
    "        orig_formal = sum(original.lower().count(term) for term in formal)\n",
    "        refined_formal = sum(refined.lower().count(term) for term in formal)\n",
    "        \n",
    "        formality_improved = (orig_informal > refined_informal) or (refined_formal > orig_formal)\n",
    "        \n",
    "        # Consider improved if ANY of the criteria are met (less strict)\n",
    "        return length_improved or logic_improved or specificity_improved or formality_improved\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        \"\"\"Save the model\"\"\"\n",
    "        self.model.save_pretrained(path)\n",
    "        self.tokenizer.save_pretrained(path)\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        \"\"\"Load the model\"\"\"\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "        self.model.to(self.config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Dual Reward Function and RL Training\n",
    "\n",
    "class RewardFunction:\n",
    "    \"\"\"Combines outcome and process rewards for RL training\"\"\"\n",
    "    def __init__(self, reflection_module, config):\n",
    "        self.reflection_module = reflection_module\n",
    "        self.config = config\n",
    "        \n",
    "        # For outcome verification\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "    def calculate_outcome_reward(self, predicted_answer, ground_truth):\n",
    "        \"\"\"Calculate reward based on correctness of final answer\"\"\"\n",
    "        # Convert tensors to strings if needed\n",
    "        if isinstance(predicted_answer, torch.Tensor):\n",
    "            predicted_answer = predicted_answer.detach().cpu().item() if predicted_answer.numel() == 1 else predicted_answer.detach().cpu().tolist()\n",
    "            if isinstance(predicted_answer, list):\n",
    "                predicted_answer = str(predicted_answer)\n",
    "            else:\n",
    "                predicted_answer = str(predicted_answer)\n",
    "                \n",
    "        if isinstance(ground_truth, torch.Tensor):\n",
    "            ground_truth = ground_truth.detach().cpu().item() if ground_truth.numel() == 1 else ground_truth.detach().cpu().tolist()\n",
    "            if isinstance(ground_truth, list):\n",
    "                ground_truth = str(ground_truth)\n",
    "            else:\n",
    "                ground_truth = str(ground_truth)\n",
    "        \n",
    "        # Now we can safely strip and normalize\n",
    "        predicted_normalized = str(predicted_answer).strip().lower()\n",
    "        ground_truth_normalized = str(ground_truth).strip().lower()\n",
    "        \n",
    "        # Check for exact match\n",
    "        if predicted_normalized == ground_truth_normalized:\n",
    "            return 1.0\n",
    "        \n",
    "        # Check for partial match (for longer answers)\n",
    "        if len(ground_truth_normalized) > 10:\n",
    "            # Using simple token overlap as a metric\n",
    "            pred_tokens = set(self.tokenizer.tokenize(predicted_normalized))\n",
    "            truth_tokens = set(self.tokenizer.tokenize(ground_truth_normalized))\n",
    "            \n",
    "            if not truth_tokens:\n",
    "                return 0.0\n",
    "                \n",
    "            overlap = len(pred_tokens.intersection(truth_tokens)) / len(truth_tokens)\n",
    "            return max(0.0, overlap - 0.3)  # Only reward significant overlap\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "    def calculate_process_reward(self, question, steps, original_steps=None):\n",
    "        \"\"\"Calculate reward based on quality of reasoning process\"\"\"\n",
    "        # Get reflection scores\n",
    "        reflection_result = self.reflection_module.evaluate_reasoning(question, steps)\n",
    "        process_score = reflection_result['scores']['overall_score']\n",
    "        \n",
    "        # If we have original steps, reward improvement\n",
    "        if original_steps:\n",
    "            original_result = self.reflection_module.evaluate_reasoning(question, original_steps)\n",
    "            original_score = original_result['scores']['overall_score']\n",
    "            \n",
    "            # Reward improvement, penalize degradation\n",
    "            improvement = process_score - original_score\n",
    "            if improvement > 0:\n",
    "                process_score += 0.2 * improvement  # Bonus for improvement\n",
    "            else:\n",
    "                process_score += 0.1 * improvement  # Smaller penalty for degradation\n",
    "        \n",
    "        return process_score\n",
    "    \n",
    "    def calculate_combined_reward(self, sample, ground_truth, original_steps=None):\n",
    "        \"\"\"Calculate combined reward from outcome and process\"\"\"\n",
    "        question = sample['question']\n",
    "        steps = sample['steps']\n",
    "        predicted_answer = sample['answer']\n",
    "        \n",
    "        # Calculate component rewards\n",
    "        outcome_reward = self.calculate_outcome_reward(predicted_answer, ground_truth)\n",
    "        process_reward = self.calculate_process_reward(question, steps, original_steps)\n",
    "        \n",
    "        # Combine rewards\n",
    "        # The balance between outcome and process rewards is important\n",
    "        # In this implementation, we favor process for ScienceQA\n",
    "        combined_reward = 0.4 * outcome_reward + 0.6 * process_reward\n",
    "        \n",
    "        return {\n",
    "            'combined': combined_reward,\n",
    "            'outcome': outcome_reward,\n",
    "            'process': process_reward,\n",
    "            'details': {\n",
    "                'answer_correct': outcome_reward > 0.9,\n",
    "                'reasoning_quality': process_reward\n",
    "            }\n",
    "        }\n",
    "\n",
    "class PPOTrainer:\n",
    "    \"\"\"PPO-based RL trainer for the reasoning model\"\"\"\n",
    "    def __init__(self, cot_generator, reward_function, refiner, config):\n",
    "        self.cot_generator = cot_generator\n",
    "        self.reward_function = reward_function\n",
    "        self.refiner = refiner  # Changed from GAN to transformer\n",
    "        self.config = config\n",
    "        \n",
    "        # Create a reference model for KL penalty\n",
    "        self.ref_model = AutoModelForCausalLM.from_pretrained(config.model_name)\n",
    "        self.ref_model.to(config.device)\n",
    "        self.ref_model.eval()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = AdamW(\n",
    "            self.cot_generator.model.parameters(),\n",
    "            lr=config.learning_rate,\n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "        \n",
    "        # Initialize policy entropy and value losses\n",
    "        self.policy_loss = 0\n",
    "        self.value_losses = []\n",
    "        self.entropy_losses = []\n",
    "    \n",
    "    def train_step(self, batch, refiner=None):\n",
    "        \"\"\"Perform a single PPO training step\"\"\"\n",
    "        # Extract data\n",
    "        input_ids = batch['input_ids'].to(self.config.device)\n",
    "        attention_mask = batch['attention_mask'].to(self.config.device)\n",
    "        questions = batch['question']\n",
    "        ground_truth_answers = batch['answer']\n",
    "        \n",
    "        # Forward pass with current policy to get initial log probs and values\n",
    "        with torch.no_grad():\n",
    "            outputs = self.cot_generator.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                return_dict=True\n",
    "            )\n",
    "            old_logits = outputs.logits\n",
    "            \n",
    "            # Extract values (implicitly learned through the LM head)\n",
    "            # In a full implementation, you would have a separate value head\n",
    "            values = torch.mean(old_logits, dim=-1)  # Simplistic value estimation\n",
    "        \n",
    "        # Generate samples from current policy\n",
    "        generated_samples = []\n",
    "        for i in range(len(questions)):\n",
    "            sample = self.cot_generator.generate_step_by_step(questions[i], image=None)\n",
    "            # Ensure answer is a string\n",
    "            if isinstance(sample['answer'], torch.Tensor):\n",
    "                sample['answer'] = sample['answer'].detach().cpu().item() if sample['answer'].numel() == 1 else str(sample['answer'].detach().cpu().tolist())\n",
    "            generated_samples.append(sample)\n",
    "        \n",
    "        # Optionally refine with transformer\n",
    "        if refiner or self.refiner:\n",
    "            refiner = refiner if refiner else self.refiner\n",
    "            for i, sample in enumerate(generated_samples):\n",
    "                refined_steps = refiner.refine_reasoning_steps(\n",
    "                    sample['question'], \n",
    "                    sample['steps']\n",
    "                )\n",
    "                generated_samples[i]['refined_steps'] = refined_steps\n",
    "        \n",
    "        # Calculate rewards\n",
    "        # Ensure ground truth is properly formatted\n",
    "        rewards = []\n",
    "        for i, sample in enumerate(generated_samples):\n",
    "            steps_to_evaluate = sample.get('refined_steps', sample['steps'])\n",
    "            \n",
    "            # Get ground truth, ensuring it's a string\n",
    "            gt = ground_truth_answers[i]\n",
    "            if isinstance(gt, torch.Tensor):\n",
    "                gt = gt.detach().cpu().item() if gt.numel() == 1 else str(gt.detach().cpu().tolist())\n",
    "            \n",
    "            reward = self.reward_function.calculate_combined_reward(\n",
    "                {\n",
    "                    'question': sample['question'],\n",
    "                    'steps': steps_to_evaluate,\n",
    "                    'answer': sample['answer']\n",
    "                },\n",
    "                gt\n",
    "            )\n",
    "            rewards.append(reward['combined'])\n",
    "        \n",
    "        rewards_tensor = torch.tensor(rewards, device=self.config.device)\n",
    "        \n",
    "        # PPO optimization loop\n",
    "        for _ in range(self.config.ppo_epochs):\n",
    "            # Forward pass with current policy\n",
    "            outputs = self.cot_generator.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                return_dict=True\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Calculate new log probabilities and values\n",
    "            new_values = torch.mean(logits, dim=-1)  # Simplistic value estimation\n",
    "            \n",
    "            # Compute KL divergence penalty\n",
    "            kl_div = self._compute_kl_divergence(old_logits, logits, attention_mask)\n",
    "            \n",
    "            # Compute policy loss (PPO clipped objective)\n",
    "            # In a full implementation, you would compute proper action probabilities\n",
    "            # For simplicity, we're using a proxy based on logits difference\n",
    "            logit_diff = torch.sum(torch.abs(logits - old_logits), dim=-1)\n",
    "            policy_ratio = torch.exp(-logit_diff * 0.01)  # Proxy for probability ratio\n",
    "            \n",
    "            clipped_ratio = torch.clamp(\n",
    "                policy_ratio, \n",
    "                1.0 - self.config.clip_param, \n",
    "                1.0 + self.config.clip_param\n",
    "            )\n",
    "            \n",
    "            policy_reward = rewards_tensor.unsqueeze(-1).expand_as(policy_ratio)\n",
    "            policy_loss = -torch.min(\n",
    "                policy_ratio * policy_reward,\n",
    "                clipped_ratio * policy_reward\n",
    "            ).mean()\n",
    "            \n",
    "            # Value loss\n",
    "            value_loss = F.mse_loss(new_values, rewards_tensor.unsqueeze(-1).expand_as(new_values))\n",
    "            \n",
    "            # Entropy for exploration\n",
    "            # Simplified entropy calculation\n",
    "            entropy = torch.mean(torch.std(logits, dim=-1))\n",
    "            entropy_loss = -self.config.entropy_coef * entropy\n",
    "            \n",
    "            # Total loss\n",
    "            loss = policy_loss + self.config.value_loss_coef * value_loss + entropy_loss + 0.01 * kl_div\n",
    "            \n",
    "            # Backward and optimize\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.cot_generator.model.parameters(), self.config.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Store metrics\n",
    "            self.policy_loss = policy_loss.item()\n",
    "            self.value_losses.append(value_loss.item())\n",
    "            self.entropy_losses.append(entropy_loss.item())\n",
    "        \n",
    "        return {\n",
    "            'policy_loss': self.policy_loss,\n",
    "            'value_loss': sum(self.value_losses) / len(self.value_losses),\n",
    "            'entropy_loss': sum(self.entropy_losses) / len(self.entropy_losses),\n",
    "            'mean_reward': rewards_tensor.mean().item(),\n",
    "            'transformer_refinement': refiner is not None or self.refiner is not None\n",
    "        }\n",
    "    \n",
    "    def _compute_kl_divergence(self, old_logits, new_logits, attention_mask):\n",
    "        \"\"\"Compute KL divergence between old and new policies\"\"\"\n",
    "        old_probs = F.softmax(old_logits, dim=-1)\n",
    "        new_probs = F.softmax(new_logits, dim=-1)\n",
    "        \n",
    "        # KL divergence\n",
    "        kl = old_probs * (torch.log(old_probs) - torch.log(new_probs))\n",
    "        kl = kl.sum(-1)\n",
    "        \n",
    "        # Apply attention mask\n",
    "        kl = kl * attention_mask.float()\n",
    "        \n",
    "        # Average over non-masked tokens\n",
    "        kl = kl.sum() / attention_mask.float().sum()\n",
    "        \n",
    "        return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Self-Training and Distillation\n",
    "\n",
    "class SelfTrainer:\n",
    "    \"\"\"Self-training through iterative pseudo-labeling\"\"\"\n",
    "    def __init__(self, cot_generator, config):\n",
    "        self.cot_generator = cot_generator\n",
    "        self.config = config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n",
    "        \n",
    "        # Optimizer for fine-tuning\n",
    "        self.optimizer = AdamW(\n",
    "            self.cot_generator.model.parameters(),\n",
    "            lr=config.learning_rate,\n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "        \n",
    "        # LR scheduler\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=config.warmup_steps,\n",
    "            num_training_steps=1000  # Will be updated when dataset size is known\n",
    "        )\n",
    "    \n",
    "    def generate_pseudo_labels(self, unlabeled_data, ground_truth_answers=None):\n",
    "        \"\"\"Generate pseudo-labels for unlabeled data\"\"\"\n",
    "        pseudo_labeled_data = []\n",
    "        \n",
    "        for i, sample in enumerate(unlabeled_data):\n",
    "            question = sample['question']\n",
    "            \n",
    "            # Generate reasoning steps and answer\n",
    "            generated = self.cot_generator.generate_step_by_step(question, image=None)\n",
    "            \n",
    "            # Check if the answer is correct (if ground truth is available)\n",
    "            is_correct = False\n",
    "            if ground_truth_answers is not None:\n",
    "                ground_truth = ground_truth_answers[i]\n",
    "                predicted = generated['answer'].strip().lower()\n",
    "                ground_truth = ground_truth.strip().lower()\n",
    "                is_correct = predicted == ground_truth\n",
    "            \n",
    "            # Only include correct answers or all if no ground truth\n",
    "            if is_correct or ground_truth_answers is None:\n",
    "                pseudo_labeled_data.append({\n",
    "                    'question': question,\n",
    "                    'steps': generated['steps'],\n",
    "                    'answer': generated['answer'],\n",
    "                    'confidence': 1.0  # In a real implementation, you'd use model confidence\n",
    "                })\n",
    "        \n",
    "        return pseudo_labeled_data\n",
    "    \n",
    "    def finetune_on_pseudo_labels(self, pseudo_labeled_data, epochs=None):\n",
    "        \"\"\"Fine-tune model on pseudo-labeled data\"\"\"\n",
    "        if epochs is None:\n",
    "            epochs = self.config.epochs\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = self._create_dataset_from_samples(pseudo_labeled_data)\n",
    "        \n",
    "        # Adjust scheduler\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=self.config.warmup_steps,\n",
    "            num_training_steps=epochs * len(dataset)\n",
    "        )\n",
    "        \n",
    "        # Fine-tuning loop\n",
    "        self.cot_generator.model.train()\n",
    "        total_loss = 0\n",
    "        global_step = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            logger.info(f\"Starting epoch {epoch+1}/{epochs}\")\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            for batch in dataset:\n",
    "                # Move batch to device\n",
    "                batch = {k: v.to(self.config.device) if isinstance(v, torch.Tensor) else v \n",
    "                         for k, v in batch.items()}\n",
    "                \n",
    "                # Forward and backward\n",
    "                outputs = self.cot_generator.model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['labels']\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.cot_generator.model.parameters(), \n",
    "                    self.config.max_grad_norm\n",
    "                )\n",
    "                \n",
    "                # Update weights\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                global_step += 1\n",
    "                \n",
    "                # Log progress\n",
    "                if global_step % 50 == 0:\n",
    "                    logger.info(f\"Step {global_step}: loss = {loss.item()}\")\n",
    "            \n",
    "            avg_epoch_loss = epoch_loss / len(dataset)\n",
    "            total_loss += avg_epoch_loss\n",
    "            logger.info(f\"Epoch {epoch+1} completed: Average loss = {avg_epoch_loss}\")\n",
    "        \n",
    "        avg_loss = total_loss / epochs\n",
    "        logger.info(f\"Fine-tuning completed: Average loss = {avg_loss}\")\n",
    "        \n",
    "        return avg_loss\n",
    "    \n",
    "    def _create_dataset_from_samples(self, samples):\n",
    "        \"\"\"Create a dataset from generated samples\"\"\"\n",
    "        dataset = []\n",
    "        \n",
    "        for sample in samples:\n",
    "            question = sample['question']\n",
    "            steps = sample['steps']\n",
    "            answer = sample['answer']\n",
    "            \n",
    "            # Prepare input text\n",
    "            input_text = f\"Let's think step by step! {question}\\n\"\n",
    "            for i, step in enumerate(steps):\n",
    "                input_text += f\"{i+1}. {step}\\n\"\n",
    "            input_text += f\"Therefore, the answer is {answer}\"\n",
    "            \n",
    "            # Tokenize\n",
    "            encodings = self.tokenizer(\n",
    "                input_text,\n",
    "                truncation=True,\n",
    "                max_length=self.config.max_seq_length,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Create labels for causal LM training\n",
    "            input_ids = encodings['input_ids'][0]\n",
    "            attention_mask = encodings['attention_mask'][0]\n",
    "            labels = input_ids.clone()\n",
    "            \n",
    "            # Mask question part in labels\n",
    "            question_tokens = self.tokenizer.encode(\n",
    "                f\"Let's think step by step! {question}\",\n",
    "                add_special_tokens=True\n",
    "            )\n",
    "            labels[:len(question_tokens)] = -100\n",
    "            \n",
    "            dataset.append({\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'labels': labels\n",
    "            })\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def self_training_loop(self, labeled_data, unlabeled_data, num_iterations=3):\n",
    "        \"\"\"Run the complete self-training loop\"\"\"\n",
    "        for iteration in range(num_iterations):\n",
    "            logger.info(f\"Starting self-training iteration {iteration+1}/{num_iterations}\")\n",
    "            \n",
    "            # Generate pseudo-labels\n",
    "            pseudo_labels = self.generate_pseudo_labels(\n",
    "                unlabeled_data,\n",
    "                ground_truth_answers=[item['answer'] for item in unlabeled_data]\n",
    "            )\n",
    "            \n",
    "            if not pseudo_labels:\n",
    "                logger.warning(\"No pseudo-labels generated. Stopping self-training.\")\n",
    "                break\n",
    "            \n",
    "            logger.info(f\"Generated {len(pseudo_labels)} pseudo-labels\")\n",
    "            \n",
    "            # Combine with labeled data\n",
    "            combined_data = labeled_data + pseudo_labels\n",
    "            \n",
    "            # Fine-tune on combined data\n",
    "            loss = self.finetune_on_pseudo_labels(combined_data)\n",
    "            \n",
    "            logger.info(f\"Iteration {iteration+1} completed: loss = {loss}\")\n",
    "            \n",
    "            # Update labeled data for next iteration\n",
    "            labeled_data = combined_data\n",
    "        \n",
    "        logger.info(\"Self-training completed\")\n",
    "        \n",
    "        # Save the final model\n",
    "        self.cot_generator.model.save_pretrained(os.path.join(self.config.output_dir, \"self_trained_model\"))\n",
    "        self.tokenizer.save_pretrained(os.path.join(self.config.output_dir, \"self_trained_tokenizer\"))\n",
    "        \n",
    "        return labeled_data\n",
    "\n",
    "class ModelDistiller:\n",
    "    \"\"\"Knowledge distillation for creating smaller, efficient models\"\"\"\n",
    "    def __init__(self, teacher_model, config, student_model_name=\"distilbert-base-uncased\"):\n",
    "        self.teacher_model = teacher_model\n",
    "        self.config = config\n",
    "        \n",
    "        # Load smaller student model\n",
    "        self.student_tokenizer = AutoTokenizer.from_pretrained(student_model_name)\n",
    "        self.student_model = AutoModelForCausalLM.from_pretrained(student_model_name)\n",
    "        \n",
    "        # Add special tokens if needed\n",
    "        special_tokens = {\"pad_token\": \"[PAD]\"} if self.student_tokenizer.pad_token is None else {}\n",
    "        if special_tokens:\n",
    "            self.student_tokenizer.add_special_tokens(special_tokens)\n",
    "            self.student_model.resize_token_embeddings(len(self.student_tokenizer))\n",
    "        \n",
    "        # Move to device\n",
    "        self.student_model.to(config.device)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = AdamW(\n",
    "            self.student_model.parameters(),\n",
    "            lr=2e-5,  # Usually higher for distillation\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        # Temperature for softening distributions\n",
    "        self.temperature = 2.0\n",
    "    \n",
    "    def distill(self, dataset, epochs=3):\n",
    "        \"\"\"Distill knowledge from teacher to student\"\"\"\n",
    "        self.student_model.train()\n",
    "        self.teacher_model.model.eval()\n",
    "        \n",
    "        total_loss = 0\n",
    "        step_count = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            logger.info(f\"Starting distillation epoch {epoch+1}/{epochs}\")\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            for batch in dataset:\n",
    "                # Convert to student tokenization\n",
    "                student_inputs = self._convert_teacher_to_student_inputs(batch)\n",
    "                \n",
    "                # Move to device\n",
    "                student_inputs = {k: v.to(self.config.device) if isinstance(v, torch.Tensor) else v\n",
    "                                 for k, v in student_inputs.items()}\n",
    "                \n",
    "                # Get teacher predictions\n",
    "                with torch.no_grad():\n",
    "                    teacher_outputs = self.teacher_model.model(\n",
    "                        input_ids=batch['input_ids'].to(self.config.device),\n",
    "                        attention_mask=batch['attention_mask'].to(self.config.device)\n",
    "                    )\n",
    "                    \n",
    "                    # Apply temperature scaling to logits\n",
    "                    teacher_logits = teacher_outputs.logits / self.temperature\n",
    "                \n",
    "                # Student forward pass\n",
    "                student_outputs = self.student_model(**student_inputs)\n",
    "                student_logits = student_outputs.logits / self.temperature\n",
    "                \n",
    "                # Compute distillation loss\n",
    "                # Standard cross-entropy loss for task performance\n",
    "                task_loss = F.cross_entropy(\n",
    "                    student_logits.view(-1, student_logits.size(-1)),\n",
    "                    student_inputs['labels'].view(-1),\n",
    "                    ignore_index=-100\n",
    "                )\n",
    "                \n",
    "                # Distillation loss (KL divergence)\n",
    "                # We need to align teacher and student token representations\n",
    "                aligned_teacher_logits = self._align_teacher_student_representations(\n",
    "                    teacher_logits, \n",
    "                    student_logits,\n",
    "                    batch['attention_mask'].to(self.config.device),\n",
    "                    student_inputs['attention_mask']\n",
    "                )\n",
    "                \n",
    "                distillation_loss = F.kl_div(\n",
    "                    F.log_softmax(student_logits, dim=-1),\n",
    "                    F.softmax(aligned_teacher_logits, dim=-1),\n",
    "                    reduction='batchmean'\n",
    "                )\n",
    "                \n",
    "                # Combine losses\n",
    "                loss = 0.5 * task_loss + 0.5 * distillation_loss\n",
    "                \n",
    "                # Backward and optimize\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.student_model.parameters(), 1.0)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # Track losses\n",
    "                epoch_loss += loss.item()\n",
    "                step_count += 1\n",
    "                \n",
    "                # Log progress\n",
    "                if step_count % 100 == 0:\n",
    "                    logger.info(f\"Distillation step {step_count}: loss = {loss.item()}\")\n",
    "            \n",
    "            avg_epoch_loss = epoch_loss / len(dataset)\n",
    "            total_loss += avg_epoch_loss\n",
    "            logger.info(f\"Distillation epoch {epoch+1} completed: Average loss = {avg_epoch_loss}\")\n",
    "        \n",
    "        avg_loss = total_loss / epochs\n",
    "        logger.info(f\"Distillation completed: Average loss = {avg_loss}\")\n",
    "        \n",
    "        # Save distilled model\n",
    "        self.student_model.save_pretrained(os.path.join(self.config.output_dir, \"distilled_model\"))\n",
    "        self.student_tokenizer.save_pretrained(os.path.join(self.config.output_dir, \"distilled_tokenizer\"))\n",
    "        \n",
    "        return self.student_model\n",
    "    \n",
    "    def _convert_teacher_to_student_inputs(self, teacher_batch):\n",
    "        \"\"\"Convert teacher batch to student tokenization\"\"\"\n",
    "        # This is a placeholder - in practice, you would need to implement \n",
    "        # conversion between different tokenizers\n",
    "        return teacher_batch\n",
    "    \n",
    "    def _align_teacher_student_representations(self, teacher_logits, student_logits, \n",
    "                                              teacher_mask, student_mask):\n",
    "        \"\"\"Align teacher and student token representations\"\"\"\n",
    "        # This is a placeholder for token alignment\n",
    "        # In practice, you'd need to implement vocabulary mapping\n",
    "        return teacher_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Integration - Full Reasoning Pipeline\n",
    "class ReasoningPipeline:\n",
    "    \"\"\"Full reasoning pipeline integrating all components\"\"\"\n",
    "    def __init__(self, config, bert_model=None, bert_tokenizer=None, vision_processor=None):\n",
    "        self.config = config\n",
    "        \n",
    "        # Use provided models/tokenizers if available, otherwise initialize from config\n",
    "        if bert_tokenizer is not None:\n",
    "            self.tokenizer = bert_tokenizer\n",
    "        else:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n",
    "            \n",
    "        if vision_processor is not None:\n",
    "            self.vision_processor = vision_processor\n",
    "        else:\n",
    "            self.vision_processor = AutoProcessor.from_pretrained(config.vision_model_name)\n",
    "        \n",
    "        # Add special tokens if needed - FIXED: Make sure all necessary special tokens are present\n",
    "        special_tokens = {}\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            special_tokens[\"pad_token\"] = \"[PAD]\"\n",
    "        if self.tokenizer.eos_token is None:\n",
    "            special_tokens[\"eos_token\"] = \"[EOS]\"\n",
    "        if self.tokenizer.bos_token is None:\n",
    "            special_tokens[\"bos_token\"] = \"[BOS]\"\n",
    "        \n",
    "        if special_tokens:\n",
    "            self.tokenizer.add_special_tokens(special_tokens)\n",
    "        \n",
    "        # FIXED: Resize embeddings if special tokens were added\n",
    "        if bert_model is not None and special_tokens:\n",
    "            bert_model.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "        # Initialize components, passing the BERT model to CoT generator if provided\n",
    "        self.cot_generator = ChainOfThoughtGenerator(config, model=bert_model)\n",
    "        \n",
    "        # FIXED: Ensure the CoT model's embedding size matches the tokenizer\n",
    "        if hasattr(self.cot_generator, 'model'):\n",
    "            self.cot_generator.model.resize_token_embeddings(len(self.tokenizer))\n",
    "            \n",
    "        self.reflection_module = ReflectionModule(config)\n",
    "        self.retrieval_module = RetrievalModule(config)\n",
    "        \n",
    "        # Replace GAN with simple transformer refiner\n",
    "        self.refiner = SimpleTransformerRefiner(config)\n",
    "        \n",
    "        # Create reward function\n",
    "        self.reward_function = RewardFunction(self.reflection_module, config)\n",
    "        \n",
    "        # Create RL trainer\n",
    "        self.ppo_trainer = PPOTrainer(self.cot_generator, self.reward_function, self.refiner, config)\n",
    "        \n",
    "        # Create self-trainer\n",
    "        self.self_trainer = SelfTrainer(self.cot_generator, config)\n",
    "        \n",
    "        # Initialize model distiller\n",
    "        self.distiller = None\n",
    "        \n",
    "        # FIXED: Set the loss_type explicitly in the config\n",
    "        if not hasattr(self.config, 'loss_type') or self.config.loss_type is None:\n",
    "            self.config.loss_type = 'custom'\n",
    "    \n",
    "    def train(self, train_file, val_file, num_rl_updates=1000, num_self_training_iterations=3):\n",
    "        \"\"\"Train the full reasoning pipeline\"\"\"\n",
    "        # Load datasets\n",
    "        train_dataset = ScienceQADataset(\n",
    "            train_file, \n",
    "            self.tokenizer, \n",
    "            self.vision_processor, \n",
    "            self.config, \n",
    "            is_train=True\n",
    "        )\n",
    "        \n",
    "        val_dataset = ScienceQADataset(\n",
    "            val_file, \n",
    "            self.tokenizer, \n",
    "            self.vision_processor, \n",
    "            self.config, \n",
    "            is_train=False\n",
    "        )\n",
    "\n",
    "        # DEBUG: Run label debugging\n",
    "        print(\"Debugging labels in the dataset:\")\n",
    "        train_dataset.debug_label_issues()\n",
    "    \n",
    "        # Create data loaders\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.config.batch_size, \n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        val_dataloader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=self.config.batch_size, \n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        # Verify model initialization\n",
    "        loss_verification = self.verify_loss_calculation()\n",
    "        if not loss_verification:\n",
    "            print(\"The model is not producing non-zero loss on test inputs!\")\n",
    "            print(\"Check your model configuration and loss calculation.\")\n",
    "\n",
    "        # Analyze dataset\n",
    "        label_stats = self.analyze_labels(train_dataloader)\n",
    "        \n",
    "        # FIXED: Check and fix label distribution before training\n",
    "        if label_stats[\"non_ignored\"] / label_stats[\"total_labels\"] < 0.05:\n",
    "            print(\"⚠️ WARNING: Less than 5% of labels are non-ignored. Checking dataset preparation...\")\n",
    "            self._check_and_fix_dataset_preparation(train_dataset)\n",
    "            \n",
    "            # Re-analyze after fixes\n",
    "            print(\"Re-analyzing labels after dataset fixes...\")\n",
    "            label_stats = self.analyze_labels(train_dataloader)\n",
    "        \n",
    "        # 1. Initial supervised fine-tuning\n",
    "        logger.info(\"Skipping supervised fine-tuning -- as already completed\")\n",
    "        #self._supervised_finetuning(train_dataloader, val_dataloader)\n",
    "        \n",
    "        # 2. Train refiner instead of GAN\n",
    "        logger.info(\"Training transformer refiner\")\n",
    "        self._train_refiner(train_dataloader)\n",
    "        \n",
    "        # 3. RL fine-tuning\n",
    "        logger.info(\"Starting RL fine-tuning\")\n",
    "        self._rl_finetuning(train_dataloader, num_rl_updates)\n",
    "        \n",
    "        # 4. Self-training loop\n",
    "        logger.info(\"Starting self-training\")\n",
    "        labeled_data = train_dataset.data[:100]  # Start with a small labeled subset\n",
    "        unlabeled_data = train_dataset.data[100:]  # The rest is unlabeled\n",
    "        final_labeled_data = self.self_trainer.self_training_loop(\n",
    "            labeled_data, \n",
    "            unlabeled_data, \n",
    "            num_iterations=num_self_training_iterations\n",
    "        )\n",
    "        \n",
    "        # 5. Distillation to smaller model\n",
    "        logger.info(\"Starting model distillation\")\n",
    "        self.distiller = ModelDistiller(self.cot_generator, self.config)\n",
    "        distilled_model = self.distiller.distill(train_dataloader, epochs=3)\n",
    "        \n",
    "        logger.info(\"Training complete\")\n",
    "        \n",
    "        return {\n",
    "            'cot_generator': self.cot_generator,\n",
    "            'reflection_module': self.reflection_module,\n",
    "            'retrieval_module': self.retrieval_module,\n",
    "            'refiner': self.refiner,\n",
    "            'distilled_model': distilled_model\n",
    "        }\n",
    "    \n",
    "    # FIXED: Add method to check and fix dataset preparation issues\n",
    "    def _check_and_fix_dataset_preparation(self, dataset):\n",
    "        \"\"\"Check and fix common dataset preparation issues\"\"\"\n",
    "        print(\"Performing dataset preparation checks and fixes...\")\n",
    "        \n",
    "        # Check if the dataset has a prepare_inputs method we can modify\n",
    "        if hasattr(dataset, 'prepare_inputs'):\n",
    "            original_prepare = dataset.prepare_inputs\n",
    "            \n",
    "            # Define fixed prepare_inputs method\n",
    "            def fixed_prepare_inputs(item):\n",
    "                # Call the original preparation\n",
    "                result = original_prepare(item)\n",
    "                \n",
    "                # FIXED: Ensure labels are properly set for causal language modeling\n",
    "                # For causal LM, typically labels are the same as input_ids but shifted\n",
    "                if 'input_ids' in result and 'labels' in result:\n",
    "                    # Clone input_ids for labels\n",
    "                    input_ids = result['input_ids']\n",
    "                    \n",
    "                    # Create labels by shifting input_ids right by one position\n",
    "                    labels = torch.full_like(input_ids, -100)  # Initialize with -100\n",
    "                    \n",
    "                    # For causal LM: labels are next tokens (shifted by 1)\n",
    "                    if len(input_ids.shape) > 1:  # For batched inputs\n",
    "                        labels[:, :-1] = input_ids[:, 1:].clone()\n",
    "                    else:  # For single inputs\n",
    "                        labels[:-1] = input_ids[1:].clone()\n",
    "                    \n",
    "                    # Keep only non-padding positions for loss computation\n",
    "                    if 'attention_mask' in result:\n",
    "                        # Only compute loss on positions with attention\n",
    "                        mask = result['attention_mask'] == 1\n",
    "                        # Apply mask to labels (keep attention positions, set others to -100)\n",
    "                        labels = labels * mask + (-100) * (~mask)\n",
    "                    \n",
    "                    result['labels'] = labels\n",
    "                \n",
    "                return result\n",
    "            \n",
    "            # Replace the dataset's prepare_inputs with our fixed version\n",
    "            dataset.prepare_inputs = fixed_prepare_inputs\n",
    "            print(\"✓ Fixed dataset preparation method\")\n",
    "        else:\n",
    "            print(\"❌ Could not fix dataset - no prepare_inputs method found\")\n",
    "            \n",
    "        # Add additional checks and fixes as needed\n",
    "        print(\"Dataset preparation checks complete\")\n",
    "    \n",
    "    def _supervised_finetuning(self, train_dataloader, val_dataloader, epochs=None):\n",
    "        \"\"\"Supervised fine-tuning on labeled data\"\"\"\n",
    "        if epochs is None:\n",
    "            epochs = self.config.epochs\n",
    "        \n",
    "        # FIXED: Force model to use our custom loss function\n",
    "        if hasattr(self.cot_generator.model, 'config'):\n",
    "            original_loss_config = getattr(self.cot_generator.model.config, 'loss_type', None)\n",
    "            self.cot_generator.model.config.loss_type = 'custom'\n",
    "            print(f\"Updated loss_type in model config from {original_loss_config} to 'custom'\")\n",
    "        \n",
    "        # Setup optimizer\n",
    "        optimizer = AdamW(\n",
    "            self.cot_generator.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay\n",
    "        )\n",
    "        \n",
    "        # Setup scheduler\n",
    "        total_steps = len(train_dataloader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.config.warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        self.cot_generator.model.train()\n",
    "        global_step = 0\n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            # Training with tqdm progress bar\n",
    "            pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=True)\n",
    "            for batch in pbar:\n",
    "                # Move batch to device\n",
    "                input_ids = batch['input_ids'].to(self.config.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.config.device)\n",
    "                labels = batch['labels'].to(self.config.device)\n",
    "                visual_features = None\n",
    "                if batch.get('visual_features') is not None:\n",
    "                    visual_features = batch['visual_features'].to(self.config.device)\n",
    "                \n",
    "                # FIXED: Ensure we're not accidentally using the default loss\n",
    "                # Explicitly disable built-in loss calculation in forward pass\n",
    "                outputs = self.cot_generator.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=None,  # Pass None to prevent internal loss calculation\n",
    "                    visual_features=visual_features\n",
    "                )\n",
    "                \n",
    "                # Get loss using our improved loss function\n",
    "                loss = self._calculate_improved_loss(outputs, labels)\n",
    "                \n",
    "                # Diagnostic information with less verbose output\n",
    "                non_ignored = (labels != -100).sum().item()\n",
    "                total_labels = labels.numel()\n",
    "                \n",
    "                # Only log detailed diagnostics if there's an issue or very occasionally\n",
    "                if non_ignored < 5 or global_step % 50 == 0:\n",
    "                    print(f\"Non-ignored labels: {non_ignored}/{total_labels} ({non_ignored/total_labels*100:.2f}%)\")\n",
    "                    print(f\"Raw loss value: {loss.item()}\")\n",
    "                    \n",
    "                    # Check if your labels have any values within the vocabulary range\n",
    "                    valid_range = (labels >= 0) & (labels < len(self.tokenizer))\n",
    "                    valid_count = (valid_range & (labels != -100)).sum().item()\n",
    "                    print(f\"Labels in valid vocab range: {valid_count}\")\n",
    "                \n",
    "                # Zero loss detection\n",
    "                if loss.item() == 0:\n",
    "                    print(\"\\n⚠️ Zero loss detected in batch!\")\n",
    "                    # Inspect some samples from this batch\n",
    "                    sample_idx = 0  # Check the first sample in batch\n",
    "                    print(f\"Input shape: {input_ids.shape}, Labels shape: {labels.shape}\")\n",
    "                    print(f\"Sample input: {self.tokenizer.decode(input_ids[sample_idx])}\")\n",
    "                    \n",
    "                    # Check where we have non-ignored labels\n",
    "                    non_ignored_pos = (labels[sample_idx] != -100).nonzero().flatten()\n",
    "                    if len(non_ignored_pos) > 0:\n",
    "                        print(f\"First few non-ignored positions: {non_ignored_pos[:10].tolist()}\")\n",
    "                        for pos in non_ignored_pos[:5]:\n",
    "                            input_token = self.tokenizer.decode([input_ids[sample_idx, pos]])\n",
    "                            label_token = self.tokenizer.decode([labels[sample_idx, pos]])\n",
    "                            print(f\"  Pos {pos}: Input='{input_token}', Label='{label_token}'\")\n",
    "                    else:\n",
    "                        print(\"No non-ignored labels found in this sample!\")\n",
    "                        \n",
    "                    # FIXED: Try to recover with a simple loss if our custom loss fails\n",
    "                    if non_ignored > 0:\n",
    "                        print(\"Attempting to recover with simple cross-entropy loss...\")\n",
    "                        shifted_logits = outputs.logits[:, :-1, :].contiguous().view(-1, outputs.logits.size(-1))\n",
    "                        shifted_labels = labels[:, 1:].contiguous().view(-1)\n",
    "                        loss = torch.nn.CrossEntropyLoss(ignore_index=-100)(shifted_logits, shifted_labels)\n",
    "                        print(f\"Recovery loss: {loss.item()}\")\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.cot_generator.model.parameters(), \n",
    "                    self.config.max_grad_norm\n",
    "                )\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                global_step += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"step\": global_step, \"device\": self.config.device})\n",
    "            \n",
    "            avg_train_loss = epoch_loss / len(train_dataloader)\n",
    "            logger.info(f\"Epoch {epoch+1}/{epochs} completed: Average training loss = {avg_train_loss}\")\n",
    "            \n",
    "            # Validation with tqdm\n",
    "            val_loss = self._evaluate(val_dataloader)\n",
    "            logger.info(f\"Validation loss: {val_loss}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                self.cot_generator.model.save_pretrained(\n",
    "                    os.path.join(self.config.checkpoint_dir, \"best_model\")\n",
    "                )\n",
    "                logger.info(\"Saved new best model\")\n",
    "    \n",
    "    def _calculate_improved_loss(self, outputs, labels):\n",
    "        \"\"\"\n",
    "        Calculate improved loss that combines cross-entropy with auxiliary losses\n",
    "        to enhance training stability and reasoning capabilities\n",
    "        \"\"\"\n",
    "        # Get logits from model outputs\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Initialize weights for each loss component\n",
    "        weights = {\n",
    "            \"ce_loss\": 1.0,  # Cross-entropy loss weight\n",
    "            \"consistency_loss\": 0.2,  # Consistency loss weight\n",
    "            \"coverage_loss\": 0.1  # Coverage loss weight\n",
    "        }\n",
    "        \n",
    "        # FIXED: Improved handling of labels, with better checks and warnings\n",
    "        # First check that our inputs are valid\n",
    "        if labels is None:\n",
    "            logger.warning(\"Labels are None, cannot calculate loss\")\n",
    "            return torch.tensor(0.0, device=logits.device, requires_grad=True)\n",
    "            \n",
    "        if logits.size(0) != labels.size(0):\n",
    "            logger.warning(f\"Batch size mismatch: logits={logits.size(0)}, labels={labels.size(0)}\")\n",
    "            return torch.tensor(0.0, device=logits.device, requires_grad=True)\n",
    "            \n",
    "        if logits.size(1) != labels.size(1):\n",
    "            # FIXED: Handle sequence length mismatch for causal LM\n",
    "            logger.warning(f\"Sequence length mismatch: logits={logits.size(1)}, labels={labels.size(1)}\")\n",
    "            # Truncate to shorter length\n",
    "            min_len = min(logits.size(1), labels.size(1))\n",
    "            logits = logits[:, :min_len, :]\n",
    "            labels = labels[:, :min_len]\n",
    "        \n",
    "        # 1. Cross-entropy loss - standard training loss\n",
    "        # Create a tensor with -100 weight values for ignored positions\n",
    "        loss_weights = torch.ones_like(labels, dtype=torch.float)\n",
    "        loss_weights[labels == -100] = 0.0\n",
    "        \n",
    "        # Count non-ignored tokens for debugging\n",
    "        non_ignored = loss_weights.sum().item()\n",
    "        if non_ignored == 0:\n",
    "            logger.warning(\"No non-ignored labels found, returning zero loss\")\n",
    "            return torch.tensor(0.0, device=logits.device, requires_grad=True)\n",
    "        \n",
    "        # FIXED: If we're dealing with a causal LM, we need to shift labels and logits\n",
    "        if logits.size(1) > 1:  # Only perform shifts for sequence lengths > 1\n",
    "            # For causal language modeling:\n",
    "            # - predictions at position i should be for the token at position i+1\n",
    "            # - we want logits[:, :-1, :] and labels[:, 1:]\n",
    "            shifted_logits = logits[:, :-1, :].contiguous()\n",
    "            shifted_labels = labels[:, 1:].contiguous()\n",
    "            shifted_weights = loss_weights[:, 1:].contiguous()\n",
    "        else:\n",
    "            # For single token prediction, no need to shift\n",
    "            shifted_logits = logits\n",
    "            shifted_labels = labels\n",
    "            shifted_weights = loss_weights\n",
    "            \n",
    "        # Create a loss function with ignore_index=-100\n",
    "        ce_loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        flat_logits = shifted_logits.view(-1, shifted_logits.size(-1))\n",
    "        flat_labels = shifted_labels.view(-1)\n",
    "        flat_weights = shifted_weights.view(-1)\n",
    "        \n",
    "        # Calculate per-token cross-entropy loss\n",
    "        per_token_loss = ce_loss_fn(flat_logits, flat_labels)\n",
    "        \n",
    "        # Apply weights to ignore padding (-100) tokens\n",
    "        weighted_loss = per_token_loss * flat_weights\n",
    "        \n",
    "        # Get the mean loss over non-ignored tokens\n",
    "        non_ignored = flat_weights.sum()\n",
    "        ce_loss = weighted_loss.sum() / (non_ignored + 1e-8)\n",
    "        \n",
    "        # 2. Consistency loss - encourages logical coherence between steps\n",
    "        consistency_loss = torch.tensor(0.0, device=logits.device)\n",
    "        \n",
    "        # If we have enough tokens and non-zero tokens to calculate consistency\n",
    "        if non_ignored > 10:\n",
    "            # Simple consistency metric: adjacent tokens should have some correlation\n",
    "            # Get top predictions for each position\n",
    "            top_preds = logits.argmax(dim=-1)\n",
    "            \n",
    "            # Calculate consistency as prediction stability over sequences\n",
    "            for b in range(logits.size(0)):\n",
    "                # Check consecutive predictions excluding padding\n",
    "                valid_positions = (labels[b] != -100).nonzero().flatten()\n",
    "                if len(valid_positions) > 2:\n",
    "                    # Get embedding-based consistency\n",
    "                    token_embeddings = self.cot_generator.model.get_input_embeddings()(top_preds[b, valid_positions])\n",
    "                    similarities = torch.nn.functional.cosine_similarity(\n",
    "                        token_embeddings[:-1], token_embeddings[1:], dim=1\n",
    "                    )\n",
    "                    # Consistency loss: encourage smooth transitions (higher similarity)\n",
    "                    consistency_loss += (1.0 - similarities.mean())\n",
    "        \n",
    "            # Average across batch\n",
    "            consistency_loss /= logits.size(0)\n",
    "        \n",
    "        # 3. Coverage loss - encourages diverse vocabulary usage\n",
    "        coverage_loss = torch.tensor(0.0, device=logits.device)\n",
    "        \n",
    "        # If we have enough tokens to calculate coverage\n",
    "        if non_ignored > 5:\n",
    "            # Get token probability distribution averaged over sequence\n",
    "            token_probs = torch.softmax(logits.view(-1, logits.size(-1)), dim=-1)\n",
    "            mean_probs = token_probs.mean(dim=0)\n",
    "            \n",
    "            # Calculate negative entropy of this distribution\n",
    "            # Lower entropy = more concentrated on few tokens = bad\n",
    "            # Higher entropy = more diverse vocabulary = good\n",
    "            eps = 1e-8  # For numerical stability\n",
    "            entropy = -torch.sum(mean_probs * torch.log(mean_probs + eps))\n",
    "            coverage_loss = 1.0 / (entropy + eps)  # Inverse of entropy\n",
    "        \n",
    "        # Combine losses with weights\n",
    "        total_loss = (\n",
    "            weights[\"ce_loss\"] * ce_loss + \n",
    "            weights[\"consistency_loss\"] * consistency_loss + \n",
    "            weights[\"coverage_loss\"] * coverage_loss\n",
    "        )\n",
    "        \n",
    "        # Store loss components for logging - use fields that won't conflict with HF\n",
    "        outputs.ce_loss_value = ce_loss\n",
    "        outputs.consistency_loss_value = consistency_loss\n",
    "        outputs.coverage_loss_value = coverage_loss\n",
    "        outputs.total_loss_value = total_loss\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def _evaluate(self, dataloader):\n",
    "        \"\"\"Evaluate model on dataloader\"\"\"\n",
    "        self.cot_generator.model.eval()\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Evaluation with tqdm progress bar\n",
    "        pbar = tqdm(dataloader, desc=\"Evaluation\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for batch in pbar:\n",
    "                # Move batch to device\n",
    "                input_ids = batch['input_ids'].to(self.config.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.config.device)\n",
    "                labels = batch['labels'].to(self.config.device)\n",
    "                visual_features = None\n",
    "                if batch.get('visual_features') is not None:\n",
    "                    visual_features = batch['visual_features'].to(self.config.device)\n",
    "                \n",
    "                # FIXED: Consistent with training, disable internal loss calculation\n",
    "                outputs = self.cot_generator.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=None,  # Pass None to prevent internal loss calculation\n",
    "                    visual_features=visual_features\n",
    "                )\n",
    "                \n",
    "                # Calculate improved loss\n",
    "                loss = self._calculate_improved_loss(outputs, labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        # Set back to training mode\n",
    "        self.cot_generator.model.train()\n",
    "        \n",
    "        return total_loss / len(dataloader)\n",
    "    \n",
    "    def _train_refiner(self, dataloader, epochs=None):\n",
    "        \"\"\"Train the transformer refiner with improved data preparation\"\"\"\n",
    "        if epochs is None:\n",
    "            epochs = self.config.refiner_epochs or 3\n",
    "            \n",
    "        # Create training examples with better sampling strategy\n",
    "        refiner_training_data = []\n",
    "        \n",
    "        # Sample batches for refiner training with progress bar\n",
    "        pbar = tqdm(dataloader, desc=\"Preparing refiner training data\", leave=True)\n",
    "        \n",
    "        for batch in pbar:\n",
    "            questions = batch['question']\n",
    "            steps_list = batch['steps']\n",
    "            \n",
    "            for question, steps in zip(questions, steps_list):\n",
    "                if len(steps) < 2:  # Skip examples with too few steps\n",
    "                    continue\n",
    "                    \n",
    "                # Create context from question\n",
    "                base_context = question\n",
    "                \n",
    "                # Use more steps for context to enable better refinement\n",
    "                for i in range(1, len(steps)):\n",
    "                    # Progressive context building (includes all previous steps)\n",
    "                    context = base_context\n",
    "                    for j in range(i):\n",
    "                        context += f\"\\nStep {j+1}: {steps[j]}\"\n",
    "                    \n",
    "                    # Add the current step to refine\n",
    "                    refiner_training_data.append({\n",
    "                        'context': context,\n",
    "                        'step': steps[i]\n",
    "                    })\n",
    "                    \n",
    "                # Balance the dataset by limiting examples per sample\n",
    "                if len(refiner_training_data) % 100 == 0:\n",
    "                    pbar.set_postfix({\"examples\": len(refiner_training_data)})\n",
    "                    \n",
    "            # Limit training data size but ensure diverse examples\n",
    "            if len(refiner_training_data) >= 2000:  # Increased from 1000\n",
    "                break\n",
    "        \n",
    "        # Shuffle the data to ensure diversity during training\n",
    "        random.shuffle(refiner_training_data)\n",
    "        \n",
    "        # Log data statistics\n",
    "        logger.info(f\"Created {len(refiner_training_data)} training examples for refiner\")\n",
    "        \n",
    "        # Train the refiner\n",
    "        logger.info(f\"Training refiner for {epochs} epochs\")\n",
    "        self.refiner.train(refiner_training_data, epochs=epochs)\n",
    "        \n",
    "        # Evaluate the refiner\n",
    "        eval_results = self._evaluate_refiner(dataloader)\n",
    "        \n",
    "        # More detailed logging\n",
    "        logger.info(f\"Refiner evaluation: Success rate = {eval_results['success_rate']:.1f}%, \"\n",
    "                    f\"Average improvement = {eval_results['average_improvement']:.3f}\")\n",
    "                    \n",
    "        # Log detailed examples if available\n",
    "        if 'detailed_results' in eval_results and len(eval_results['detailed_results']) > 0:\n",
    "            example = eval_results['detailed_results'][0]\n",
    "            logger.info(f\"Example refinement:\")\n",
    "            logger.info(f\"  Question: {example['question'][:100]}...\")\n",
    "            \n",
    "            for i, step_imp in enumerate(example['step_improvements'][:2]):\n",
    "                logger.info(f\"  Step {i+1} - Improved: {step_imp['improved']}\")\n",
    "                logger.info(f\"    Original: {step_imp['original'][:100]}...\")\n",
    "                logger.info(f\"    Refined:  {step_imp['refined'][:100]}...\")\n",
    "        \n",
    "        # Save refiner checkpoint\n",
    "        os.makedirs(os.path.join(self.config.checkpoint_dir, \"refiner\"), exist_ok=True)\n",
    "        self.refiner.save_model(\n",
    "            os.path.join(self.config.checkpoint_dir, \"refiner\")\n",
    "        )\n",
    "        return eval_results\n",
    "\n",
    "    def _evaluate_refiner(self, dataloader):\n",
    "        \"\"\"Evaluate refiner quality with improved sampling\"\"\"\n",
    "        # Sample a diverse set of examples from validation set\n",
    "        examples = []\n",
    "        \n",
    "        # Use tqdm for sampling with better strategy\n",
    "        pbar = tqdm(dataloader, desc=\"Sampling for refiner evaluation\", leave=False)\n",
    "        \n",
    "        # Track seen questions to ensure diversity\n",
    "        seen_questions = set()\n",
    "        \n",
    "        for batch in pbar:\n",
    "            questions = batch['question']\n",
    "            steps_list = batch['steps']\n",
    "            \n",
    "            for question, steps in zip(questions, steps_list):\n",
    "                # Skip if we've seen a very similar question already\n",
    "                question_key = question[:50]  # Use prefix as key\n",
    "                if question_key in seen_questions:\n",
    "                    continue\n",
    "                    \n",
    "                for question, steps in zip(questions, steps_list):\n",
    "                    has_enough_steps = len(steps) >= 2\n",
    "                    has_long_step = any(len(step) > 10 for step in steps)\n",
    "                    \n",
    "                    logger.info(f\"Example - has_enough_steps: {has_enough_steps}, has_long_step: {has_long_step}\")\n",
    "                    logger.info(f\"Steps count: {len(steps)}, Step lengths: {[len(step) for step in steps]}\")\n",
    "                \n",
    "                    if len(steps) >= 2 and any(len(step) > 10 for step in steps):\n",
    "\n",
    "                        examples.append({\n",
    "                            'question': question,\n",
    "                            'steps': steps\n",
    "                        })\n",
    "                        seen_questions.add(question_key)\n",
    "                    \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\"samples\": len(examples)})\n",
    "            \n",
    "            # Collect more examples for better evaluation\n",
    "            if len(examples) >= 20:  # Increased from 10\n",
    "                break\n",
    "                \n",
    "        logger.info(f\"Evaluating refiner on {len(examples)} examples\")\n",
    "\n",
    "        if len(examples) == 0:\n",
    "            logger.warning(\"No evaluation examples found. Check filtering criteria.\")\n",
    "            # Add at least one simple example for basic evaluation\n",
    "            if len(batch['steps']) > 0:\n",
    "                examples.append({\n",
    "                    'question': batch['question'][0],\n",
    "                    'steps': batch['steps'][0]\n",
    "                })\n",
    "        \n",
    "        # Evaluate refiner quality with improved metric\n",
    "        return self.refiner.evaluate_refinement(examples, self.reflection_module)\n",
    "    \n",
    "    def _rl_finetuning(self, dataloader, num_updates):\n",
    "        \"\"\"Perform RL fine-tuning\"\"\"\n",
    "        # Setup learning rate scheduler\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.ppo_trainer.optimizer,\n",
    "            num_warmup_steps=int(0.1 * num_updates),\n",
    "            num_training_steps=num_updates\n",
    "        )\n",
    "        \n",
    "        # RL training loop\n",
    "        global_step = 0\n",
    "        best_reward = float('-inf')\n",
    "        \n",
    "        logger.info(f\"Starting RL fine-tuning: {num_updates} updates\")\n",
    "        \n",
    "        # Create progress bar for RL updates\n",
    "        pbar = tqdm(total=num_updates, desc=\"RL Fine-tuning\", leave=True)\n",
    "        \n",
    "        while global_step < num_updates:\n",
    "            # Sample batch from dataloader\n",
    "            for batch in dataloader:\n",
    "                # Perform PPO update with refiner instead of GAN\n",
    "                metrics = self.ppo_trainer.train_step(batch, self.refiner)\n",
    "                \n",
    "                # Update learning rate\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Update progress bar\n",
    "                global_step += 1\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\n",
    "                    \"policy_loss\": f\"{metrics['policy_loss']:.4f}\", \n",
    "                    \"value_loss\": f\"{metrics['value_loss']:.4f}\", \n",
    "                    \"reward\": f\"{metrics['mean_reward']:.4f}\"\n",
    "                })\n",
    "                \n",
    "                # Log metrics periodically\n",
    "                if global_step % 10 == 0:\n",
    "                    logger.info(\n",
    "                        f\"RL step {global_step}/{num_updates}: \"\n",
    "                        f\"Policy loss = {metrics['policy_loss']:.4f}, \"\n",
    "                        f\"Value loss = {metrics['value_loss']:.4f}, \"\n",
    "                        f\"Mean reward = {metrics['mean_reward']:.4f}\"\n",
    "                    )\n",
    "                \n",
    "                # Save best model\n",
    "                if metrics['mean_reward'] > best_reward:\n",
    "                    best_reward = metrics['mean_reward']\n",
    "                    self.cot_generator.model.save_pretrained(\n",
    "                        os.path.join(self.config.checkpoint_dir, \"best_rl_model\")\n",
    "                    )\n",
    "                    logger.info(f\"New best reward: {best_reward:.4f} - Saved model\")\n",
    "                \n",
    "                # Evaluate periodically\n",
    "                if global_step % 100 == 0:\n",
    "                    eval_reward = self._evaluate_rl()\n",
    "                    logger.info(f\"RL evaluation reward: {eval_reward:.4f}\")\n",
    "                    pbar.set_postfix({\n",
    "                        \"policy_loss\": f\"{metrics['policy_loss']:.4f}\", \n",
    "                        \"value_loss\": f\"{metrics['value_loss']:.4f}\", \n",
    "                        \"reward\": f\"{metrics['mean_reward']:.4f}\",\n",
    "                        \"eval\": f\"{eval_reward:.4f}\"\n",
    "                    })\n",
    "                \n",
    "                # Check if we've reached the target number of updates\n",
    "                if global_step >= num_updates:\n",
    "                    break\n",
    "        \n",
    "        # Close the progress bar\n",
    "        pbar.close()\n",
    "        logger.info(f\"RL fine-tuning complete: Best reward = {best_reward:.4f}\")\n",
    "    \n",
    "    def _evaluate_rl(self, num_samples=10):\n",
    "        \"\"\"Evaluate current policy\"\"\"\n",
    "        self.cot_generator.model.eval()\n",
    "        total_reward = 0.0\n",
    "        \n",
    "        # Sample examples for evaluation\n",
    "        examples = []\n",
    "        temp_data = self.self_trainer.generate_pseudo_labels(\n",
    "            [{'question': item} for item in self.config.eval_questions]\n",
    "        )\n",
    "        data_loader = DataLoader(temp_data, batch_size=1)\n",
    "        \n",
    "        # Use tqdm for sampling\n",
    "        pbar = tqdm(data_loader, desc=\"Sampling for RL evaluation\", leave=False)\n",
    "        for batch in pbar:\n",
    "            examples.append(batch)\n",
    "            if len(examples) >= num_samples:\n",
    "                break\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\"samples\": len(examples)})\n",
    "        \n",
    "        # Evaluate on examples with progress bar\n",
    "        eval_pbar = tqdm(examples, desc=\"RL Evaluation\", leave=False)\n",
    "        for example in eval_pbar:\n",
    "            # Generate reasoning\n",
    "            sample = self.cot_generator.generate_step_by_step(example['question'][0])\n",
    "            \n",
    "            # Calculate reward\n",
    "            reward = self.reward_function.calculate_combined_reward(\n",
    "                sample,\n",
    "                example['answer'][0]\n",
    "            )\n",
    "            \n",
    "            total_reward += reward['combined']\n",
    "            \n",
    "            # Update progress bar\n",
    "            eval_pbar.set_postfix({\"reward\": f\"{reward['combined']:.4f}\"})\n",
    "        \n",
    "        # Set back to training mode\n",
    "        self.cot_generator.model.train()\n",
    "        \n",
    "        # Calculate average reward\n",
    "        avg_reward = total_reward / len(examples)\n",
    "        return avg_reward\n",
    "    \n",
    "    def generate(self, question, image=None):\n",
    "        \"\"\"Generate reasoning for a question\"\"\"\n",
    "        logger.info(f\"Generating reasoning for: {question}\")\n",
    "        \n",
    "        # Lookup relevant information\n",
    "        logger.info(\"Retrieving context information...\")\n",
    "        retrieved_info = self.retrieval_module.retrieve(question)\n",
    "        \n",
    "        # Generate initial chain-of-thought\n",
    "        logger.info(\"Generating initial chain-of-thought...\")\n",
    "        result = self.cot_generator.generate_step_by_step(\n",
    "            question, \n",
    "            image=image,\n",
    "            retrieved_context=retrieved_info\n",
    "        )\n",
    "        \n",
    "        # Apply refiner\n",
    "        logger.info(\"Applying transformer refiner...\")\n",
    "        refined_steps = self.refiner.refine_reasoning_steps(\n",
    "            question,\n",
    "            result['steps']\n",
    "        )\n",
    "        result['refined_steps'] = refined_steps\n",
    "        \n",
    "        # Evaluate if refinement is better\n",
    "        logger.info(\"Evaluating refinement quality...\")\n",
    "        original_score = self.reflection_module.evaluate_reasoning(\n",
    "            question,\n",
    "            result['steps']\n",
    "        )['scores']['overall_score']\n",
    "        \n",
    "        refined_score = self.reflection_module.evaluate_reasoning(\n",
    "            question,\n",
    "            refined_steps\n",
    "        )['scores']['overall_score']\n",
    "        \n",
    "        # Use refined steps if better\n",
    "        if refined_score > original_score:\n",
    "            logger.info(f\"Using refined steps (score improved: {original_score:.2f} -> {refined_score:.2f})\")\n",
    "            result['steps'] = refined_steps\n",
    "            result['used_refinement'] = True\n",
    "        else:\n",
    "            logger.info(f\"Keeping original steps (refinement score: {refined_score:.2f} vs original: {original_score:.2f})\")\n",
    "        \n",
    "        # Get final reflection\n",
    "        logger.info(\"Generating final reflection...\")\n",
    "        reflection = self.reflection_module.evaluate_reasoning(\n",
    "            question,\n",
    "            result['steps']\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Reasoning generation complete\")\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'steps': result['steps'],\n",
    "            'answer': result['answer'],\n",
    "            'reflection': reflection,\n",
    "            'retrieved_context': retrieved_info\n",
    "        }\n",
    "    \n",
    "    def analyze_labels(self, dataloader, num_batches=5):\n",
    "        \"\"\"Analyze label distribution in the dataset\"\"\"\n",
    "        label_stats = {\n",
    "            \"total_labels\": 0,\n",
    "            \"non_ignored\": 0,\n",
    "            \"min_value\": float('inf'),\n",
    "            \"max_value\": -float('inf'),\n",
    "            \"label_counts\": {},\n",
    "            \"avg_length\": 0,\n",
    "            \"vocab_size\": len(self.tokenizer)\n",
    "        }\n",
    "        \n",
    "        print(f\"Analyzing labels from {num_batches} batches...\")\n",
    "        \n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "                \n",
    "            labels = batch['labels'].numpy().flatten()\n",
    "            \n",
    "            # Update statistics\n",
    "            label_stats[\"total_labels\"] += labels.size\n",
    "            label_stats[\"non_ignored\"] += (labels != -100).sum()\n",
    "            \n",
    "            # Update min/max (excluding -100)\n",
    "            valid_labels = labels[labels != -100]\n",
    "            if valid_labels.size > 0:\n",
    "                label_stats[\"min_value\"] = min(label_stats[\"min_value\"], valid_labels.min())\n",
    "                label_stats[\"max_value\"] = max(label_stats[\"max_value\"], valid_labels.max())\n",
    "                \n",
    "            # Count values\n",
    "            unique, counts = np.unique(labels, return_counts=True)\n",
    "            for val, count in zip(unique, counts):\n",
    "                if val not in label_stats[\"label_counts\"]:\n",
    "                    label_stats[\"label_counts\"][val] = 0\n",
    "                label_stats[\"label_counts\"][val] += count\n",
    "                \n",
    "            # Track average length of non-ignored sequences\n",
    "            for l in batch['labels']:\n",
    "                non_ignored_length = (l != -100).sum().item()\n",
    "                label_stats[\"avg_length\"] += non_ignored_length\n",
    "        \n",
    "        # Calculate average\n",
    "        if i > 0:\n",
    "            label_stats[\"avg_length\"] /= (i * batch['labels'].size(0))\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\nLabel Analysis Results:\")\n",
    "        print(f\"Vocabulary size: {label_stats['vocab_size']}\")\n",
    "        print(f\"Total labels: {label_stats['total_labels']}\")\n",
    "        print(f\"Non-ignored labels: {label_stats['non_ignored']} ({label_stats['non_ignored']/label_stats['total_labels']*100:.2f}%)\")\n",
    "        print(f\"Min value (excluding -100): {label_stats['min_value']}\")\n",
    "        print(f\"Max value: {label_stats['max_value']}\")\n",
    "        print(f\"Average non-ignored length: {label_stats['avg_length']:.1f} tokens\")\n",
    "        \n",
    "        # Check for out of vocabulary indices\n",
    "        if label_stats[\"max_value\"] >= label_stats[\"vocab_size\"]:\n",
    "            print(\"\\n⚠️ WARNING: Some labels are outside the vocabulary range!\")\n",
    "            print(f\"Max label value ({label_stats['max_value']}) >= Vocabulary size ({label_stats['vocab_size']})\")\n",
    "        \n",
    "        # Value distribution excluding -100\n",
    "        print(\"\\nTop label values (excluding -100):\")\n",
    "        value_counts = {k: v for k, v in label_stats[\"label_counts\"].items() if k != -100}\n",
    "        top_values = sorted(value_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        for val, count in top_values:\n",
    "            token = self.tokenizer.decode([val])\n",
    "            print(f\"  {val} ('{token}'): {count} occurrences\")\n",
    "        \n",
    "        return label_stats\n",
    "    \n",
    "    def verify_loss_calculation(self):\n",
    "        \"\"\"Verify loss calculation with a simple input\"\"\"\n",
    "        print(\"\\nVerifying loss calculation with test input...\")\n",
    "        \n",
    "        # Create a simple input\n",
    "        text = \"This is a test sentence.\"\n",
    "        encoding = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        input_ids = encoding[\"input_ids\"].to(self.config.device)\n",
    "        attention_mask = encoding[\"attention_mask\"].to(self.config.device)\n",
    "        \n",
    "        # Create labels (shift input_ids right by one)\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        # Check if EOS token exists, use PAD token or a default token if not\n",
    "        eos_token_id = self.tokenizer.eos_token_id\n",
    "        if eos_token_id is None:\n",
    "            # Try pad token, or use a default token ID (usually 0 or 1)\n",
    "            eos_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else 0\n",
    "            print(f\"Warning: No EOS token found. Using alternative token ID: {eos_token_id}\")\n",
    "        \n",
    "        # Now create the shifted labels\n",
    "        labels = torch.cat([labels[:, 1:], torch.tensor([[eos_token_id]]).to(self.config.device)], dim=1)\n",
    "        \n",
    "        # Show the tokens and labels\n",
    "        print(\"Input tokens:\", self.tokenizer.convert_ids_to_tokens(input_ids[0]))\n",
    "        print(\"Label tokens:\", self.tokenizer.convert_ids_to_tokens(labels[0]))\n",
    "        \n",
    "        # Forward pass\n",
    "        self.cot_generator.model.train()  # Ensure in training mode\n",
    "        outputs = self.cot_generator(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        # Calculate our improved loss\n",
    "        loss = self._calculate_improved_loss(outputs, labels)\n",
    "        \n",
    "        print(f\"Test loss calculation result: {loss.item()}\")\n",
    "        \n",
    "        if loss.item() == 0:\n",
    "            print(\"⚠️ WARNING: Loss is still zero on test input!\")\n",
    "            return False\n",
    "        else:\n",
    "            print(\"✓ Loss calculation is working properly!\")\n",
    "            \n",
    "        # Check logits shape and activation\n",
    "        logits = outputs.logits\n",
    "        print(f\"Logits shape: {logits.shape}\")\n",
    "        print(f\"Logits mean: {logits.mean().item()}\")\n",
    "        print(f\"Logits std: {logits.std().item()}\")\n",
    "        \n",
    "        # Check predictions for expected tokens\n",
    "        for i in range(min(3, input_ids.shape[1])):\n",
    "            next_token_logits = logits[0, i, :]\n",
    "            top_tokens = torch.topk(next_token_logits, 5)\n",
    "            print(f\"\\nTop predictions for position {i} (input: '{self.tokenizer.decode([input_ids[0, i]])}'):\")\n",
    "            for token_id, score in zip(top_tokens.indices, top_tokens.values):\n",
    "                token = self.tokenizer.decode([token_id])\n",
    "                print(f\"  {token} (ID: {token_id}): {score.item():.4f}\")\n",
    "        \n",
    "        return loss.item() > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/10/2025 21:23:19 - INFO - __main__ - JUPYTER NOTEBOOK LOGGING INITIALIZED\n",
      "03/10/2025 21:23:19 - INFO - __main__ - Using device: mps\n",
      "03/10/2025 21:23:19 - INFO - __main__ - Model: bert-base-uncased\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logger created. Check both console output above and the log file in your output directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChainOfThoughtGenerator initialized with:\n",
      "  Tokenizer: bert-base-uncased\n",
      "  Model: bert-base-uncased\n",
      "  Vision Model: openai/clip-vit-base-patch32\n",
      "  Device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/10/2025 21:23:31 - INFO - __main__ - Loaded 100 exemplars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from cache: checkpoints/cached_t5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "/Users/Viku/GitHub/ReCoT/.venv/lib/python3.10/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "03/10/2025 21:23:33 - INFO - __main__ - Loading ScienceQA data from /Users/Viku/Datasets/ScienceQA/train/train.json\n",
      "03/10/2025 21:23:59 - INFO - __main__ - Processed 12726 ScienceQA examples\n",
      "03/10/2025 21:23:59 - INFO - __main__ - Loading ScienceQA data from /Users/Viku/Datasets/ScienceQA/val/val.json\n",
      "03/10/2025 21:24:04 - INFO - __main__ - Processed 4241 ScienceQA examples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging labels in the dataset:\n",
      "Sample 0: Non-ignored labels: 29/512 (5.66%)\n",
      "\n",
      "=== SAMPLE 0 ===\n",
      "Total length: 512\n",
      "Non-padded tokens: 48\n",
      "Padded tokens: 464\n",
      "Non-ignored labels: 29\n",
      "Non-ignored percentage: 5.66%\n",
      "Non-ignored / Non-padded ratio: 60.42%\n",
      "\n",
      "Sample tokens (first 10):\n",
      "Pos 0: Input='[CLS]' | Label='IGNORED' | Mask=1\n",
      "Pos 1: Input='let' | Label='[CLS]' | Mask=1\n",
      "Pos 2: Input=''' | Label='let' | Mask=1\n",
      "Pos 3: Input='s' | Label=''' | Mask=1\n",
      "Pos 4: Input='think' | Label='s' | Mask=1\n",
      "Pos 5: Input='step' | Label='think' | Mask=1\n",
      "Pos 6: Input='by' | Label='IGNORED' | Mask=1\n",
      "Pos 7: Input='step' | Label='by' | Mask=1\n",
      "Pos 8: Input='!' | Label='IGNORED' | Mask=1\n",
      "Pos 9: Input='context' | Label='!' | Mask=1\n",
      "\n",
      "Ignored label positions:\n",
      "[0, 6, 8, 10, 12, 14, 16, 18, 20, 22] ... [502, 503, 504, 505, 506, 507, 508, 509, 510, 511]\n",
      "Sample 1: Non-ignored labels: 49/512 (9.57%)\n",
      "\n",
      "=== SAMPLE 1 ===\n",
      "Total length: 512\n",
      "Non-padded tokens: 88\n",
      "Padded tokens: 424\n",
      "Non-ignored labels: 49\n",
      "Non-ignored percentage: 9.57%\n",
      "Non-ignored / Non-padded ratio: 55.68%\n",
      "\n",
      "Sample tokens (first 10):\n",
      "Pos 0: Input='[CLS]' | Label='IGNORED' | Mask=1\n",
      "Pos 1: Input='let' | Label='[CLS]' | Mask=1\n",
      "Pos 2: Input=''' | Label='let' | Mask=1\n",
      "Pos 3: Input='s' | Label=''' | Mask=1\n",
      "Pos 4: Input='think' | Label='s' | Mask=1\n",
      "Pos 5: Input='step' | Label='think' | Mask=1\n",
      "Pos 6: Input='by' | Label='IGNORED' | Mask=1\n",
      "Pos 7: Input='step' | Label='by' | Mask=1\n",
      "Pos 8: Input='!' | Label='IGNORED' | Mask=1\n",
      "Pos 9: Input='context' | Label='!' | Mask=1\n",
      "\n",
      "Ignored label positions:\n",
      "[0, 6, 8, 10, 12, 14, 16, 18, 20, 22] ... [502, 503, 504, 505, 506, 507, 508, 509, 510, 511]\n",
      "Sample 2: Non-ignored labels: 55/512 (10.74%)\n",
      "\n",
      "=== SAMPLE 2 ===\n",
      "Total length: 512\n",
      "Non-padded tokens: 101\n",
      "Padded tokens: 411\n",
      "Non-ignored labels: 55\n",
      "Non-ignored percentage: 10.74%\n",
      "Non-ignored / Non-padded ratio: 54.46%\n",
      "\n",
      "Sample tokens (first 10):\n",
      "Pos 0: Input='[CLS]' | Label='IGNORED' | Mask=1\n",
      "Pos 1: Input='let' | Label='[CLS]' | Mask=1\n",
      "Pos 2: Input=''' | Label='let' | Mask=1\n",
      "Pos 3: Input='s' | Label=''' | Mask=1\n",
      "Pos 4: Input='think' | Label='s' | Mask=1\n",
      "Pos 5: Input='step' | Label='think' | Mask=1\n",
      "Pos 6: Input='by' | Label='IGNORED' | Mask=1\n",
      "Pos 7: Input='step' | Label='by' | Mask=1\n",
      "Pos 8: Input='!' | Label='IGNORED' | Mask=1\n",
      "Pos 9: Input='context' | Label='!' | Mask=1\n",
      "\n",
      "Ignored label positions:\n",
      "[0, 6, 8, 10, 12, 14, 16, 18, 20, 22] ... [502, 503, 504, 505, 506, 507, 508, 509, 510, 511]\n",
      "\n",
      "Verifying loss calculation with test input...\n",
      "Input tokens: ['[CLS]', 'this', 'is', 'a', 'test', 'sentence', '.', '[SEP]']\n",
      "Label tokens: ['this', 'is', 'a', 'test', 'sentence', '.', '[SEP]', '[EOS]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "03/10/2025 21:24:04 - INFO - __main__ - Skipping supervised fine-tuning -- as already completed\n",
      "03/10/2025 21:24:04 - INFO - __main__ - Training transformer refiner\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss calculation result: 9.087302207946777\n",
      "✓ Loss calculation is working properly!\n",
      "Logits shape: torch.Size([1, 8, 30524])\n",
      "Logits mean: -7.146275520324707\n",
      "Logits std: 2.88424015045166\n",
      "\n",
      "Top predictions for position 0 (input: '[CLS]'):\n",
      "  and (ID: 1998): 7.7626\n",
      "  of (ID: 1997): 6.7241\n",
      "  in (ID: 1999): 6.5357\n",
      "  , (ID: 1010): 6.2768\n",
      "  . (ID: 1012): 6.2642\n",
      "\n",
      "Top predictions for position 1 (input: 'this'):\n",
      "  and (ID: 1998): 7.9312\n",
      "  is (ID: 2003): 5.9540\n",
      "  in (ID: 1999): 5.9331\n",
      "  , (ID: 1010): 5.6260\n",
      "  with (ID: 2007): 4.9781\n",
      "\n",
      "Top predictions for position 2 (input: 'is'):\n",
      "  is (ID: 2003): 5.5263\n",
      "  and (ID: 1998): 4.9881\n",
      "  , (ID: 1010): 4.7009\n",
      "  act (ID: 2552): 4.6559\n",
      "  formed (ID: 2719): 4.5885\n",
      "Analyzing labels from 5 batches...\n",
      "\n",
      "Label Analysis Results:\n",
      "Vocabulary size: 30524\n",
      "Total labels: 10240\n",
      "Non-ignored labels: 704 (6.88%)\n",
      "Min value (excluding -100): 101\n",
      "Max value: 28519\n",
      "Average non-ignored length: 35.2 tokens\n",
      "\n",
      "Top label values (excluding -100):\n",
      "  1024 (':'): 49 occurrences\n",
      "  1996 ('the'): 37 occurrences\n",
      "  1006 ('('): 30 occurrences\n",
      "  1007 (')'): 29 occurrences\n",
      "  1010 (','): 27 occurrences\n",
      "  1005 ('''): 24 occurrences\n",
      "  2003 ('is'): 23 occurrences\n",
      "  1037 ('a'): 21 occurrences\n",
      "  101 ('[CLS]'): 20 occurrences\n",
      "  102 ('[SEP]'): 20 occurrences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Preparing refiner training data:  21%|██        | 666/3182 [00:00<00:01, 1997.50it/s, examples=1800]\n",
      "03/10/2025 21:24:04 - INFO - __main__ - Created 2001 training examples for refiner\n",
      "03/10/2025 21:24:04 - INFO - __main__ - Training refiner for 2 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2001 training examples...\n",
      "Created 2001 training examples with meaningful improvements\n",
      "Starting training for 2 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training refiner (Epoch 1/2):   0%|          | 0/251 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Training refiner (Epoch 1/2): 100%|██████████| 251/251 [01:39<00:00,  2.52it/s, loss=0.1449]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: Average Loss = 1.7976\n",
      "New best loss: 1.7976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training refiner (Epoch 2/2): 100%|██████████| 251/251 [01:36<00:00,  2.59it/s, loss=0.0092]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: Average Loss = 0.0343\n",
      "New best loss: 0.0343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling for refiner evaluation:  14%|█▍        | 452/3182 [00:00<00:02, 1066.97it/s, samples=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 2: Non-ignored labels: 55/512 (10.74%)\n",
      "Sample 0: Non-ignored labels: 29/512 (5.66%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling for refiner evaluation:  69%|██████▉   | 2209/3182 [00:02<00:00, 1079.86it/s, samples=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: Non-ignored labels: 49/512 (9.57%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/10/2025 21:27:24 - INFO - __main__ - Evaluating refiner on 0 examples                         \n",
      "03/10/2025 21:27:24 - WARNING - __main__ - No evaluation examples found. Check filtering criteria.\n",
      "03/10/2025 21:27:29 - INFO - __main__ - Refiner evaluation: Success rate = 100.0%, Average improvement = 1.000\n",
      "03/10/2025 21:27:29 - INFO - __main__ - Example refinement:\n",
      "03/10/2025 21:27:29 - INFO - __main__ -   Question: Context: \n",
      "Question: According to the passage, which statement is true?\n",
      "Choices: (A) The Senate is a ...\n",
      "03/10/2025 21:27:29 - INFO - __main__ -   Step 1 - Improved: True\n",
      "03/10/2025 21:27:29 - INFO - __main__ -     Original: ...\n",
      "03/10/2025 21:27:29 - INFO - __main__ -     Refined:  this means that  this result follows from the principles established in previous steps and is a dire...\n",
      "03/10/2025 21:27:29 - INFO - __main__ -   Step 2 - Improved: True\n",
      "03/10/2025 21:27:29 - INFO - __main__ -     Original: ...\n",
      "03/10/2025 21:27:29 - INFO - __main__ -     Refined:  this means that  this result follows from the principles established in previous steps and is a dire...\n",
      "03/10/2025 21:27:30 - INFO - __main__ - Starting RL fine-tuning\n",
      "03/10/2025 21:27:30 - INFO - __main__ - Starting RL fine-tuning: 1000 updates\n",
      "RL Fine-tuning:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m ReasoningPipeline(config, bert_model, bert_tokenizer, vision_processor)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Train pipeline\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m trained_components \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_rl_updates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrl_updates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_self_training_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_training_iterations\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Evaluate final model\u001b[39;00m\n\u001b[1;32m     63\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating final model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 127\u001b[0m, in \u001b[0;36mReasoningPipeline.train\u001b[0;34m(self, train_file, val_file, num_rl_updates, num_self_training_iterations)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# 3. RL fine-tuning\u001b[39;00m\n\u001b[1;32m    126\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting RL fine-tuning\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rl_finetuning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rl_updates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# 4. Self-training loop\u001b[39;00m\n\u001b[1;32m    130\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting self-training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 652\u001b[0m, in \u001b[0;36mReasoningPipeline._rl_finetuning\u001b[0;34m(self, dataloader, num_updates)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m global_step \u001b[38;5;241m<\u001b[39m num_updates:\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;66;03m# Sample batch from dataloader\u001b[39;00m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;66;03m# Perform PPO update with refiner instead of GAN\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mppo_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefiner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    654\u001b[0m         \u001b[38;5;66;03m# Update learning rate\u001b[39;00m\n\u001b[1;32m    655\u001b[0m         scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[12], line 145\u001b[0m, in \u001b[0;36mPPOTrainer.train_step\u001b[0;34m(self, batch, refiner)\u001b[0m\n\u001b[1;32m    143\u001b[0m generated_samples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(questions)):\n\u001b[0;32m--> 145\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcot_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_step_by_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# Ensure answer is a string\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m], torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "Cell \u001b[0;32mIn[8], line 116\u001b[0m, in \u001b[0;36mChainOfThoughtGenerator.generate_step_by_step\u001b[0;34m(self, question, image, retrieved_context, num_steps, max_length)\u001b[0m\n\u001b[1;32m    113\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(step_prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 116\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Generate just the next step\u001b[39;49;00m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    121\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m step_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(step_output[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;28mlen\u001b[39m(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]):], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    124\u001b[0m current_prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m step_text\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2215\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2216\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2217\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2218\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2219\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2220\u001b[0m     )\n\u001b[1;32m   2222\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2223\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2236\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2237\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2242\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2243\u001b[0m     )\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:3214\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3212\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3214\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3216\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3217\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3218\u001b[0m     outputs,\n\u001b[1;32m   3219\u001b[0m     model_kwargs,\n\u001b[1;32m   3220\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3221\u001b[0m )\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1358\u001b[0m, in \u001b[0;36mBertLMHeadModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1356\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1358\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1374\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1375\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:515\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:440\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    438\u001b[0m )\n\u001b[0;32m--> 440\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    450\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create output directories\n",
    "# First, ensure the output directory exists\n",
    "if not os.path.exists(config.output_dir):\n",
    "    os.makedirs(config.output_dir)\n",
    "    print(f\"Created output directory: {config.output_dir}\")\n",
    "\n",
    "# Clear any existing handlers (important for Jupyter as cells can be re-run)\n",
    "logging.root.handlers = []\n",
    "\n",
    "# Configure logging with both file and console output\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(config.output_dir, \"notebook.log\")),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Get logger after configuration\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Test log entries\n",
    "logger.info(\"JUPYTER NOTEBOOK LOGGING INITIALIZED\")\n",
    "logger.info(f\"Using device: {config.device}\")\n",
    "logger.info(f\"Model: {config.model_name}\")\n",
    "\n",
    "# To verify logging is working correctly\n",
    "print(\"Logger created. Check both console output above and the log file in your output directory.\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Sample evaluation questions\n",
    "config.eval_questions = [\n",
    "    \"What happens when water boils?\",\n",
    "    \"How does gravity work?\",\n",
    "    \"Why does the moon have phases?\",\n",
    "    \"What is photosynthesis?\",\n",
    "    \"How do magnets work?\"\n",
    "]\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = ReasoningPipeline(config, bert_model, bert_tokenizer, vision_processor)\n",
    "\n",
    "# Train pipeline\n",
    "trained_components = pipeline.train(\n",
    "    config.train_path,\n",
    "    config.val_path,\n",
    "    num_rl_updates=config.rl_updates,\n",
    "    num_self_training_iterations=config.self_training_iterations\n",
    ")\n",
    "\n",
    "# Evaluate final model\n",
    "logger.info(\"Evaluating final model\")\n",
    "\n",
    "# Create test dataloader\n",
    "test_dataset = ScienceQADataset(\n",
    "    config.val_path,\n",
    "    pipeline.tokenizer,\n",
    "    pipeline.vision_processor,\n",
    "    config,\n",
    "    is_train=False\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Calculate validation metrics\n",
    "val_loss = pipeline._evaluate(test_dataloader)\n",
    "logger.info(f\"Final validation loss: {val_loss:.4f}\")\n",
    "\n",
    "# Generate examples for qualitative evaluation\n",
    "logger.info(\"Generating example outputs\")\n",
    "for question in config.eval_questions:\n",
    "    result = pipeline.generate(question)\n",
    "    logger.info(f\"Question: {question}\")\n",
    "    logger.info(f\"Steps:\")\n",
    "    for i, step in enumerate(result['steps']):\n",
    "        logger.info(f\" {i+1}. {step}\")\n",
    "    logger.info(f\"Answer: {result['answer']}\")\n",
    "    logger.info(f\"Reflection score: {result['reflection']['scores']['overall_score']:.2f}\")\n",
    "    logger.info(\"---\")\n",
    "\n",
    "logger.info(\"Training and evaluation complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
