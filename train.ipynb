{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Viku/GitHub/ReCoT/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    AutoProcessor,\n",
    "    CLIPVisionModel,\n",
    "    AdamW, \n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoModel\n",
    ")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "import json\n",
    "import logging\n",
    "import faiss\n",
    "from PIL import Image\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed: int = 11):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MISTRAL_API_KEY\"]=\"jJAuJZkjVcy2ynUhan375sHNviHiBeJU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config class to hold hyperparameters\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Base Model\n",
    "        self.model_name = \"bert-base-uncased\"  # can be replaced with any suitable LLM\n",
    "        self.tokenizer_name = \"bert-base-uncased\"\n",
    "        \n",
    "        # Vision Model\n",
    "        self.vision_model_name = \"openai/clip-vit-base-patch32\"\n",
    "        \n",
    "        # ScienceQA Dataset\n",
    "        self.file_path = \"/Users/Viku/Datasets/ScienceQA\"\n",
    "        self.train_path = \"/Users/Viku/Datasets/ScienceQA/train/train.json\"\n",
    "        self.val_path = \"/Users/Viku/Datasets/ScienceQA/val/val.json\"\n",
    "        self.max_seq_length = 512\n",
    "        self.batch_size = 4\n",
    "        \n",
    "        # Training\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 0.01\n",
    "        self.epochs = 3\n",
    "        self.warmup_steps = 100\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.gradient_accumulation_steps = 8\n",
    "        \n",
    "        # RL Training\n",
    "        self.ppo_epochs = 4\n",
    "        self.reward_scale = 0.01\n",
    "        self.clip_param = 0.2\n",
    "        self.value_loss_coef = 0.5\n",
    "        self.entropy_coef = 0.01\n",
    "        \n",
    "        # GAN\n",
    "        self.gan_learning_rate = 2e-5\n",
    "        self.gan_weight_decay = 0.01\n",
    "        self.gan_batch_size = 16\n",
    "        self.gan_epochs = 2\n",
    "        \n",
    "        # Retrieval\n",
    "        self.retrieval_top_k = 3\n",
    "        self.embedding_dim = 768\n",
    "        \n",
    "        # Reflection\n",
    "        self.reflection_threshold = 0.7\n",
    "        \n",
    "        # Paths\n",
    "        self.output_dir = \"outputs/\"\n",
    "        self.checkpoint_dir = \"checkpoints/\"\n",
    "        self.exemplar_path = \"data/exemplars.json\"\n",
    "        \n",
    "        # Device\n",
    "        self.device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "\n",
    "        self.max_answer_length = 64\n",
    "        self.rl_updates = 1000\n",
    "        self.self_training_iterations = 3\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 12:26:31,148 - __main__ - INFO - Initializing BERT model: bert-base-uncased\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "2025-03-06 12:26:34,802 - __main__ - INFO - BERT model initialized successfully\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertLMHeadModel, AutoImageProcessor, BertConfig\n",
    "\n",
    "# Define the initialize_bert function\n",
    "def initialize_bert():\n",
    "    logger.info(f\"Initializing BERT model: {config.model_name}\")\n",
    "    \n",
    "    # First get the original config\n",
    "    bert_config = BertConfig.from_pretrained(config.model_name)\n",
    "    \n",
    "    # Set is_decoder=True\n",
    "    bert_config.is_decoder = True\n",
    "    \n",
    "    # Initialize tokenizer normally\n",
    "    tokenizer = BertTokenizer.from_pretrained(config.tokenizer_name)\n",
    "    \n",
    "    # Initialize model with this modified config\n",
    "    model = BertLMHeadModel.from_pretrained(\n",
    "        config.model_name, \n",
    "        config=bert_config,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    \n",
    "    vision_processor = AutoImageProcessor.from_pretrained(config.vision_model_name)\n",
    "    return model, tokenizer, vision_processor\n",
    "\n",
    "# Now call the function to initialize the models\n",
    "bert_model, bert_tokenizer, vision_processor = initialize_bert()\n",
    "logger.info(f\"BERT model initialized successfully\")\n",
    "\n",
    "# After initializing the model\n",
    "model_path = os.path.join(config.checkpoint_dir, \"bert_decoder\")\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "bert_model.save_pretrained(model_path)\n",
    "bert_tokenizer.save_pretrained(model_path)\n",
    "\n",
    "# Then reload it\n",
    "bert_model = BertLMHeadModel.from_pretrained(model_path)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScienceQADataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, vision_processor, config, is_train=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vision_processor = vision_processor\n",
    "        self.config = config\n",
    "        self.is_train = is_train\n",
    "        self.base_dir = os.path.dirname(file_path)  # Get directory containing the JSON file\n",
    "        self.data = self.load_and_preprocess_data(file_path)\n",
    "        \n",
    "    def load_and_preprocess_data(self, file_path):\n",
    "        logger.info(f\"Loading ScienceQA data from {file_path}\")\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        processed_data = []\n",
    "        for item in data:\n",
    "            # Extract fields specific to ScienceQA\n",
    "            question = item.get('question', '')\n",
    "            context = item.get('context', '')\n",
    "            choices = item.get('choices', [])\n",
    "            answer = item.get('answer', '')\n",
    "            explanation = item.get('explanation', '')\n",
    "            question_id = item.get('id', '')  # Get question ID for image path\n",
    "            \n",
    "            # Format choices as text\n",
    "            choices_text = \"\"\n",
    "            for i, choice in enumerate(choices):\n",
    "                choices_text += f\"({chr(65+i)}) {choice} \"\n",
    "            \n",
    "            # Combine context and question\n",
    "            full_question = f\"Context: {context}\\nQuestion: {question}\\nChoices: {choices_text}\"\n",
    "            \n",
    "            # Split explanation into reasoning steps\n",
    "            steps = self.extract_reasoning_steps(explanation)\n",
    "            \n",
    "            # Process image if available\n",
    "            visual_features = None\n",
    "            if 'image' in item and item['image']:\n",
    "                # Construct image path based on question ID in train/val folder structure\n",
    "                # Assuming question_id corresponds to the folder name\n",
    "                image_folder = os.path.join(self.base_dir, str(question_id))\n",
    "                image_files = [f for f in os.listdir(image_folder) if f.endswith('.png')] if os.path.exists(image_folder) else []\n",
    "                \n",
    "                if image_files:\n",
    "                    image_path = os.path.join(image_folder, image_files[0])\n",
    "                    try:\n",
    "                        image = Image.open(image_path).convert('RGB')\n",
    "                        visual_features = self.process_image(image)\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error processing image {image_path}: {e}\")\n",
    "            \n",
    "            # Tokenize question\n",
    "            question_tokens = self.tokenizer.encode(\n",
    "                \"Let's think step by step! \" + full_question, \n",
    "                add_special_tokens=True,\n",
    "                truncation=True,\n",
    "                max_length=self.config.max_seq_length // 2\n",
    "            )\n",
    "            \n",
    "            # Tokenize each step separately\n",
    "            steps_tokens = []\n",
    "            for step in steps:\n",
    "                step_tokens = self.tokenizer.encode(\n",
    "                    step,\n",
    "                    add_special_tokens=False,\n",
    "                    truncation=True,\n",
    "                    max_length=self.config.max_seq_length // (2 * max(1, len(steps)))\n",
    "                )\n",
    "                steps_tokens.append(step_tokens)\n",
    "            \n",
    "            # Tokenize answer\n",
    "            answer_tokens = self.tokenizer.encode(\n",
    "                f\"Therefore, the answer is {answer}\",\n",
    "                add_special_tokens=False,\n",
    "                truncation=True,\n",
    "                max_length=self.config.max_seq_length // 4\n",
    "            )\n",
    "            \n",
    "            processed_data.append({\n",
    "                'question': full_question,\n",
    "                'question_tokens': question_tokens,\n",
    "                'steps': steps,\n",
    "                'steps_tokens': steps_tokens,\n",
    "                'answer': answer,\n",
    "                'answer_tokens': answer_tokens,\n",
    "                'visual_features': visual_features,\n",
    "                'has_image': visual_features is not None\n",
    "            })\n",
    "        \n",
    "        logger.info(f\"Processed {len(processed_data)} ScienceQA examples\")\n",
    "        return processed_data\n",
    "    \n",
    "    def process_image(self, image):\n",
    "        \"\"\"Process image using CLIP vision encoder\"\"\"\n",
    "        print(\"Processing Image\")\n",
    "        inputs = self.vision_processor(images=image, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            vision_model = CLIPVisionModel.from_pretrained(self.config.vision_model_name)\n",
    "            vision_model.to(self.config.device)\n",
    "            outputs = vision_model(**{k: v.to(self.config.device) for k, v in inputs.items()})\n",
    "            visual_features = outputs.pooler_output.cpu().numpy()\n",
    "        return visual_features[0]  # Return the feature vector\n",
    "    \n",
    "    def extract_reasoning_steps(self, explanation):\n",
    "        \"\"\"Extract reasoning steps from explanation\"\"\"\n",
    "        # Method 1: Split by numbered steps if present\n",
    "        numbered_pattern = re.compile(r'\\d+\\.\\s+')\n",
    "        if numbered_pattern.search(explanation):\n",
    "            steps = [step.strip() for step in numbered_pattern.split(explanation) if step.strip()]\n",
    "            if steps and not steps[0][0].isdigit():  # Remove introduction if it doesn't start with a number\n",
    "                steps = steps[1:]\n",
    "            return steps or [explanation]\n",
    "        \n",
    "        # Method 2: Split by sentences assuming each sentence is a step\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', explanation)\n",
    "        if len(sentences) > 1:\n",
    "            return [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        # Default: Treat the whole explanation as one step\n",
    "        return [explanation]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Combine question, steps, and answer tokens for input\n",
    "        input_tokens = item['question_tokens'].copy()\n",
    "        for step_tokens in item['steps_tokens']:\n",
    "            input_tokens.extend(step_tokens)\n",
    "        input_tokens.extend(item['answer_tokens'])\n",
    "        \n",
    "        # Pad or truncate to max sequence length\n",
    "        if len(input_tokens) > self.config.max_seq_length:\n",
    "            input_tokens = input_tokens[:self.config.max_seq_length]\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = [1] * len(input_tokens)\n",
    "        padding_length = self.config.max_seq_length - len(input_tokens)\n",
    "        input_tokens.extend([self.tokenizer.pad_token_id] * padding_length)\n",
    "        attention_mask.extend([0] * padding_length)\n",
    "        \n",
    "        # Create labels for language modeling (shift right)\n",
    "        # We don't compute loss for the question part\n",
    "        labels = [-100] * len(item['question_tokens'])\n",
    "        for step_tokens in item['steps_tokens']:\n",
    "            labels.extend(step_tokens)\n",
    "        labels.extend(item['answer_tokens'])\n",
    "        \n",
    "        # Pad labels\n",
    "        if len(labels) > self.config.max_seq_length:\n",
    "            labels = labels[:self.config.max_seq_length]\n",
    "        labels.extend([-100] * (self.config.max_seq_length - len(labels)))\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_tokens, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long),\n",
    "            'question': item['question'],\n",
    "            'steps': item['steps'],\n",
    "            'answer': item['answer'],\n",
    "            'visual_features': torch.tensor(item['visual_features'], dtype=torch.float) if item['visual_features'] is not None else torch.zeros(768),\n",
    "            'has_image': item['has_image']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Base LLM & Chain-of-Thought Generator\n",
    "\n",
    "class ChainOfThoughtGenerator(nn.Module):\n",
    "    def __init__(self, config, model=None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "        else:\n",
    "            # Initialize model from config\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(config.model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n",
    "        \n",
    "        # Add special tokens if needed\n",
    "        special_tokens = {\"pad_token\": \"[PAD]\"} if self.tokenizer.pad_token is None else {}\n",
    "        if special_tokens:\n",
    "            self.tokenizer.add_special_tokens(special_tokens)\n",
    "        \n",
    "        # Load the base language model\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(config.model_name)\n",
    "        if special_tokens:\n",
    "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "        # Vision encoder for multimodal inputs\n",
    "        self.vision_processor = AutoProcessor.from_pretrained(config.vision_model_name)\n",
    "        self.vision_model = CLIPVisionModel.from_pretrained(config.vision_model_name)\n",
    "        \n",
    "        # Vision-language integration layer\n",
    "        self.vision_projection = nn.Linear(\n",
    "            self.vision_model.config.hidden_size,\n",
    "            self.model.config.hidden_size\n",
    "        )\n",
    "        \n",
    "        # Move models to device\n",
    "        self.model.to(config.device)\n",
    "        self.vision_model.to(config.device)\n",
    "        self.vision_projection.to(config.device)\n",
    "\n",
    "        print(\"ChainOfThoughtGenerator initialized with:\")\n",
    "        print(f\"  Tokenizer: {config.tokenizer_name}\")\n",
    "        print(f\"  Model: {config.model_name}\")\n",
    "        print(f\"  Vision Model: {config.vision_model_name}\")\n",
    "        print(f\"  Device: {config.device}\")\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        \"\"\"Encode image using vision model\"\"\"\n",
    "        print(\"Encoding image...\")\n",
    "        vision_inputs = self.vision_processor(images=image, return_tensors=\"pt\")\n",
    "        vision_inputs = {k: v.to(self.config.device) for k, v in vision_inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            vision_outputs = self.vision_model(**vision_inputs)\n",
    "            image_features = vision_outputs.pooler_output\n",
    "            projected_features = self.vision_projection(image_features)\n",
    "        \n",
    "        print(\"Image encoding completed.\")\n",
    "        return projected_features\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None, visual_features=None):\n",
    "        # \"\"\"Forward pass with optional visual features\"\"\"\n",
    "        # print(\"Forward pass started.\")\n",
    "        # print(f\"  Input IDs: {input_ids.shape}\")\n",
    "        # print(f\"  Attention Mask: {attention_mask.shape}\")\n",
    "        # if labels is not None:\n",
    "        #     print(f\"  Labels: {labels.shape}\")\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        # print(\"Model forward pass completed.\")\n",
    "\n",
    "        # If visual features are available, enhance the hidden states\n",
    "        if visual_features is not None:\n",
    "            # print(\"Processing visual features...\")\n",
    "            if hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:\n",
    "                projected_visual = self.vision_projection(visual_features)\n",
    "                last_hidden = outputs.hidden_states[-1]\n",
    "                enhanced_hidden = last_hidden + projected_visual.unsqueeze(1)\n",
    "                # print(\"Visual features integrated into hidden states.\")\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def generate_step_by_step(self, question, image=None, num_steps=5, max_length=512):\n",
    "        \"\"\"Generate a chain-of-thought reasoning process for a given question\"\"\"\n",
    "        print(f\"Generating reasoning for question: {question}\")\n",
    "\n",
    "        # Prepare input\n",
    "        prompt = f\"Let's think step by step! {question}\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.config.device) for k, v in inputs.items()}\n",
    "        \n",
    "        print(f\"Tokenized input: {inputs}\")\n",
    "\n",
    "        # Process image if provided\n",
    "        visual_embedding = None\n",
    "        if image is not None:\n",
    "            print(\"Processing image for reasoning...\")\n",
    "            visual_embedding = self.encode_image(image)\n",
    "\n",
    "        # Generate reasoning steps and answer\n",
    "        with torch.no_grad():\n",
    "            print(\"Generating response from model...\")\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.7,\n",
    "                do_sample=True\n",
    "            )\n",
    "        \n",
    "        # Decode the generated text\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Generated text: {generated_text}\")\n",
    "\n",
    "        # Extract reasoning steps and answer\n",
    "        generated_text = generated_text[len(prompt):]  # Remove the prompt\n",
    "        parts = generated_text.split(\"\\nTherefore, the answer is\")\n",
    "        \n",
    "        reasoning = parts[0]\n",
    "        answer = parts[1] if len(parts) > 1 else \"No clear answer provided.\"\n",
    "\n",
    "        # Split reasoning into steps\n",
    "        steps = []\n",
    "        for step in reasoning.split(\"\\n\"):\n",
    "            if step.strip():\n",
    "                steps.append(step.strip())\n",
    "\n",
    "        print(\"Generated reasoning steps:\")\n",
    "        for i, step in enumerate(steps):\n",
    "            print(f\"  Step {i+1}: {step}\")\n",
    "\n",
    "        print(f\"Final answer: {answer.strip()}\")\n",
    "\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"steps\": steps,\n",
    "            \"answer\": answer.strip(),\n",
    "            \"full_text\": generated_text\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Reflection Module\n",
    "\n",
    "class ReflectionModule(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Use a smaller model for efficiency\n",
    "        self.encoder = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "        # Scoring layers\n",
    "        self.coherence_scorer = nn.Linear(self.encoder.config.hidden_size, 1)\n",
    "        self.language_scorer = nn.Linear(self.encoder.config.hidden_size, 1)\n",
    "        self.progress_scorer = nn.Linear(self.encoder.config.hidden_size, 1)\n",
    "        \n",
    "        # Tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "        # Move to device\n",
    "        self.encoder.to(config.device)\n",
    "        self.coherence_scorer.to(config.device)\n",
    "        self.language_scorer.to(config.device)\n",
    "        self.progress_scorer.to(config.device)\n",
    "    \n",
    "    def forward(self, question, steps, previous_steps=None):\n",
    "        \"\"\"\n",
    "        Evaluate the quality of reasoning steps\n",
    "        \n",
    "        Args:\n",
    "            question: The original question\n",
    "            steps: List of reasoning steps to evaluate\n",
    "            previous_steps: Optional previous steps for context\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of scores for each step and overall\n",
    "        \"\"\"\n",
    "        all_scores = []\n",
    "        \n",
    "        # Process each step\n",
    "        for i, step in enumerate(steps):\n",
    "            # Create context from question and previous steps\n",
    "            context = question\n",
    "            if previous_steps:\n",
    "                context += \" \" + \" \".join(previous_steps)\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = self.tokenizer(\n",
    "                context, \n",
    "                step, \n",
    "                truncation=True, \n",
    "                padding=True, \n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            inputs = {k: v.to(self.config.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Get embeddings\n",
    "            with torch.no_grad():\n",
    "                outputs = self.encoder(**inputs)\n",
    "                pooled_output = outputs.last_hidden_state[:, 0]  # Use [CLS] token\n",
    "            \n",
    "            # Calculate scores\n",
    "            coherence_score = torch.sigmoid(self.coherence_scorer(pooled_output)).item()\n",
    "            language_score = torch.sigmoid(self.language_scorer(pooled_output)).item()\n",
    "            progress_score = torch.sigmoid(self.progress_scorer(pooled_output)).item()\n",
    "            \n",
    "            # Calculate a combined score\n",
    "            combined_score = (coherence_score + language_score + progress_score) / 3\n",
    "            \n",
    "            all_scores.append({\n",
    "                'step': i+1,\n",
    "                'coherence': coherence_score,\n",
    "                'language': language_score,\n",
    "                'progress': progress_score,\n",
    "                'combined': combined_score\n",
    "            })\n",
    "            \n",
    "            # Update previous steps for next iteration\n",
    "            previous_steps = (previous_steps or []) + [step]\n",
    "        \n",
    "        # Calculate overall score\n",
    "        overall_score = sum(s['combined'] for s in all_scores) / len(all_scores) if all_scores else 0\n",
    "        \n",
    "        return {\n",
    "            'step_scores': all_scores,\n",
    "            'overall_score': overall_score\n",
    "        }\n",
    "    \n",
    "    def evaluate_reasoning(self, question, steps, answer=None):\n",
    "        \"\"\"Evaluate the overall reasoning process\"\"\"\n",
    "        step_scores = self.forward(question, steps)\n",
    "        \n",
    "        # Check if reasoning meets the threshold\n",
    "        meets_threshold = step_scores['overall_score'] >= self.config.reflection_threshold\n",
    "        \n",
    "        return {\n",
    "            'scores': step_scores,\n",
    "            'meets_threshold': meets_threshold,\n",
    "            'feedback': self.generate_feedback(step_scores) if not meets_threshold else None\n",
    "        }\n",
    "    \n",
    "    def generate_feedback(self, scores):\n",
    "        \"\"\"Generate feedback based on scores\"\"\"\n",
    "        feedback = []\n",
    "        \n",
    "        for step_score in scores['step_scores']:\n",
    "            step_num = step_score['step']\n",
    "            if step_score['coherence'] < 0.6:\n",
    "                feedback.append(f\"Step {step_num} lacks coherence with the context.\")\n",
    "            if step_score['language'] < 0.6:\n",
    "                feedback.append(f\"Step {step_num} has language issues.\")\n",
    "            if step_score['progress'] < 0.6:\n",
    "                feedback.append(f\"Step {step_num} doesn't make sufficient progress toward the answer.\")\n",
    "        \n",
    "        if not feedback:\n",
    "            feedback = [\"The reasoning needs improvement, but specific issues weren't identified.\"]\n",
    "        \n",
    "        return feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Retrieval Module\n",
    "\n",
    "class RetrievalModule:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.encoder = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        \n",
    "        # Move to device\n",
    "        self.encoder.to(config.device)\n",
    "        \n",
    "        # Load exemplars\n",
    "        self.exemplars = self.load_exemplars()\n",
    "        \n",
    "        # Build index for fast retrieval\n",
    "        self.index = self.build_index()\n",
    "    \n",
    "    def load_exemplars(self):\n",
    "        \"\"\"Load exemplar reasoning sequences\"\"\"\n",
    "        if not os.path.exists(self.config.exemplar_path):\n",
    "            logger.warning(f\"Exemplar file {self.config.exemplar_path} not found, using empty exemplars\")\n",
    "            return []\n",
    "        \n",
    "        with open(self.config.exemplar_path, 'r') as f:\n",
    "            exemplars = json.load(f)\n",
    "        \n",
    "        # Pre-compute embeddings for each exemplar\n",
    "        for exemplar in exemplars:\n",
    "            exemplar['embedding'] = self.encode_text(exemplar['question']).cpu().numpy()\n",
    "        \n",
    "        logger.info(f\"Loaded {len(exemplars)} exemplars\")\n",
    "        return exemplars\n",
    "    \n",
    "    def build_index(self):\n",
    "        \"\"\"Build FAISS index for fast retrieval\"\"\"\n",
    "        if not self.exemplars:\n",
    "            return None\n",
    "        \n",
    "        # Extract embeddings\n",
    "        embeddings = np.array([ex['embedding'] for ex in self.exemplars]).astype('float32')\n",
    "        \n",
    "        # Build index\n",
    "        index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        index.add(embeddings)\n",
    "        \n",
    "        return index\n",
    "    \n",
    "    def encode_text(self, text):\n",
    "        \"\"\"Encode text using the sentence transformer\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            text, \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {k: v.to(self.config.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.encoder(**inputs)\n",
    "            # Use mean pooling\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            mask = attention_mask.unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
    "            masked_embeddings = outputs.last_hidden_state * mask\n",
    "            summed = torch.sum(masked_embeddings, 1)\n",
    "            counts = torch.clamp(torch.sum(mask, 1), min=1e-9)\n",
    "            mean_pooled = summed / counts\n",
    "        \n",
    "        return mean_pooled[0]  # Return the embedding vector\n",
    "    \n",
    "    def retrieve_similar_examples(self, question, k=None):\n",
    "        \"\"\"Retrieve similar exemplars for a given question\"\"\"\n",
    "        if k is None:\n",
    "            k = self.config.retrieval_top_k\n",
    "        \n",
    "        if not self.exemplars or self.index is None:\n",
    "            return []\n",
    "        \n",
    "        # Encode the query\n",
    "        query_embedding = self.encode_text(question).cpu().numpy().reshape(1, -1).astype('float32')\n",
    "        \n",
    "        # Search for similar examples\n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        \n",
    "        # Get the exemplars\n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if idx < len(self.exemplars):\n",
    "                exemplar = self.exemplars[idx].copy()\n",
    "                exemplar['similarity'] = float(1.0 / (1.0 + distances[0][i]))  # Convert distance to similarity\n",
    "                exemplar.pop('embedding', None)  # Remove embedding from result\n",
    "                results.append(exemplar)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_demonstration_prompt(self, question, k=None):\n",
    "        \"\"\"Get a few-shot demonstration prompt based on retrieved examples\"\"\"\n",
    "        examples = self.retrieve_similar_examples(question, k)\n",
    "        \n",
    "        if not examples:\n",
    "            return f\"Let's think step by step! {question}\"\n",
    "        \n",
    "        prompt = \"I'll solve some similar problems step by step, then answer your question.\\n\\n\"\n",
    "        \n",
    "        # Add examples\n",
    "        for i, example in enumerate(examples):\n",
    "            prompt += f\"Example {i+1}:\\n\"\n",
    "            prompt += f\"Question: {example['question']}\\n\"\n",
    "            prompt += \"Reasoning:\\n\"\n",
    "            for j, step in enumerate(example['steps']):\n",
    "                prompt += f\"{j+1}. {step}\\n\"\n",
    "            prompt += f\"Answer: {example['answer']}\\n\\n\"\n",
    "        \n",
    "        # Add the current question\n",
    "        prompt += f\"Now, let's solve your question step by step!\\n{question}\\n\"\n",
    "        \n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(nn.Module):\n",
    "    \"\"\"Generator model for refining intermediate reasoning steps\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Small model for efficiency\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        self.model.to(config.device)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    def generate_refined_step(self, context, original_step=None, max_length=100):\n",
    "        \"\"\"Generate a refined reasoning step\"\"\"\n",
    "        # Prepare input\n",
    "        if original_step:\n",
    "            prompt = f\"{context}\\nRefined step: \"\n",
    "        else:\n",
    "            prompt = f\"{context}\\nNext step: \"\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.config.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate refined step\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_length=len(inputs[\"input_ids\"][0]) + max_length,\n",
    "            temperature=0.8,\n",
    "            top_p=0.92,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "        \n",
    "        # Decode the generated text\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        refined_step = generated_text[len(prompt):].strip()\n",
    "        \n",
    "        return refined_step\n",
    "\n",
    "class TextDiscriminator(nn.Module):\n",
    "    \"\"\"Discriminator model for evaluating reasoning steps\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Use a small pretrained model\n",
    "        self.encoder = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "        # Scoring layer\n",
    "        self.scorer = nn.Sequential(\n",
    "            nn.Linear(self.encoder.config.hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.encoder.to(config.device)\n",
    "        self.scorer.to(config.device)\n",
    "    \n",
    "    def forward(self, context, step):\n",
    "        \"\"\"Evaluate the quality of a reasoning step\"\"\"\n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            context, \n",
    "            step, \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {k: v.to(self.config.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = self.encoder(**inputs)\n",
    "            pooled_output = outputs.last_hidden_state[:, 0]  # Use [CLS] token\n",
    "        \n",
    "        # Calculate score\n",
    "        score = self.scorer(pooled_output)\n",
    "        \n",
    "        return score\n",
    "\n",
    "class GANModule:\n",
    "    \"\"\"GAN-based text refinement module\"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.generator = TextGenerator(config)\n",
    "        self.discriminator = TextDiscriminator(config)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.gen_optimizer = AdamW(\n",
    "            self.generator.parameters(),\n",
    "            lr=config.gan_learning_rate,\n",
    "            weight_decay=config.gan_weight_decay\n",
    "        )\n",
    "        self.disc_optimizer = AdamW(\n",
    "            self.discriminator.parameters(),\n",
    "            lr=config.gan_learning_rate,\n",
    "            weight_decay=config.gan_weight_decay\n",
    "        )\n",
    "    \n",
    "    def train_step(self, real_examples):\n",
    "        \"\"\"Single GAN training step\"\"\"\n",
    "        # Train discriminator\n",
    "        self.disc_optimizer.zero_grad()\n",
    "        \n",
    "        # Real examples\n",
    "        real_contexts = [ex['context'] for ex in real_examples]\n",
    "        real_steps = [ex['step'] for ex in real_examples]\n",
    "        real_scores = []\n",
    "        \n",
    "        for context, step in zip(real_contexts, real_steps):\n",
    "            real_score = self.discriminator(context, step)\n",
    "            real_scores.append(real_score)\n",
    "        \n",
    "        real_loss = sum([(1 - score) ** 2 for score in real_scores]) / len(real_scores)\n",
    "        \n",
    "        # Generated examples\n",
    "        fake_steps = []\n",
    "        for context in real_contexts:\n",
    "            with torch.no_grad():\n",
    "                fake_step = self.generator.generate_refined_step(context)\n",
    "                fake_steps.append(fake_step)\n",
    "        \n",
    "        fake_scores = []\n",
    "        for context, step in zip(real_contexts, fake_steps):\n",
    "            fake_score = self.discriminator(context, step)\n",
    "            fake_scores.append(fake_score)\n",
    "        \n",
    "        fake_loss = sum([score ** 2 for score in fake_scores]) / len(fake_scores)\n",
    "        \n",
    "        # Combine losses\n",
    "        disc_loss = real_loss + fake_loss\n",
    "        disc_loss.backward()\n",
    "        self.disc_optimizer.step()\n",
    "        \n",
    "        # Train generator\n",
    "        self.gen_optimizer.zero_grad()\n",
    "        \n",
    "        # Generate new fake examples\n",
    "        new_fake_steps = []\n",
    "        for context in real_contexts:\n",
    "            fake_step = self.generator.generate_refined_step(context)\n",
    "            new_fake_steps.append(fake_step)\n",
    "        \n",
    "        # Calculate generator loss\n",
    "        gen_scores = []\n",
    "        for context, step in zip(real_contexts, new_fake_steps):\n",
    "            gen_score = self.discriminator(context, step)\n",
    "            gen_scores.append(gen_score)\n",
    "        \n",
    "        gen_loss = sum([(1 - score) ** 2 for score in gen_scores]) / len(gen_scores)\n",
    "        gen_loss.backward()\n",
    "        self.gen_optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            'disc_loss': disc_loss.item(),\n",
    "            'gen_loss': gen_loss.item(),\n",
    "            'real_score_mean': sum([s.item() for s in real_scores]) / len(real_scores),\n",
    "            'fake_score_mean': sum([s.item() for s in fake_scores]) / len(fake_scores)\n",
    "        }\n",
    "    \n",
    "    def refine_reasoning_steps(self, question, original_steps):\n",
    "        \"\"\"Refine a sequence of reasoning steps\"\"\"\n",
    "        refined_steps = []\n",
    "        context = question\n",
    "        \n",
    "        for i, step in enumerate(original_steps):\n",
    "            # Context includes the question and previous steps\n",
    "            current_context = context\n",
    "            \n",
    "            # Generate refined step\n",
    "            refined_step = self.generator.generate_refined_step(current_context, step)\n",
    "            \n",
    "            # Evaluate original and refined\n",
    "            orig_score = self.discriminator(current_context, step).item()\n",
    "            refined_score = self.discriminator(current_context, refined_step).item()\n",
    "            \n",
    "            # Use the better step\n",
    "            if refined_score > orig_score:\n",
    "                refined_steps.append(refined_step)\n",
    "                context += f\"\\nStep {i+1}: {refined_step}\"\n",
    "            else:\n",
    "                refined_steps.append(step)\n",
    "                context += f\"\\nStep {i+1}: {step}\"\n",
    "        \n",
    "        return refined_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Dual Reward Function and RL Training\n",
    "\n",
    "class RewardFunction:\n",
    "    \"\"\"Combines outcome and process rewards for RL training\"\"\"\n",
    "    def __init__(self, reflection_module, config):\n",
    "        self.reflection_module = reflection_module\n",
    "        self.config = config\n",
    "        \n",
    "        # For outcome verification\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "    def calculate_outcome_reward(self, predicted_answer, ground_truth):\n",
    "        \"\"\"Calculate reward based on correctness of final answer\"\"\"\n",
    "        # For ScienceQA, we can do a simple match check\n",
    "        # In a real system, you might use more sophisticated answer verification\n",
    "        predicted_normalized = predicted_answer.strip().lower()\n",
    "        ground_truth_normalized = ground_truth.strip().lower()\n",
    "        \n",
    "        # Check for exact match\n",
    "        if predicted_normalized == ground_truth_normalized:\n",
    "            return 1.0\n",
    "        \n",
    "        # Check for partial match (for longer answers)\n",
    "        if len(ground_truth_normalized) > 10:\n",
    "            # Using simple token overlap as a metric\n",
    "            pred_tokens = set(self.tokenizer.tokenize(predicted_normalized))\n",
    "            truth_tokens = set(self.tokenizer.tokenize(ground_truth_normalized))\n",
    "            \n",
    "            if not truth_tokens:\n",
    "                return 0.0\n",
    "                \n",
    "            overlap = len(pred_tokens.intersection(truth_tokens)) / len(truth_tokens)\n",
    "            return max(0.0, overlap - 0.3)  # Only reward significant overlap\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "    def calculate_process_reward(self, question, steps, original_steps=None):\n",
    "        \"\"\"Calculate reward based on quality of reasoning process\"\"\"\n",
    "        # Get reflection scores\n",
    "        reflection_result = self.reflection_module.evaluate_reasoning(question, steps)\n",
    "        process_score = reflection_result['scores']['overall_score']\n",
    "        \n",
    "        # If we have original steps, reward improvement\n",
    "        if original_steps:\n",
    "            original_result = self.reflection_module.evaluate_reasoning(question, original_steps)\n",
    "            original_score = original_result['scores']['overall_score']\n",
    "            \n",
    "            # Reward improvement, penalize degradation\n",
    "            improvement = process_score - original_score\n",
    "            if improvement > 0:\n",
    "                process_score += 0.2 * improvement  # Bonus for improvement\n",
    "            else:\n",
    "                process_score += 0.1 * improvement  # Smaller penalty for degradation\n",
    "        \n",
    "        return process_score\n",
    "    \n",
    "    def calculate_combined_reward(self, sample, ground_truth, original_steps=None):\n",
    "        \"\"\"Calculate combined reward from outcome and process\"\"\"\n",
    "        question = sample['question']\n",
    "        steps = sample['steps']\n",
    "        predicted_answer = sample['answer']\n",
    "        \n",
    "        # Calculate component rewards\n",
    "        outcome_reward = self.calculate_outcome_reward(predicted_answer, ground_truth)\n",
    "        process_reward = self.calculate_process_reward(question, steps, original_steps)\n",
    "        \n",
    "        # Combine rewards\n",
    "        # The balance between outcome and process rewards is important\n",
    "        # In this implementation, we favor process for ScienceQA\n",
    "        combined_reward = 0.4 * outcome_reward + 0.6 * process_reward\n",
    "        \n",
    "        return {\n",
    "            'combined': combined_reward,\n",
    "            'outcome': outcome_reward,\n",
    "            'process': process_reward,\n",
    "            'details': {\n",
    "                'answer_correct': outcome_reward > 0.9,\n",
    "                'reasoning_quality': process_reward\n",
    "            }\n",
    "        }\n",
    "\n",
    "class PPOTrainer:\n",
    "    \"\"\"PPO-based RL trainer for the reasoning model\"\"\"\n",
    "    def __init__(self, cot_generator, reward_function, config):\n",
    "        self.cot_generator = cot_generator\n",
    "        self.reward_function = reward_function\n",
    "        self.config = config\n",
    "        \n",
    "        # Create a reference model for KL penalty\n",
    "        self.ref_model = AutoModelForCausalLM.from_pretrained(config.model_name)\n",
    "        self.ref_model.to(config.device)\n",
    "        self.ref_model.eval()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = AdamW(\n",
    "            self.cot_generator.model.parameters(),\n",
    "            lr=config.learning_rate,\n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "        \n",
    "        # Initialize policy entropy and value losses\n",
    "        self.policy_loss = 0\n",
    "        self.value_losses = []\n",
    "        self.entropy_losses = []\n",
    "    \n",
    "    def train_step(self, batch, gan_module=None):\n",
    "        \"\"\"Perform a single PPO training step\"\"\"\n",
    "        # Extract data\n",
    "        input_ids = batch['input_ids'].to(self.config.device)\n",
    "        attention_mask = batch['attention_mask'].to(self.config.device)\n",
    "        questions = batch['question']\n",
    "        ground_truth_answers = batch['answer']\n",
    "        \n",
    "        # Forward pass with current policy to get initial log probs and values\n",
    "        with torch.no_grad():\n",
    "            outputs = self.cot_generator.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                return_dict=True\n",
    "            )\n",
    "            old_logits = outputs.logits\n",
    "            \n",
    "            # Extract values (implicitly learned through the LM head)\n",
    "            # In a full implementation, you would have a separate value head\n",
    "            values = torch.mean(old_logits, dim=-1)  # Simplistic value estimation\n",
    "        \n",
    "        # Generate samples from current policy\n",
    "        generated_samples = []\n",
    "        for i in range(len(questions)):\n",
    "            sample = self.cot_generator.generate_step_by_step(questions[i], image=None)\n",
    "            generated_samples.append(sample)\n",
    "        \n",
    "        # Optionally refine with GAN\n",
    "        if gan_module:\n",
    "            for i, sample in enumerate(generated_samples):\n",
    "                refined_steps = gan_module.refine_reasoning_steps(\n",
    "                    sample['question'], \n",
    "                    sample['steps']\n",
    "                )\n",
    "                generated_samples[i]['refined_steps'] = refined_steps\n",
    "        \n",
    "        # Calculate rewards\n",
    "        rewards = []\n",
    "        for i, sample in enumerate(generated_samples):\n",
    "            steps_to_evaluate = sample.get('refined_steps', sample['steps'])\n",
    "            reward = self.reward_function.calculate_combined_reward(\n",
    "                {\n",
    "                    'question': sample['question'],\n",
    "                    'steps': steps_to_evaluate,\n",
    "                    'answer': sample['answer']\n",
    "                },\n",
    "                ground_truth_answers[i]\n",
    "            )\n",
    "            rewards.append(reward['combined'])\n",
    "        \n",
    "        rewards_tensor = torch.tensor(rewards, device=self.config.device)\n",
    "        \n",
    "        # PPO optimization loop\n",
    "        for _ in range(self.config.ppo_epochs):\n",
    "            # Forward pass with current policy\n",
    "            outputs = self.cot_generator.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                return_dict=True\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Calculate new log probabilities and values\n",
    "            new_values = torch.mean(logits, dim=-1)  # Simplistic value estimation\n",
    "            \n",
    "            # Compute KL divergence penalty\n",
    "            kl_div = self._compute_kl_divergence(old_logits, logits, attention_mask)\n",
    "            \n",
    "            # Compute policy loss (PPO clipped objective)\n",
    "            # In a full implementation, you would compute proper action probabilities\n",
    "            # For simplicity, we're using a proxy based on logits difference\n",
    "            logit_diff = torch.sum(torch.abs(logits - old_logits), dim=-1)\n",
    "            policy_ratio = torch.exp(-logit_diff * 0.01)  # Proxy for probability ratio\n",
    "            \n",
    "            clipped_ratio = torch.clamp(\n",
    "                policy_ratio, \n",
    "                1.0 - self.config.clip_param, \n",
    "                1.0 + self.config.clip_param\n",
    "            )\n",
    "            \n",
    "            policy_reward = rewards_tensor.unsqueeze(-1).expand_as(policy_ratio)\n",
    "            policy_loss = -torch.min(\n",
    "                policy_ratio * policy_reward,\n",
    "                clipped_ratio * policy_reward\n",
    "            ).mean()\n",
    "            \n",
    "            # Value loss\n",
    "            value_loss = F.mse_loss(new_values, rewards_tensor.unsqueeze(-1).expand_as(new_values))\n",
    "            \n",
    "            # Entropy for exploration\n",
    "            # Simplified entropy calculation\n",
    "            entropy = torch.mean(torch.std(logits, dim=-1))\n",
    "            entropy_loss = -self.config.entropy_coef * entropy\n",
    "            \n",
    "            # Total loss\n",
    "            loss = policy_loss + self.config.value_loss_coef * value_loss + entropy_loss + 0.01 * kl_div\n",
    "            \n",
    "            # Backward and optimize\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.cot_generator.model.parameters(), self.config.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Store metrics\n",
    "            self.policy_loss = policy_loss.item()\n",
    "            self.value_losses.append(value_loss.item())\n",
    "            self.entropy_losses.append(entropy_loss.item())\n",
    "        \n",
    "        return {\n",
    "            'policy_loss': self.policy_loss,\n",
    "            'value_loss': sum(self.value_losses) / len(self.value_losses),\n",
    "            'entropy_loss': sum(self.entropy_losses) / len(self.entropy_losses),\n",
    "            'mean_reward': rewards_tensor.mean().item()\n",
    "        }\n",
    "    \n",
    "    def _compute_kl_divergence(self, old_logits, new_logits, attention_mask):\n",
    "        \"\"\"Compute KL divergence between old and new policies\"\"\"\n",
    "        old_probs = F.softmax(old_logits, dim=-1)\n",
    "        new_probs = F.softmax(new_logits, dim=-1)\n",
    "        \n",
    "        # KL divergence\n",
    "        kl = old_probs * (torch.log(old_probs) - torch.log(new_probs))\n",
    "        kl = kl.sum(-1)\n",
    "        \n",
    "        # Apply attention mask\n",
    "        kl = kl * attention_mask.float()\n",
    "        \n",
    "        # Average over non-masked tokens\n",
    "        kl = kl.sum() / attention_mask.float().sum()\n",
    "        \n",
    "        return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Self-Training and Distillation\n",
    "\n",
    "class SelfTrainer:\n",
    "    \"\"\"Self-training through iterative pseudo-labeling\"\"\"\n",
    "    def __init__(self, cot_generator, config):\n",
    "        self.cot_generator = cot_generator\n",
    "        self.config = config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n",
    "        \n",
    "        # Optimizer for fine-tuning\n",
    "        self.optimizer = AdamW(\n",
    "            self.cot_generator.model.parameters(),\n",
    "            lr=config.learning_rate,\n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "        \n",
    "        # LR scheduler\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=config.warmup_steps,\n",
    "            num_training_steps=1000  # Will be updated when dataset size is known\n",
    "        )\n",
    "    \n",
    "    def generate_pseudo_labels(self, unlabeled_data, ground_truth_answers=None):\n",
    "        \"\"\"Generate pseudo-labels for unlabeled data\"\"\"\n",
    "        pseudo_labeled_data = []\n",
    "        \n",
    "        for i, sample in enumerate(unlabeled_data):\n",
    "            question = sample['question']\n",
    "            \n",
    "            # Generate reasoning steps and answer\n",
    "            generated = self.cot_generator.generate_step_by_step(question, image=None)\n",
    "            \n",
    "            # Check if the answer is correct (if ground truth is available)\n",
    "            is_correct = False\n",
    "            if ground_truth_answers is not None:\n",
    "                ground_truth = ground_truth_answers[i]\n",
    "                predicted = generated['answer'].strip().lower()\n",
    "                ground_truth = ground_truth.strip().lower()\n",
    "                is_correct = predicted == ground_truth\n",
    "            \n",
    "            # Only include correct answers or all if no ground truth\n",
    "            if is_correct or ground_truth_answers is None:\n",
    "                pseudo_labeled_data.append({\n",
    "                    'question': question,\n",
    "                    'steps': generated['steps'],\n",
    "                    'answer': generated['answer'],\n",
    "                    'confidence': 1.0  # In a real implementation, you'd use model confidence\n",
    "                })\n",
    "        \n",
    "        return pseudo_labeled_data\n",
    "    \n",
    "    def finetune_on_pseudo_labels(self, pseudo_labeled_data, epochs=None):\n",
    "        \"\"\"Fine-tune model on pseudo-labeled data\"\"\"\n",
    "        if epochs is None:\n",
    "            epochs = self.config.epochs\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = self._create_dataset_from_samples(pseudo_labeled_data)\n",
    "        \n",
    "        # Adjust scheduler\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=self.config.warmup_steps,\n",
    "            num_training_steps=epochs * len(dataset)\n",
    "        )\n",
    "        \n",
    "        # Fine-tuning loop\n",
    "        self.cot_generator.model.train()\n",
    "        total_loss = 0\n",
    "        global_step = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            logger.info(f\"Starting epoch {epoch+1}/{epochs}\")\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            for batch in dataset:\n",
    "                # Move batch to device\n",
    "                batch = {k: v.to(self.config.device) if isinstance(v, torch.Tensor) else v \n",
    "                         for k, v in batch.items()}\n",
    "                \n",
    "                # Forward and backward\n",
    "                outputs = self.cot_generator.model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['labels']\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.cot_generator.model.parameters(), \n",
    "                    self.config.max_grad_norm\n",
    "                )\n",
    "                \n",
    "                # Update weights\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                global_step += 1\n",
    "                \n",
    "                # Log progress\n",
    "                if global_step % 50 == 0:\n",
    "                    logger.info(f\"Step {global_step}: loss = {loss.item()}\")\n",
    "            \n",
    "            avg_epoch_loss = epoch_loss / len(dataset)\n",
    "            total_loss += avg_epoch_loss\n",
    "            logger.info(f\"Epoch {epoch+1} completed: Average loss = {avg_epoch_loss}\")\n",
    "        \n",
    "        avg_loss = total_loss / epochs\n",
    "        logger.info(f\"Fine-tuning completed: Average loss = {avg_loss}\")\n",
    "        \n",
    "        return avg_loss\n",
    "    \n",
    "    def _create_dataset_from_samples(self, samples):\n",
    "        \"\"\"Create a dataset from generated samples\"\"\"\n",
    "        dataset = []\n",
    "        \n",
    "        for sample in samples:\n",
    "            question = sample['question']\n",
    "            steps = sample['steps']\n",
    "            answer = sample['answer']\n",
    "            \n",
    "            # Prepare input text\n",
    "            input_text = f\"Let's think step by step! {question}\\n\"\n",
    "            for i, step in enumerate(steps):\n",
    "                input_text += f\"{i+1}. {step}\\n\"\n",
    "            input_text += f\"Therefore, the answer is {answer}\"\n",
    "            \n",
    "            # Tokenize\n",
    "            encodings = self.tokenizer(\n",
    "                input_text,\n",
    "                truncation=True,\n",
    "                max_length=self.config.max_seq_length,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Create labels for causal LM training\n",
    "            input_ids = encodings['input_ids'][0]\n",
    "            attention_mask = encodings['attention_mask'][0]\n",
    "            labels = input_ids.clone()\n",
    "            \n",
    "            # Mask question part in labels\n",
    "            question_tokens = self.tokenizer.encode(\n",
    "                f\"Let's think step by step! {question}\",\n",
    "                add_special_tokens=True\n",
    "            )\n",
    "            labels[:len(question_tokens)] = -100\n",
    "            \n",
    "            dataset.append({\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'labels': labels\n",
    "            })\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def self_training_loop(self, labeled_data, unlabeled_data, num_iterations=3):\n",
    "        \"\"\"Run the complete self-training loop\"\"\"\n",
    "        for iteration in range(num_iterations):\n",
    "            logger.info(f\"Starting self-training iteration {iteration+1}/{num_iterations}\")\n",
    "            \n",
    "            # Generate pseudo-labels\n",
    "            pseudo_labels = self.generate_pseudo_labels(\n",
    "                unlabeled_data,\n",
    "                ground_truth_answers=[item['answer'] for item in unlabeled_data]\n",
    "            )\n",
    "            \n",
    "            if not pseudo_labels:\n",
    "                logger.warning(\"No pseudo-labels generated. Stopping self-training.\")\n",
    "                break\n",
    "            \n",
    "            logger.info(f\"Generated {len(pseudo_labels)} pseudo-labels\")\n",
    "            \n",
    "            # Combine with labeled data\n",
    "            combined_data = labeled_data + pseudo_labels\n",
    "            \n",
    "            # Fine-tune on combined data\n",
    "            loss = self.finetune_on_pseudo_labels(combined_data)\n",
    "            \n",
    "            logger.info(f\"Iteration {iteration+1} completed: loss = {loss}\")\n",
    "            \n",
    "            # Update labeled data for next iteration\n",
    "            labeled_data = combined_data\n",
    "        \n",
    "        logger.info(\"Self-training completed\")\n",
    "        \n",
    "        # Save the final model\n",
    "        self.cot_generator.model.save_pretrained(os.path.join(self.config.output_dir, \"self_trained_model\"))\n",
    "        self.tokenizer.save_pretrained(os.path.join(self.config.output_dir, \"self_trained_tokenizer\"))\n",
    "        \n",
    "        return labeled_data\n",
    "\n",
    "class ModelDistiller:\n",
    "    \"\"\"Knowledge distillation for creating smaller, efficient models\"\"\"\n",
    "    def __init__(self, teacher_model, config, student_model_name=\"distilbert-base-uncased\"):\n",
    "        self.teacher_model = teacher_model\n",
    "        self.config = config\n",
    "        \n",
    "        # Load smaller student model\n",
    "        self.student_tokenizer = AutoTokenizer.from_pretrained(student_model_name)\n",
    "        self.student_model = AutoModelForCausalLM.from_pretrained(student_model_name)\n",
    "        \n",
    "        # Add special tokens if needed\n",
    "        special_tokens = {\"pad_token\": \"[PAD]\"} if self.student_tokenizer.pad_token is None else {}\n",
    "        if special_tokens:\n",
    "            self.student_tokenizer.add_special_tokens(special_tokens)\n",
    "            self.student_model.resize_token_embeddings(len(self.student_tokenizer))\n",
    "        \n",
    "        # Move to device\n",
    "        self.student_model.to(config.device)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = AdamW(\n",
    "            self.student_model.parameters(),\n",
    "            lr=2e-5,  # Usually higher for distillation\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        # Temperature for softening distributions\n",
    "        self.temperature = 2.0\n",
    "    \n",
    "    def distill(self, dataset, epochs=3):\n",
    "        \"\"\"Distill knowledge from teacher to student\"\"\"\n",
    "        self.student_model.train()\n",
    "        self.teacher_model.model.eval()\n",
    "        \n",
    "        total_loss = 0\n",
    "        step_count = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            logger.info(f\"Starting distillation epoch {epoch+1}/{epochs}\")\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            for batch in dataset:\n",
    "                # Convert to student tokenization\n",
    "                student_inputs = self._convert_teacher_to_student_inputs(batch)\n",
    "                \n",
    "                # Move to device\n",
    "                student_inputs = {k: v.to(self.config.device) if isinstance(v, torch.Tensor) else v\n",
    "                                 for k, v in student_inputs.items()}\n",
    "                \n",
    "                # Get teacher predictions\n",
    "                with torch.no_grad():\n",
    "                    teacher_outputs = self.teacher_model.model(\n",
    "                        input_ids=batch['input_ids'].to(self.config.device),\n",
    "                        attention_mask=batch['attention_mask'].to(self.config.device)\n",
    "                    )\n",
    "                    \n",
    "                    # Apply temperature scaling to logits\n",
    "                    teacher_logits = teacher_outputs.logits / self.temperature\n",
    "                \n",
    "                # Student forward pass\n",
    "                student_outputs = self.student_model(**student_inputs)\n",
    "                student_logits = student_outputs.logits / self.temperature\n",
    "                \n",
    "                # Compute distillation loss\n",
    "                # Standard cross-entropy loss for task performance\n",
    "                task_loss = F.cross_entropy(\n",
    "                    student_logits.view(-1, student_logits.size(-1)),\n",
    "                    student_inputs['labels'].view(-1),\n",
    "                    ignore_index=-100\n",
    "                )\n",
    "                \n",
    "                # Distillation loss (KL divergence)\n",
    "                # We need to align teacher and student token representations\n",
    "                aligned_teacher_logits = self._align_teacher_student_representations(\n",
    "                    teacher_logits, \n",
    "                    student_logits,\n",
    "                    batch['attention_mask'].to(self.config.device),\n",
    "                    student_inputs['attention_mask']\n",
    "                )\n",
    "                \n",
    "                distillation_loss = F.kl_div(\n",
    "                    F.log_softmax(student_logits, dim=-1),\n",
    "                    F.softmax(aligned_teacher_logits, dim=-1),\n",
    "                    reduction='batchmean'\n",
    "                )\n",
    "                \n",
    "                # Combine losses\n",
    "                loss = 0.5 * task_loss + 0.5 * distillation_loss\n",
    "                \n",
    "                # Backward and optimize\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.student_model.parameters(), 1.0)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # Track losses\n",
    "                epoch_loss += loss.item()\n",
    "                step_count += 1\n",
    "                \n",
    "                # Log progress\n",
    "                if step_count % 100 == 0:\n",
    "                    logger.info(f\"Distillation step {step_count}: loss = {loss.item()}\")\n",
    "            \n",
    "            avg_epoch_loss = epoch_loss / len(dataset)\n",
    "            total_loss += avg_epoch_loss\n",
    "            logger.info(f\"Distillation epoch {epoch+1} completed: Average loss = {avg_epoch_loss}\")\n",
    "        \n",
    "        avg_loss = total_loss / epochs\n",
    "        logger.info(f\"Distillation completed: Average loss = {avg_loss}\")\n",
    "        \n",
    "        # Save distilled model\n",
    "        self.student_model.save_pretrained(os.path.join(self.config.output_dir, \"distilled_model\"))\n",
    "        self.student_tokenizer.save_pretrained(os.path.join(self.config.output_dir, \"distilled_tokenizer\"))\n",
    "        \n",
    "        return self.student_model\n",
    "    \n",
    "    def _convert_teacher_to_student_inputs(self, teacher_batch):\n",
    "        \"\"\"Convert teacher batch to student tokenization\"\"\"\n",
    "        # This is a placeholder - in practice, you would need to implement \n",
    "        # conversion between different tokenizers\n",
    "        return teacher_batch\n",
    "    \n",
    "    def _align_teacher_student_representations(self, teacher_logits, student_logits, \n",
    "                                              teacher_mask, student_mask):\n",
    "        \"\"\"Align teacher and student token representations\"\"\"\n",
    "        # This is a placeholder for token alignment\n",
    "        # In practice, you'd need to implement vocabulary mapping\n",
    "        return teacher_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Integration - Full Reasoning Pipeline\n",
    "class ReasoningPipeline:\n",
    "    \"\"\"Full reasoning pipeline integrating all components\"\"\"\n",
    "    def __init__(self, config, bert_model=None, bert_tokenizer=None, vision_processor=None):\n",
    "        self.config = config\n",
    "        \n",
    "        # Use provided models/tokenizers if available, otherwise initialize from config\n",
    "        if bert_tokenizer is not None:\n",
    "            self.tokenizer = bert_tokenizer\n",
    "        else:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n",
    "            \n",
    "        if vision_processor is not None:\n",
    "            self.vision_processor = vision_processor\n",
    "        else:\n",
    "            self.vision_processor = AutoProcessor.from_pretrained(config.vision_model_name)\n",
    "        \n",
    "        # Add special tokens if needed\n",
    "        special_tokens = {\"pad_token\": \"[PAD]\"} if self.tokenizer.pad_token is None else {}\n",
    "        if special_tokens:\n",
    "            self.tokenizer.add_special_tokens(special_tokens)\n",
    "        \n",
    "        # Initialize components, passing the BERT model to CoT generator if provided\n",
    "        self.cot_generator = ChainOfThoughtGenerator(config, model=bert_model)\n",
    "        self.reflection_module = ReflectionModule(config)\n",
    "        self.retrieval_module = RetrievalModule(config)\n",
    "        self.gan_module = GANModule(config)\n",
    "        \n",
    "        # Create reward function\n",
    "        self.reward_function = RewardFunction(self.reflection_module, config)\n",
    "        \n",
    "        # Create RL trainer\n",
    "        self.ppo_trainer = PPOTrainer(self.cot_generator, self.reward_function, config)\n",
    "        \n",
    "        # Create self-trainer\n",
    "        self.self_trainer = SelfTrainer(self.cot_generator, config)\n",
    "        \n",
    "        # Initialize model distiller\n",
    "        self.distiller = None\n",
    "    \n",
    "    def train(self, train_file, val_file, num_rl_updates=1000, num_self_training_iterations=3):\n",
    "        \"\"\"Train the full reasoning pipeline\"\"\"\n",
    "        # Load datasets\n",
    "        train_dataset = ScienceQADataset(\n",
    "            train_file, \n",
    "            self.tokenizer, \n",
    "            self.vision_processor, \n",
    "            self.config, \n",
    "            is_train=True\n",
    "        )\n",
    "        \n",
    "        val_dataset = ScienceQADataset(\n",
    "            val_file, \n",
    "            self.tokenizer, \n",
    "            self.vision_processor, \n",
    "            self.config, \n",
    "            is_train=False\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.config.batch_size, \n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        val_dataloader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=self.config.batch_size, \n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        # 1. Initial supervised fine-tuning\n",
    "        logger.info(\"Starting supervised fine-tuning\")\n",
    "        self._supervised_finetuning(train_dataloader, val_dataloader)\n",
    "        \n",
    "        # 2. Train GAN components\n",
    "        logger.info(\"Training GAN components\")\n",
    "        self._train_gan(train_dataloader)\n",
    "        \n",
    "        # 3. RL fine-tuning\n",
    "        logger.info(\"Starting RL fine-tuning\")\n",
    "        self._rl_finetuning(train_dataloader, num_rl_updates)\n",
    "        \n",
    "        # 4. Self-training loop\n",
    "        logger.info(\"Starting self-training\")\n",
    "        labeled_data = train_dataset.data[:100]  # Start with a small labeled subset\n",
    "        unlabeled_data = train_dataset.data[100:]  # The rest is unlabeled\n",
    "        final_labeled_data = self.self_trainer.self_training_loop(\n",
    "            labeled_data, \n",
    "            unlabeled_data, \n",
    "            num_iterations=num_self_training_iterations\n",
    "        )\n",
    "        \n",
    "        # 5. Distillation to smaller model\n",
    "        logger.info(\"Starting model distillation\")\n",
    "        self.distiller = ModelDistiller(self.cot_generator, self.config)\n",
    "        distilled_model = self.distiller.distill(train_dataloader, epochs=3)\n",
    "        \n",
    "        logger.info(\"Training complete\")\n",
    "        \n",
    "        return {\n",
    "            'cot_generator': self.cot_generator,\n",
    "            'reflection_module': self.reflection_module,\n",
    "            'retrieval_module': self.retrieval_module,\n",
    "            'gan_module': self.gan_module,\n",
    "            'distilled_model': distilled_model\n",
    "        }\n",
    "    \n",
    "    def _supervised_finetuning(self, train_dataloader, val_dataloader, epochs=None):\n",
    "        \"\"\"Supervised fine-tuning on labeled data\"\"\"\n",
    "        if epochs is None:\n",
    "            epochs = self.config.epochs\n",
    "        \n",
    "        # Setup optimizer\n",
    "        optimizer = AdamW(\n",
    "            self.cot_generator.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay\n",
    "        )\n",
    "        \n",
    "        # Setup scheduler\n",
    "        total_steps = len(train_dataloader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.config.warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        self.cot_generator.model.train()\n",
    "        global_step = 0\n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            # Training with tqdm progress bar\n",
    "            pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=True)\n",
    "            for batch in pbar:\n",
    "                # Move batch to device\n",
    "                input_ids = batch['input_ids'].to(self.config.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.config.device)\n",
    "                labels = batch['labels'].to(self.config.device)\n",
    "                visual_features = None\n",
    "                if batch.get('visual_features') is not None:\n",
    "                    visual_features = batch['visual_features'].to(self.config.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.cot_generator(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                    visual_features=visual_features\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.cot_generator.model.parameters(), \n",
    "                    self.config.max_grad_norm\n",
    "                )\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                global_step += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"step\": global_step})\n",
    "            \n",
    "            avg_train_loss = epoch_loss / len(train_dataloader)\n",
    "            logger.info(f\"Epoch {epoch+1}/{epochs} completed: Average training loss = {avg_train_loss}\")\n",
    "            \n",
    "            # Validation with tqdm\n",
    "            val_loss = self._evaluate(val_dataloader)\n",
    "            logger.info(f\"Validation loss: {val_loss}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                self.cot_generator.model.save_pretrained(\n",
    "                    os.path.join(self.config.checkpoint_dir, \"best_model\")\n",
    "                )\n",
    "                logger.info(\"Saved new best model\")\n",
    "    \n",
    "    def _evaluate(self, dataloader):\n",
    "        \"\"\"Evaluate model on dataloader\"\"\"\n",
    "        self.cot_generator.model.eval()\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Evaluation with tqdm progress bar\n",
    "        pbar = tqdm(dataloader, desc=\"Evaluation\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for batch in pbar:\n",
    "                # Move batch to device\n",
    "                input_ids = batch['input_ids'].to(self.config.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.config.device)\n",
    "                labels = batch['labels'].to(self.config.device)\n",
    "                visual_features = None\n",
    "                if batch.get('visual_features') is not None:\n",
    "                    visual_features = batch['visual_features'].to(self.config.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.cot_generator(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                    visual_features=visual_features\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss.item()\n",
    "                total_loss += loss\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\"loss\": f\"{loss:.4f}\"})\n",
    "        \n",
    "        # Set back to training mode\n",
    "        self.cot_generator.model.train()\n",
    "        \n",
    "        return total_loss / len(dataloader)\n",
    "    \n",
    "    def _train_gan(self, dataloader, epochs=None):\n",
    "        \"\"\"Train GAN components\"\"\"\n",
    "        if epochs is None:\n",
    "            epochs = self.config.gan_epochs\n",
    "        \n",
    "        # Create training examples\n",
    "        gan_training_data = []\n",
    "        \n",
    "        # Sample batches for GAN training with progress bar\n",
    "        pbar = tqdm(dataloader, desc=\"Preparing GAN training data\", leave=True)\n",
    "        for batch in pbar:\n",
    "            questions = batch['question']\n",
    "            steps_list = batch['steps']\n",
    "            \n",
    "            for question, steps in zip(questions, steps_list):\n",
    "                for i in range(1, len(steps)):\n",
    "                    context = question + \" \" + \" \".join(steps[:i])\n",
    "                    gan_training_data.append({\n",
    "                        'context': context,\n",
    "                        'step': steps[i]\n",
    "                    })\n",
    "                    \n",
    "                    # Break after a few examples per sample\n",
    "                    if i >= 3:\n",
    "                        break\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\"examples\": len(gan_training_data)})\n",
    "            \n",
    "            # Limit training data size\n",
    "            if len(gan_training_data) >= 1000:\n",
    "                break\n",
    "        \n",
    "        # Train GAN for specified epochs\n",
    "        for epoch in range(epochs):\n",
    "            logger.info(f\"GAN training epoch {epoch+1}/{epochs}\")\n",
    "            epoch_g_loss = 0\n",
    "            epoch_d_loss = 0\n",
    "            \n",
    "            # Shuffle training data\n",
    "            random.shuffle(gan_training_data)\n",
    "            \n",
    "            # Create batches\n",
    "            batch_size = self.config.gan_batch_size\n",
    "            num_batches = len(gan_training_data) // batch_size\n",
    "            \n",
    "            # Train with progress bar\n",
    "            pbar = tqdm(range(num_batches), desc=f\"GAN Epoch {epoch+1}/{epochs}\", leave=True)\n",
    "            for i in pbar:\n",
    "                batch_data = gan_training_data[i * batch_size:(i + 1) * batch_size]\n",
    "                \n",
    "                # Train discriminator\n",
    "                d_loss = self.gan_module.train_discriminator(batch_data)\n",
    "                epoch_d_loss += d_loss\n",
    "                \n",
    "                # Train generator\n",
    "                g_loss = self.gan_module.train_generator(batch_data)\n",
    "                epoch_g_loss += g_loss\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\n",
    "                    \"G_loss\": f\"{g_loss:.4f}\", \n",
    "                    \"D_loss\": f\"{d_loss:.4f}\"\n",
    "                })\n",
    "            \n",
    "            # Average losses\n",
    "            avg_g_loss = epoch_g_loss / num_batches\n",
    "            avg_d_loss = epoch_d_loss / num_batches\n",
    "            logger.info(f\"GAN epoch {epoch+1} completed: \"\n",
    "                        f\"Average G loss = {avg_g_loss:.4f}, \"\n",
    "                        f\"Average D loss = {avg_d_loss:.4f}\")\n",
    "            \n",
    "            # Evaluate GAN\n",
    "            self._evaluate_gan(dataloader)\n",
    "            \n",
    "            # Save GAN checkpoint\n",
    "            self.gan_module.save_checkpoint(\n",
    "                os.path.join(self.config.checkpoint_dir, f\"gan_checkpoint_epoch_{epoch+1}\")\n",
    "            )\n",
    "    \n",
    "    def _evaluate_gan(self, dataloader):\n",
    "        \"\"\"Evaluate GAN quality\"\"\"\n",
    "        # Sample a few examples from validation set\n",
    "        examples = []\n",
    "        \n",
    "        # Use tqdm for sampling\n",
    "        pbar = tqdm(dataloader, desc=\"Sampling for GAN evaluation\", leave=False)\n",
    "        for batch in pbar:\n",
    "            questions = batch['question']\n",
    "            steps_list = batch['steps']\n",
    "            \n",
    "            for question, steps in zip(questions, steps_list):\n",
    "                if len(steps) >= 3:  # Ensure enough steps for evaluation\n",
    "                    examples.append({\n",
    "                        'question': question,\n",
    "                        'steps': steps\n",
    "                    })\n",
    "                \n",
    "                # Limit evaluation examples\n",
    "                if len(examples) >= 10:\n",
    "                    break\n",
    "            \n",
    "            if len(examples) >= 10:\n",
    "                break\n",
    "                \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\"samples\": len(examples)})\n",
    "        \n",
    "        # Evaluate GAN refinement quality\n",
    "        logger.info(\"Evaluating GAN refinement quality:\")\n",
    "        success_count = 0\n",
    "        \n",
    "        # Progress bar for evaluation\n",
    "        pbar = tqdm(examples, desc=\"GAN Evaluation\", leave=False)\n",
    "        for example in pbar:\n",
    "            # Get partial reasoning chain (first half of steps)\n",
    "            partial_steps = example['steps'][:len(example['steps']) // 2]\n",
    "            \n",
    "            # Generate continuation with GAN\n",
    "            refined_steps = self.gan_module.refine_reasoning_steps(\n",
    "                example['question'],\n",
    "                partial_steps\n",
    "            )\n",
    "            \n",
    "            # Evaluate with reflection module\n",
    "            original_score = self.reflection_module.evaluate_reasoning(\n",
    "                example['question'],\n",
    "                example['steps']\n",
    "            )['scores']['overall_score']\n",
    "            \n",
    "            refined_score = self.reflection_module.evaluate_reasoning(\n",
    "                example['question'],\n",
    "                partial_steps + refined_steps\n",
    "            )['scores']['overall_score']\n",
    "            \n",
    "            # Check improvement\n",
    "            improvement = refined_score - original_score\n",
    "            status = \"✓\" if improvement >= 0 else \"✗\"\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                \"orig\": f\"{original_score:.2f}\",\n",
    "                \"refined\": f\"{refined_score:.2f}\",\n",
    "                \"diff\": f\"{improvement:.2f} {status}\"\n",
    "            })\n",
    "            \n",
    "            # Log individual example\n",
    "            logger.info(f\"  Example {examples.index(example)+1}: \"\n",
    "                        f\"Original: {original_score:.2f}, \"\n",
    "                        f\"Refined: {refined_score:.2f}, \"\n",
    "                        f\"Diff: {improvement:.2f} {status}\")\n",
    "            \n",
    "            if improvement >= 0:\n",
    "                success_count += 1\n",
    "        \n",
    "        success_rate = success_count / len(examples) * 100\n",
    "        logger.info(f\"GAN evaluation complete: Success rate = {success_rate:.1f}%\")\n",
    "        \n",
    "        return success_rate\n",
    "    \n",
    "    def _rl_finetuning(self, dataloader, num_updates):\n",
    "        \"\"\"Perform RL fine-tuning\"\"\"\n",
    "        # Setup learning rate scheduler\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.ppo_trainer.optimizer,\n",
    "            num_warmup_steps=int(0.1 * num_updates),\n",
    "            num_training_steps=num_updates\n",
    "        )\n",
    "        \n",
    "        # RL training loop\n",
    "        global_step = 0\n",
    "        best_reward = float('-inf')\n",
    "        \n",
    "        logger.info(f\"Starting RL fine-tuning: {num_updates} updates\")\n",
    "        \n",
    "        # Create progress bar for RL updates\n",
    "        pbar = tqdm(total=num_updates, desc=\"RL Fine-tuning\", leave=True)\n",
    "        \n",
    "        while global_step < num_updates:\n",
    "            # Sample batch from dataloader\n",
    "            for batch in dataloader:\n",
    "                # Perform PPO update\n",
    "                metrics = self.ppo_trainer.train_step(batch, self.gan_module)\n",
    "                \n",
    "                # Update learning rate\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Update progress bar\n",
    "                global_step += 1\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\n",
    "                    \"policy_loss\": f\"{metrics['policy_loss']:.4f}\", \n",
    "                    \"value_loss\": f\"{metrics['value_loss']:.4f}\", \n",
    "                    \"reward\": f\"{metrics['mean_reward']:.4f}\"\n",
    "                })\n",
    "                \n",
    "                # Log metrics periodically\n",
    "                if global_step % 10 == 0:\n",
    "                    logger.info(\n",
    "                        f\"RL step {global_step}/{num_updates}: \"\n",
    "                        f\"Policy loss = {metrics['policy_loss']:.4f}, \"\n",
    "                        f\"Value loss = {metrics['value_loss']:.4f}, \"\n",
    "                        f\"Mean reward = {metrics['mean_reward']:.4f}\"\n",
    "                    )\n",
    "                \n",
    "                # Save best model\n",
    "                if metrics['mean_reward'] > best_reward:\n",
    "                    best_reward = metrics['mean_reward']\n",
    "                    self.cot_generator.model.save_pretrained(\n",
    "                        os.path.join(self.config.checkpoint_dir, \"best_rl_model\")\n",
    "                    )\n",
    "                    logger.info(f\"New best reward: {best_reward:.4f} - Saved model\")\n",
    "                \n",
    "                # Evaluate periodically\n",
    "                if global_step % 100 == 0:\n",
    "                    eval_reward = self._evaluate_rl()\n",
    "                    logger.info(f\"RL evaluation reward: {eval_reward:.4f}\")\n",
    "                    pbar.set_postfix({\n",
    "                        \"policy_loss\": f\"{metrics['policy_loss']:.4f}\", \n",
    "                        \"value_loss\": f\"{metrics['value_loss']:.4f}\", \n",
    "                        \"reward\": f\"{metrics['mean_reward']:.4f}\",\n",
    "                        \"eval\": f\"{eval_reward:.4f}\"\n",
    "                    })\n",
    "                \n",
    "                # Check if we've reached the target number of updates\n",
    "                if global_step >= num_updates:\n",
    "                    break\n",
    "        \n",
    "        # Close the progress bar\n",
    "        pbar.close()\n",
    "        logger.info(f\"RL fine-tuning complete: Best reward = {best_reward:.4f}\")\n",
    "    \n",
    "    def _evaluate_rl(self, num_samples=10):\n",
    "        \"\"\"Evaluate current policy\"\"\"\n",
    "        self.cot_generator.model.eval()\n",
    "        total_reward = 0.0\n",
    "        \n",
    "        # Sample examples for evaluation\n",
    "        examples = []\n",
    "        temp_data = self.self_trainer.generate_pseudo_labels(\n",
    "            [{'question': item} for item in self.config.eval_questions]\n",
    "        )\n",
    "        data_loader = DataLoader(temp_data, batch_size=1)\n",
    "        \n",
    "        # Use tqdm for sampling\n",
    "        pbar = tqdm(data_loader, desc=\"Sampling for RL evaluation\", leave=False)\n",
    "        for batch in pbar:\n",
    "            examples.append(batch)\n",
    "            if len(examples) >= num_samples:\n",
    "                break\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\"samples\": len(examples)})\n",
    "        \n",
    "        # Evaluate on examples with progress bar\n",
    "        eval_pbar = tqdm(examples, desc=\"RL Evaluation\", leave=False)\n",
    "        for example in eval_pbar:\n",
    "            # Generate reasoning\n",
    "            sample = self.cot_generator.generate_step_by_step(example['question'][0])\n",
    "            \n",
    "            # Calculate reward\n",
    "            reward = self.reward_function.calculate_combined_reward(\n",
    "                sample,\n",
    "                example['answer'][0]\n",
    "            )\n",
    "            \n",
    "            total_reward += reward['combined']\n",
    "            \n",
    "            # Update progress bar\n",
    "            eval_pbar.set_postfix({\"reward\": f\"{reward['combined']:.4f}\"})\n",
    "        \n",
    "        # Set back to training mode\n",
    "        self.cot_generator.model.train()\n",
    "        \n",
    "        # Calculate average reward\n",
    "        avg_reward = total_reward / len(examples)\n",
    "        return avg_reward\n",
    "    \n",
    "    def generate(self, question, image=None):\n",
    "        \"\"\"Generate reasoning for a question\"\"\"\n",
    "        logger.info(f\"Generating reasoning for: {question}\")\n",
    "        \n",
    "        # Lookup relevant information\n",
    "        logger.info(\"Retrieving context information...\")\n",
    "        retrieved_info = self.retrieval_module.retrieve(question)\n",
    "        \n",
    "        # Generate initial chain-of-thought\n",
    "        logger.info(\"Generating initial chain-of-thought...\")\n",
    "        result = self.cot_generator.generate_step_by_step(\n",
    "            question, \n",
    "            image=image,\n",
    "            retrieved_context=retrieved_info\n",
    "        )\n",
    "        \n",
    "        # Apply GAN refinement if available\n",
    "        if self.gan_module is not None:\n",
    "            logger.info(\"Applying GAN refinement...\")\n",
    "            refined_steps = self.gan_module.refine_reasoning_steps(\n",
    "                question,\n",
    "                result['steps']\n",
    "            )\n",
    "            result['refined_steps'] = refined_steps\n",
    "            \n",
    "            # Evaluate if refinement is better\n",
    "            logger.info(\"Evaluating refinement quality...\")\n",
    "            original_score = self.reflection_module.evaluate_reasoning(\n",
    "                question,\n",
    "                result['steps']\n",
    "            )['scores']['overall_score']\n",
    "            \n",
    "            refined_score = self.reflection_module.evaluate_reasoning(\n",
    "                question,\n",
    "                refined_steps\n",
    "            )['scores']['overall_score']\n",
    "            \n",
    "            # Use refined steps if better\n",
    "            if refined_score > original_score:\n",
    "                logger.info(f\"Using refined steps (score improved: {original_score:.2f} -> {refined_score:.2f})\")\n",
    "                result['steps'] = refined_steps\n",
    "                result['used_refinement'] = True\n",
    "            else:\n",
    "                logger.info(f\"Keeping original steps (refinement score: {refined_score:.2f} vs original: {original_score:.2f})\")\n",
    "        \n",
    "        # Get final reflection\n",
    "        logger.info(\"Generating final reflection...\")\n",
    "        reflection = self.reflection_module.evaluate_reasoning(\n",
    "            question,\n",
    "            result['steps']\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Reasoning generation complete\")\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'steps': result['steps'],\n",
    "            'answer': result['answer'],\n",
    "            'reflection': reflection,\n",
    "            'retrieved_context': retrieved_info\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 12:26:35,797 - __main__ - INFO - Configuration: {'model_name': 'bert-base-uncased', 'tokenizer_name': 'bert-base-uncased', 'vision_model_name': 'openai/clip-vit-base-patch32', 'file_path': '/Users/Viku/Datasets/ScienceQA', 'train_path': '/Users/Viku/Datasets/ScienceQA/train/train.json', 'val_path': '/Users/Viku/Datasets/ScienceQA/val/val.json', 'max_seq_length': 512, 'batch_size': 4, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'epochs': 3, 'warmup_steps': 100, 'max_grad_norm': 1.0, 'gradient_accumulation_steps': 8, 'ppo_epochs': 4, 'reward_scale': 0.01, 'clip_param': 0.2, 'value_loss_coef': 0.5, 'entropy_coef': 0.01, 'gan_learning_rate': 2e-05, 'gan_weight_decay': 0.01, 'gan_batch_size': 16, 'gan_epochs': 2, 'retrieval_top_k': 3, 'embedding_dim': 768, 'reflection_threshold': 0.7, 'output_dir': 'outputs/', 'checkpoint_dir': 'checkpoints/', 'exemplar_path': 'data/exemplars.json', 'device': device(type='mps'), 'max_answer_length': 64, 'rl_updates': 1000, 'self_training_iterations': 3}\n",
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChainOfThoughtGenerator initialized with:\n",
      "  Tokenizer: bert-base-uncased\n",
      "  Model: bert-base-uncased\n",
      "  Vision Model: openai/clip-vit-base-patch32\n",
      "  Device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 12:26:44,759 - __main__ - WARNING - Exemplar file data/exemplars.json not found, using empty exemplars\n",
      "/Users/Viku/GitHub/ReCoT/.venv/lib/python3.10/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "2025-03-06 12:26:48,922 - __main__ - INFO - Loading ScienceQA data from /Users/Viku/Datasets/ScienceQA/train/train.json\n",
      "2025-03-06 12:27:31,651 - __main__ - INFO - Processed 12726 ScienceQA examples\n",
      "2025-03-06 12:27:31,659 - __main__ - INFO - Loading ScienceQA data from /Users/Viku/Datasets/ScienceQA/val/val.json\n",
      "2025-03-06 12:27:38,995 - __main__ - INFO - Processed 4241 ScienceQA examples\n",
      "2025-03-06 12:27:38,999 - __main__ - INFO - Starting supervised fine-tuning\n",
      "Epoch 1/3 [Train]:   0%|          | 0/3182 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "Epoch 1/3 [Train]:   2%|▏         | 77/3182 [03:56<2:48:05,  3.25s/it, loss=0.0001, step=77] "
     ]
    }
   ],
   "source": [
    "# Create output directories\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Setup logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(config.output_dir, \"train.log\")),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger.info(f\"Configuration: {vars(config)}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Sample evaluation questions\n",
    "config.eval_questions = [\n",
    "    \"What happens when water boils?\",\n",
    "    \"How does gravity work?\",\n",
    "    \"Why does the moon have phases?\",\n",
    "    \"What is photosynthesis?\",\n",
    "    \"How do magnets work?\"\n",
    "]\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = ReasoningPipeline(config, bert_model, bert_tokenizer, vision_processor)\n",
    "\n",
    "# Train pipeline\n",
    "trained_components = pipeline.train(\n",
    "    config.train_path,\n",
    "    config.val_path,\n",
    "    num_rl_updates=config.rl_updates,\n",
    "    num_self_training_iterations=config.self_training_iterations\n",
    ")\n",
    "\n",
    "# Evaluate final model\n",
    "logger.info(\"Evaluating final model\")\n",
    "\n",
    "# Create test dataloader\n",
    "test_dataset = ScienceQADataset(\n",
    "    config.val_path,\n",
    "    pipeline.tokenizer,\n",
    "    pipeline.vision_processor,\n",
    "    config,\n",
    "    is_train=False\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Calculate validation metrics\n",
    "val_loss = pipeline._evaluate(test_dataloader)\n",
    "logger.info(f\"Final validation loss: {val_loss:.4f}\")\n",
    "\n",
    "# Generate examples for qualitative evaluation\n",
    "logger.info(\"Generating example outputs\")\n",
    "for question in config.eval_questions:\n",
    "    result = pipeline.generate(question)\n",
    "    logger.info(f\"Question: {question}\")\n",
    "    logger.info(f\"Steps:\")\n",
    "    for i, step in enumerate(result['steps']):\n",
    "        logger.info(f\" {i+1}. {step}\")\n",
    "    logger.info(f\"Answer: {result['answer']}\")\n",
    "    logger.info(f\"Reflection score: {result['reflection']['scores']['overall_score']:.2f}\")\n",
    "    logger.info(\"---\")\n",
    "\n",
    "logger.info(\"Training and evaluation complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
