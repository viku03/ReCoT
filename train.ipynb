{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    AutoProcessor,\n",
    "    CLIPVisionModel,\n",
    "    AdamW, \n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoModel,\n",
    "    AutoModelForSeq2SeqLM\n",
    ")\n",
    "import json\n",
    "import logging\n",
    "import faiss\n",
    "from PIL import Image\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed: int = 11):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MISTRAL_API_KEY\"]=\"jJAuJZkjVcy2ynUhan375sHNviHiBeJU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config class to hold hyperparameters\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Base Model\n",
    "        self.model_name = \"bert-base-uncased\"  # can be replaced with any suitable LLM\n",
    "        self.tokenizer_name = \"bert-base-uncased\"\n",
    "        \n",
    "        # Vision Model\n",
    "        self.vision_model_name = \"openai/clip-vit-base-patch32\"\n",
    "        \n",
    "        # ScienceQA Dataset\n",
    "        self.file_path = \"/Users/Viku/Datasets/ScienceQA\"\n",
    "        self.train_path = \"/Users/Viku/Datasets/ScienceQA/train/train.json\"\n",
    "        self.val_path = \"/Users/Viku/Datasets/ScienceQA/val/val.json\"\n",
    "        self.max_seq_length = 512\n",
    "        self.batch_size = 4\n",
    "        \n",
    "        # Training\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 0.01\n",
    "        self.epochs = 3\n",
    "        self.warmup_steps = 100\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.gradient_accumulation_steps = 8\n",
    "        \n",
    "        # RL Training\n",
    "        self.ppo_epochs = 4\n",
    "        self.reward_scale = 0.01\n",
    "        self.clip_param = 0.2\n",
    "        self.value_loss_coef = 0.5\n",
    "        self.entropy_coef = 0.01\n",
    "        \n",
    "        # Transformer Refiner\n",
    "        self.refiner_model_name = \"bert-base-uncased\"  # Can be smaller than main model\n",
    "        self.refiner_learning_rate = 2e-5\n",
    "        self.refiner_weight_decay = 0.01\n",
    "        self.refiner_batch_size = 16\n",
    "        self.refiner_epochs = 2\n",
    "        self.refiner_max_seq_length = 256  # Can be shorter than main model\n",
    "        \n",
    "        # Retrieval\n",
    "        self.retrieval_top_k = 3\n",
    "        self.embedding_dim = 768\n",
    "        \n",
    "        # Reflection\n",
    "        self.reflection_threshold = 0.7\n",
    "        \n",
    "        # Paths\n",
    "        self.output_dir = \"outputs/\"\n",
    "        self.checkpoint_dir = \"checkpoints/\"\n",
    "        self.exemplar_path = \"data/exemplars.json\"\n",
    "        \n",
    "        # Device\n",
    "        os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "        self.device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "\n",
    "        # Self Training\n",
    "        self.max_answer_length = 64\n",
    "        self.rl_updates = 1000\n",
    "        self.self_training_iterations = 3\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 15:47:12,567 - __main__ - INFO - Initializing BERT model: bert-base-uncased\n",
      "2025-03-06 15:47:15,753 - __main__ - INFO - BERT model initialized successfully\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertLMHeadModel, AutoImageProcessor, BertConfig\n",
    "\n",
    "# Define the initialize_bert function\n",
    "def initialize_bert():\n",
    "    logger.info(f\"Initializing BERT model: {config.model_name}\")\n",
    "    \n",
    "    # First get the original config\n",
    "    bert_config = BertConfig.from_pretrained(config.model_name)\n",
    "    \n",
    "    # Set is_decoder=True\n",
    "    bert_config.is_decoder = True\n",
    "    \n",
    "    # Initialize tokenizer normally\n",
    "    tokenizer = BertTokenizer.from_pretrained(config.tokenizer_name)\n",
    "    \n",
    "    # Initialize model with this modified config\n",
    "    model = BertLMHeadModel.from_pretrained(\n",
    "        config.model_name, \n",
    "        config=bert_config,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    \n",
    "    vision_processor = AutoImageProcessor.from_pretrained(config.vision_model_name)\n",
    "    return model, tokenizer, vision_processor\n",
    "\n",
    "# Now call the function to initialize the models\n",
    "bert_model, bert_tokenizer, vision_processor = initialize_bert()\n",
    "logger.info(f\"BERT model initialized successfully\")\n",
    "\n",
    "# After initializing the model\n",
    "model_path = os.path.join(config.checkpoint_dir, \"bert_decoder\")\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "bert_model.save_pretrained(model_path)\n",
    "bert_tokenizer.save_pretrained(model_path)\n",
    "\n",
    "# Then reload it\n",
    "bert_model = BertLMHeadModel.from_pretrained(model_path)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScienceQADataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, vision_processor, config, is_train=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vision_processor = vision_processor\n",
    "        self.config = config\n",
    "        self.is_train = is_train\n",
    "        self.base_dir = os.path.dirname(file_path)  # Get directory containing the JSON file\n",
    "        self.data = self.load_and_preprocess_data(file_path)\n",
    "        \n",
    "    def load_and_preprocess_data(self, file_path):\n",
    "        logger.info(f\"Loading ScienceQA data from {file_path}\")\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        processed_data = []\n",
    "        for item in data:\n",
    "            # Extract fields specific to ScienceQA\n",
    "            question = item.get('question', '')\n",
    "            context = item.get('context', '')\n",
    "            choices = item.get('choices', [])\n",
    "            answer = item.get('answer', '')\n",
    "            explanation = item.get('explanation', '')\n",
    "            question_id = item.get('id', '')  # Get question ID for image path\n",
    "            \n",
    "            # Format choices as text\n",
    "            choices_text = \"\"\n",
    "            for i, choice in enumerate(choices):\n",
    "                choices_text += f\"({chr(65+i)}) {choice} \"\n",
    "            \n",
    "            # Combine context and question\n",
    "            full_question = f\"Context: {context}\\nQuestion: {question}\\nChoices: {choices_text}\"\n",
    "            \n",
    "            # Split explanation into reasoning steps\n",
    "            steps = self.extract_reasoning_steps(explanation)\n",
    "            \n",
    "            # Process image if available\n",
    "            visual_features = None\n",
    "            if 'image' in item and item['image']:\n",
    "                # Construct image path based on question ID in train/val folder structure\n",
    "                # Assuming question_id corresponds to the folder name\n",
    "                image_folder = os.path.join(self.base_dir, str(question_id))\n",
    "                image_files = [f for f in os.listdir(image_folder) if f.endswith('.png')] if os.path.exists(image_folder) else []\n",
    "                \n",
    "                if image_files:\n",
    "                    image_path = os.path.join(image_folder, image_files[0])\n",
    "                    try:\n",
    "                        image = Image.open(image_path).convert('RGB')\n",
    "                        visual_features = self.process_image(image)\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error processing image {image_path}: {e}\")\n",
    "            \n",
    "            # Tokenize question\n",
    "            question_tokens = self.tokenizer.encode(\n",
    "                \"Let's think step by step! \" + full_question, \n",
    "                add_special_tokens=True,\n",
    "                truncation=True,\n",
    "                max_length=self.config.max_seq_length // 2\n",
    "            )\n",
    "            \n",
    "            # Tokenize each step separately\n",
    "            steps_tokens = []\n",
    "            for step in steps:\n",
    "                step_tokens = self.tokenizer.encode(\n",
    "                    step,\n",
    "                    add_special_tokens=False,\n",
    "                    truncation=True,\n",
    "                    max_length=self.config.max_seq_length // (2 * max(1, len(steps)))\n",
    "                )\n",
    "                steps_tokens.append(step_tokens)\n",
    "            \n",
    "            # Tokenize answer\n",
    "            answer_tokens = self.tokenizer.encode(\n",
    "                f\"Therefore, the answer is {answer}\",\n",
    "                add_special_tokens=False,\n",
    "                truncation=True,\n",
    "                max_length=self.config.max_seq_length // 4\n",
    "            )\n",
    "            \n",
    "            processed_data.append({\n",
    "                'question': full_question,\n",
    "                'question_tokens': question_tokens,\n",
    "                'steps': steps,\n",
    "                'steps_tokens': steps_tokens,\n",
    "                'answer': answer,\n",
    "                'answer_tokens': answer_tokens,\n",
    "                'visual_features': visual_features,\n",
    "                'has_image': visual_features is not None\n",
    "            })\n",
    "        \n",
    "        logger.info(f\"Processed {len(processed_data)} ScienceQA examples\")\n",
    "        return processed_data\n",
    "    \n",
    "    def debug_label_issues(self):\n",
    "        \"\"\"Print detailed debugging info for label issues\"\"\"\n",
    "        # Get a few samples\n",
    "        for idx in range(3):\n",
    "            sample = self.__getitem__(idx)\n",
    "            \n",
    "            # Get data\n",
    "            input_ids = sample['input_ids'].tolist()\n",
    "            attention_mask = sample['attention_mask'].tolist()\n",
    "            labels = sample['labels'].tolist()\n",
    "            \n",
    "            # Count stats\n",
    "            total = len(labels)\n",
    "            non_ignored = sum(1 for l in labels if l != -100)\n",
    "            padded = attention_mask.count(0)\n",
    "            non_padded = attention_mask.count(1)\n",
    "            \n",
    "            print(f\"\\n=== SAMPLE {idx} ===\")\n",
    "            print(f\"Total length: {total}\")\n",
    "            print(f\"Non-padded tokens: {non_padded}\")\n",
    "            print(f\"Padded tokens: {padded}\")\n",
    "            print(f\"Non-ignored labels: {non_ignored}\")\n",
    "            print(f\"Non-ignored percentage: {non_ignored/total*100:.2f}%\")\n",
    "            print(f\"Non-ignored / Non-padded ratio: {non_ignored/max(1,non_padded)*100:.2f}%\")\n",
    "            \n",
    "            # Show a sample of the tokens\n",
    "            print(\"\\nSample tokens (first 10):\")\n",
    "            for i in range(min(10, len(input_ids))):\n",
    "                input_token = self.tokenizer.decode([input_ids[i]])\n",
    "                label_val = labels[i]\n",
    "                label_token = self.tokenizer.decode([label_val]) if label_val != -100 else \"IGNORED\"\n",
    "                mask = attention_mask[i]\n",
    "                \n",
    "                print(f\"Pos {i}: Input='{input_token}' | Label='{label_token}' | Mask={mask}\")\n",
    "            \n",
    "            # Print stats on where ignored labels are\n",
    "            print(\"\\nIgnored label positions:\")\n",
    "            ignored_positions = [i for i, l in enumerate(labels) if l == -100]\n",
    "            if len(ignored_positions) > 20:\n",
    "                print(f\"{ignored_positions[:10]} ... {ignored_positions[-10:]}\")\n",
    "            else:\n",
    "                print(ignored_positions)\n",
    "    \n",
    "    def process_image(self, image):\n",
    "        \"\"\"Process image using CLIP vision encoder\"\"\"\n",
    "        print(\"Processing Image\")\n",
    "        inputs = self.vision_processor(images=image, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            vision_model = CLIPVisionModel.from_pretrained(self.config.vision_model_name)\n",
    "            vision_model.to(self.config.device)\n",
    "            outputs = vision_model(**{k: v.to(self.config.device) for k, v in inputs.items()})\n",
    "            visual_features = outputs.pooler_output.cpu().numpy()\n",
    "        return visual_features[0]  # Return the feature vector\n",
    "    \n",
    "    def extract_reasoning_steps(self, explanation):\n",
    "        \"\"\"Extract reasoning steps from explanation\"\"\"\n",
    "        # Method 1: Split by numbered steps if present\n",
    "        numbered_pattern = re.compile(r'\\d+\\.\\s+')\n",
    "        if numbered_pattern.search(explanation):\n",
    "            steps = [step.strip() for step in numbered_pattern.split(explanation) if step.strip()]\n",
    "            if steps and not steps[0][0].isdigit():  # Remove introduction if it doesn't start with a number\n",
    "                steps = steps[1:]\n",
    "            return steps or [explanation]\n",
    "        \n",
    "        # Method 2: Split by sentences assuming each sentence is a step\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', explanation)\n",
    "        if len(sentences) > 1:\n",
    "            return [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        # Default: Treat the whole explanation as one step\n",
    "        return [explanation]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Combine question, steps, and answer tokens for input\n",
    "        input_tokens = item['question_tokens'].copy()\n",
    "        for step_tokens in item['steps_tokens']:\n",
    "            input_tokens.extend(step_tokens)\n",
    "        input_tokens.extend(item['answer_tokens'])\n",
    "        \n",
    "        # Pad or truncate to max sequence length\n",
    "        if len(input_tokens) > self.config.max_seq_length:\n",
    "            input_tokens = input_tokens[:self.config.max_seq_length]\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = [1] * len(input_tokens)\n",
    "        padding_length = self.config.max_seq_length - len(input_tokens)\n",
    "        input_tokens.extend([self.tokenizer.pad_token_id] * padding_length)\n",
    "        attention_mask.extend([0] * padding_length)\n",
    "        \n",
    "        # FIXED: Create shifted labels for causal language modeling\n",
    "        # Each token should predict the next token in the sequence\n",
    "        labels = input_tokens.copy()\n",
    "        labels = [-100] + labels[:-1]  # Shift right by one position\n",
    "        \n",
    "        # Make sure padding tokens are ignored in loss\n",
    "        for i in range(len(attention_mask)):\n",
    "            if attention_mask[i] == 0:\n",
    "                labels[i] = -100\n",
    "        \n",
    "        # Optional: If you still want to focus more on reasoning steps and answer\n",
    "        # You can keep some labels as -100 but not all of them\n",
    "        question_length = len(item['question_tokens'])\n",
    "        if question_length > 10:  # Only if question is long enough\n",
    "            # Keep first and last few tokens of question, mask middle ones\n",
    "            # This maintains signal while focusing on important parts\n",
    "            middle_start = min(5, question_length // 4)\n",
    "            middle_end = min(question_length - 5, question_length * 3 // 4)\n",
    "            for i in range(middle_start, middle_end):\n",
    "                if i < self.config.max_seq_length:\n",
    "                    labels[i] = -100\n",
    "                    \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_tokens, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long),\n",
    "            'question': item['question'],\n",
    "            'steps': item['steps'],\n",
    "            'answer': item['answer'],\n",
    "            'visual_features': torch.tensor(item['visual_features'], dtype=torch.float) if item['visual_features'] is not None else torch.zeros(768),\n",
    "            'has_image': item['has_image']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Base LLM & Chain-of-Thought Generator\n",
    "\n",
    "class ChainOfThoughtGenerator(nn.Module):\n",
    "    def __init__(self, config, model=None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # First, initialize the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n",
    "        \n",
    "        # Then check for special tokens\n",
    "        special_tokens = {\"pad_token\": \"[PAD]\"} if self.tokenizer.pad_token is None else {}\n",
    "        \n",
    "        # Now initialize the model\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "        else:\n",
    "            # Initialize model from config\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(config.model_name)\n",
    "        \n",
    "        # Add special tokens if needed\n",
    "        if special_tokens:\n",
    "            self.tokenizer.add_special_tokens(special_tokens)\n",
    "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "        # The rest of the code remains the same\n",
    "        # Vision encoder for multimodal inputs\n",
    "        self.vision_processor = AutoProcessor.from_pretrained(config.vision_model_name)\n",
    "        self.vision_model = CLIPVisionModel.from_pretrained(config.vision_model_name)\n",
    "        \n",
    "        # Vision-language integration layer\n",
    "        self.vision_projection = nn.Linear(\n",
    "            self.vision_model.config.hidden_size,\n",
    "            self.model.config.hidden_size\n",
    "        )\n",
    "        \n",
    "        # Move models to device\n",
    "        self.model.to(config.device)\n",
    "        self.vision_model.to(config.device)\n",
    "        self.vision_projection.to(config.device)\n",
    "\n",
    "        print(\"ChainOfThoughtGenerator initialized with:\")\n",
    "        print(f\"  Tokenizer: {config.tokenizer_name}\")\n",
    "        print(f\"  Model: {config.model_name}\")\n",
    "        print(f\"  Vision Model: {config.vision_model_name}\")\n",
    "        print(f\"  Device: {config.device}\")\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        \"\"\"Encode image using vision model\"\"\"\n",
    "        print(\"Encoding image...\")\n",
    "        vision_inputs = self.vision_processor(images=image, return_tensors=\"pt\")\n",
    "        vision_inputs = {k: v.to(self.config.device) for k, v in vision_inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            vision_outputs = self.vision_model(**vision_inputs)\n",
    "            image_features = vision_outputs.pooler_output\n",
    "            projected_features = self.vision_projection(image_features)\n",
    "        \n",
    "        print(\"Image encoding completed.\")\n",
    "        return projected_features\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None, visual_features=None):\n",
    "        # \"\"\"Forward pass with optional visual features\"\"\"\n",
    "        # print(\"Forward pass started.\")\n",
    "        # print(f\"  Input IDs: {input_ids.shape}\")\n",
    "        # print(f\"  Attention Mask: {attention_mask.shape}\")\n",
    "        # if labels is not None:\n",
    "        #     print(f\"  Labels: {labels.shape}\")\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        # print(\"Model forward pass completed.\")\n",
    "\n",
    "        # If visual features are available, enhance the hidden states\n",
    "        if visual_features is not None:\n",
    "            # print(\"Processing visual features...\")\n",
    "            if hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:\n",
    "                projected_visual = self.vision_projection(visual_features)\n",
    "                last_hidden = outputs.hidden_states[-1]\n",
    "                enhanced_hidden = last_hidden + projected_visual.unsqueeze(1)\n",
    "                # print(\"Visual features integrated into hidden states.\")\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def generate_step_by_step(self, question, image=None, num_steps=5, max_length=512):\n",
    "        \"\"\"Generate a chain-of-thought reasoning process for a given question\"\"\"\n",
    "        print(f\"Generating reasoning for question: {question}\")\n",
    "\n",
    "        # Prepare input\n",
    "        prompt = f\"Let's think step by step! {question}\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.config.device) for k, v in inputs.items()}\n",
    "        \n",
    "        print(f\"Tokenized input: {inputs}\")\n",
    "\n",
    "        # Process image if provided\n",
    "        visual_embedding = None\n",
    "        if image is not None:\n",
    "            print(\"Processing image for reasoning...\")\n",
    "            visual_embedding = self.encode_image(image)\n",
    "\n",
    "        # Generate reasoning steps and answer\n",
    "        with torch.no_grad():\n",
    "            print(\"Generating response from model...\")\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.7,\n",
    "                do_sample=True\n",
    "            )\n",
    "        \n",
    "        # Decode the generated text\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Generated text: {generated_text}\")\n",
    "\n",
    "        # Extract reasoning steps and answer\n",
    "        generated_text = generated_text[len(prompt):]  # Remove the prompt\n",
    "        parts = generated_text.split(\"\\nTherefore, the answer is\")\n",
    "        \n",
    "        reasoning = parts[0]\n",
    "        answer = parts[1] if len(parts) > 1 else \"No clear answer provided.\"\n",
    "\n",
    "        # Split reasoning into steps\n",
    "        steps = []\n",
    "        for step in reasoning.split(\"\\n\"):\n",
    "            if step.strip():\n",
    "                steps.append(step.strip())\n",
    "\n",
    "        print(\"Generated reasoning steps:\")\n",
    "        for i, step in enumerate(steps):\n",
    "            print(f\"  Step {i+1}: {step}\")\n",
    "\n",
    "        print(f\"Final answer: {answer.strip()}\")\n",
    "\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"steps\": steps,\n",
    "            \"answer\": answer.strip(),\n",
    "            \"full_text\": generated_text\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Reflection Module\n",
    "\n",
    "class ReflectionModule(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Use a smaller model for efficiency\n",
    "        self.encoder = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "        # Scoring layers\n",
    "        self.coherence_scorer = nn.Linear(self.encoder.config.hidden_size, 1)\n",
    "        self.language_scorer = nn.Linear(self.encoder.config.hidden_size, 1)\n",
    "        self.progress_scorer = nn.Linear(self.encoder.config.hidden_size, 1)\n",
    "        \n",
    "        # Tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "        # Move to device\n",
    "        self.encoder.to(config.device)\n",
    "        self.coherence_scorer.to(config.device)\n",
    "        self.language_scorer.to(config.device)\n",
    "        self.progress_scorer.to(config.device)\n",
    "    \n",
    "    def forward(self, question, steps, previous_steps=None):\n",
    "        \"\"\"\n",
    "        Evaluate the quality of reasoning steps\n",
    "        \n",
    "        Args:\n",
    "            question: The original question\n",
    "            steps: List of reasoning steps to evaluate\n",
    "            previous_steps: Optional previous steps for context\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of scores for each step and overall\n",
    "        \"\"\"\n",
    "        all_scores = []\n",
    "        \n",
    "        # Process each step\n",
    "        for i, step in enumerate(steps):\n",
    "            # Create context from question and previous steps\n",
    "            context = question\n",
    "            if previous_steps:\n",
    "                context += \" \" + \" \".join(previous_steps)\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = self.tokenizer(\n",
    "                context, \n",
    "                step, \n",
    "                truncation=True, \n",
    "                padding=True, \n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            inputs = {k: v.to(self.config.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Get embeddings\n",
    "            with torch.no_grad():\n",
    "                outputs = self.encoder(**inputs)\n",
    "                pooled_output = outputs.last_hidden_state[:, 0]  # Use [CLS] token\n",
    "            \n",
    "            # Calculate scores\n",
    "            coherence_score = torch.sigmoid(self.coherence_scorer(pooled_output)).item()\n",
    "            language_score = torch.sigmoid(self.language_scorer(pooled_output)).item()\n",
    "            progress_score = torch.sigmoid(self.progress_scorer(pooled_output)).item()\n",
    "            \n",
    "            # Calculate a combined score\n",
    "            combined_score = (coherence_score + language_score + progress_score) / 3\n",
    "            \n",
    "            all_scores.append({\n",
    "                'step': i+1,\n",
    "                'coherence': coherence_score,\n",
    "                'language': language_score,\n",
    "                'progress': progress_score,\n",
    "                'combined': combined_score\n",
    "            })\n",
    "            \n",
    "            # Update previous steps for next iteration\n",
    "            previous_steps = (previous_steps or []) + [step]\n",
    "        \n",
    "        # Calculate overall score\n",
    "        overall_score = sum(s['combined'] for s in all_scores) / len(all_scores) if all_scores else 0\n",
    "        \n",
    "        return {\n",
    "            'step_scores': all_scores,\n",
    "            'overall_score': overall_score\n",
    "        }\n",
    "    \n",
    "    def evaluate_reasoning(self, question, steps, answer=None):\n",
    "        \"\"\"Evaluate the overall reasoning process\"\"\"\n",
    "        step_scores = self.forward(question, steps)\n",
    "        \n",
    "        # Check if reasoning meets the threshold\n",
    "        meets_threshold = step_scores['overall_score'] >= self.config.reflection_threshold\n",
    "        \n",
    "        return {\n",
    "            'scores': step_scores,\n",
    "            'meets_threshold': meets_threshold,\n",
    "            'feedback': self.generate_feedback(step_scores) if not meets_threshold else None\n",
    "        }\n",
    "    \n",
    "    def generate_feedback(self, scores):\n",
    "        \"\"\"Generate feedback based on scores\"\"\"\n",
    "        feedback = []\n",
    "        \n",
    "        for step_score in scores['step_scores']:\n",
    "            step_num = step_score['step']\n",
    "            if step_score['coherence'] < 0.6:\n",
    "                feedback.append(f\"Step {step_num} lacks coherence with the context.\")\n",
    "            if step_score['language'] < 0.6:\n",
    "                feedback.append(f\"Step {step_num} has language issues.\")\n",
    "            if step_score['progress'] < 0.6:\n",
    "                feedback.append(f\"Step {step_num} doesn't make sufficient progress toward the answer.\")\n",
    "        \n",
    "        if not feedback:\n",
    "            feedback = [\"The reasoning needs improvement, but specific issues weren't identified.\"]\n",
    "        \n",
    "        return feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Retrieval Module\n",
    "\n",
    "class RetrievalModule:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.encoder = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        \n",
    "        # Move to device\n",
    "        self.encoder.to(config.device)\n",
    "        \n",
    "        # Load exemplars\n",
    "        self.exemplars = self.load_exemplars()\n",
    "        \n",
    "        # Build index for fast retrieval\n",
    "        self.index = self.build_index()\n",
    "    \n",
    "    def load_exemplars(self):\n",
    "        \"\"\"Load exemplar reasoning sequences\"\"\"\n",
    "        if not os.path.exists(self.config.exemplar_path):\n",
    "            logger.warning(f\"Exemplar file {self.config.exemplar_path} not found, using empty exemplars\")\n",
    "            return []\n",
    "        \n",
    "        with open(self.config.exemplar_path, 'r') as f:\n",
    "            exemplars = json.load(f)\n",
    "        \n",
    "        # Pre-compute embeddings for each exemplar\n",
    "        for exemplar in exemplars:\n",
    "            exemplar['embedding'] = self.encode_text(exemplar['question']).cpu().numpy()\n",
    "        \n",
    "        logger.info(f\"Loaded {len(exemplars)} exemplars\")\n",
    "        return exemplars\n",
    "    \n",
    "    def build_index(self):\n",
    "        \"\"\"Build FAISS index for fast retrieval\"\"\"\n",
    "        if not self.exemplars:\n",
    "            return None\n",
    "        \n",
    "        # Extract embeddings\n",
    "        embeddings = np.array([ex['embedding'] for ex in self.exemplars]).astype('float32')\n",
    "        \n",
    "        # Build index\n",
    "        index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        index.add(embeddings)\n",
    "        \n",
    "        return index\n",
    "    \n",
    "    def encode_text(self, text):\n",
    "        \"\"\"Encode text using the sentence transformer\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            text, \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {k: v.to(self.config.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.encoder(**inputs)\n",
    "            # Use mean pooling\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            mask = attention_mask.unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
    "            masked_embeddings = outputs.last_hidden_state * mask\n",
    "            summed = torch.sum(masked_embeddings, 1)\n",
    "            counts = torch.clamp(torch.sum(mask, 1), min=1e-9)\n",
    "            mean_pooled = summed / counts\n",
    "        \n",
    "        return mean_pooled[0]  # Return the embedding vector\n",
    "    \n",
    "    def retrieve_similar_examples(self, question, k=None):\n",
    "        \"\"\"Retrieve similar exemplars for a given question\"\"\"\n",
    "        if k is None:\n",
    "            k = self.config.retrieval_top_k\n",
    "        \n",
    "        if not self.exemplars or self.index is None:\n",
    "            return []\n",
    "        \n",
    "        # Encode the query\n",
    "        query_embedding = self.encode_text(question).cpu().numpy().reshape(1, -1).astype('float32')\n",
    "        \n",
    "        # Search for similar examples\n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        \n",
    "        # Get the exemplars\n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if idx < len(self.exemplars):\n",
    "                exemplar = self.exemplars[idx].copy()\n",
    "                exemplar['similarity'] = float(1.0 / (1.0 + distances[0][i]))  # Convert distance to similarity\n",
    "                exemplar.pop('embedding', None)  # Remove embedding from result\n",
    "                results.append(exemplar)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_demonstration_prompt(self, question, k=None):\n",
    "        \"\"\"Get a few-shot demonstration prompt based on retrieved examples\"\"\"\n",
    "        examples = self.retrieve_similar_examples(question, k)\n",
    "        \n",
    "        if not examples:\n",
    "            return f\"Let's think step by step! {question}\"\n",
    "        \n",
    "        prompt = \"I'll solve some similar problems step by step, then answer your question.\\n\\n\"\n",
    "        \n",
    "        # Add examples\n",
    "        for i, example in enumerate(examples):\n",
    "            prompt += f\"Example {i+1}:\\n\"\n",
    "            prompt += f\"Question: {example['question']}\\n\"\n",
    "            prompt += \"Reasoning:\\n\"\n",
    "            for j, step in enumerate(example['steps']):\n",
    "                prompt += f\"{j+1}. {step}\\n\"\n",
    "            prompt += f\"Answer: {example['answer']}\\n\\n\"\n",
    "        \n",
    "        # Add the current question\n",
    "        prompt += f\"Now, let's solve your question step by step!\\n{question}\\n\"\n",
    "        \n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Transformer Module\n",
    "\n",
    "class SimpleTransformerRefiner:\n",
    "    \"\"\"A lightweight transformer-based model for refining reasoning steps\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Use a small, efficient transformer model\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "        \n",
    "        # Move model to device\n",
    "        self.model.to(config.device)\n",
    "        \n",
    "    def train(self, train_examples, epochs=3, batch_size=8):\n",
    "        \"\"\"Train the refiner model on examples\"\"\"\n",
    "        # Prepare optimizer\n",
    "        optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.refiner_learning_rate or 5e-5,\n",
    "            weight_decay=self.config.refiner_weight_decay or 0.01\n",
    "        )\n",
    "        \n",
    "        # Prepare data\n",
    "        train_inputs = []\n",
    "        train_targets = []\n",
    "        \n",
    "        for example in train_examples:\n",
    "            # Format: \"Question: {question} Previous steps: {prev_steps} Original step: {orig_step}\"\n",
    "            context = f\"Question: {example['context']} Original step: {example['step']}\"\n",
    "            train_inputs.append(f\"refine: {context}\")\n",
    "            train_targets.append(example['step'])  # Initially target is same as input for good examples\n",
    "        \n",
    "        # Convert to dataset\n",
    "        dataset = self._prepare_dataset(train_inputs, train_targets)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Training loop\n",
    "        self.model.train()\n",
    "        total_steps = len(dataloader) * epochs\n",
    "        \n",
    "        # Create scheduler\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=int(0.1 * total_steps),\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        # Main training loop\n",
    "        global_step = 0\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            # Training with progress bar\n",
    "            pbar = tqdm(dataloader, desc=f\"Training refiner (Epoch {epoch+1}/{epochs})\")\n",
    "            for batch in pbar:\n",
    "                # Get batch\n",
    "                input_ids = batch[\"input_ids\"].to(self.config.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.config.device)\n",
    "                labels = batch[\"labels\"].to(self.config.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Update metrics\n",
    "                epoch_loss += loss.item()\n",
    "                global_step += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "                \n",
    "            # Log epoch stats\n",
    "            avg_loss = epoch_loss / len(dataloader)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}: Average Loss = {avg_loss:.4f}\")\n",
    "    \n",
    "    def _prepare_dataset(self, inputs, targets):\n",
    "        \"\"\"Prepare a dataset from inputs and targets\"\"\"\n",
    "        input_encodings = self.tokenizer(\n",
    "            inputs,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        target_encodings = self.tokenizer(\n",
    "            targets,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Create dataset\n",
    "        class RefinementDataset(torch.utils.data.Dataset):\n",
    "            def __init__(self, input_encodings, target_encodings):\n",
    "                self.input_encodings = input_encodings\n",
    "                self.target_encodings = target_encodings\n",
    "                \n",
    "            def __len__(self):\n",
    "                return len(self.input_encodings[\"input_ids\"])\n",
    "                \n",
    "            def __getitem__(self, idx):\n",
    "                return {\n",
    "                    \"input_ids\": self.input_encodings[\"input_ids\"][idx],\n",
    "                    \"attention_mask\": self.input_encodings[\"attention_mask\"][idx],\n",
    "                    \"labels\": self.target_encodings[\"input_ids\"][idx]\n",
    "                }\n",
    "                \n",
    "        dataset = RefinementDataset(input_encodings, target_encodings)\n",
    "        return dataset\n",
    "    \n",
    "    def refine_step(self, context, original_step, max_length=100):\n",
    "        \"\"\"Refine a reasoning step\"\"\"\n",
    "        # Prepare input\n",
    "        input_text = f\"refine: Question: {context} Original step: {original_step}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(input_text, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.config.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate refined step\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        # Decode output\n",
    "        refined_step = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        return refined_step\n",
    "    \n",
    "    def refine_reasoning_steps(self, question, original_steps):\n",
    "        \"\"\"Refine a sequence of reasoning steps\"\"\"\n",
    "        refined_steps = []\n",
    "        context = question\n",
    "        \n",
    "        for i, step in enumerate(original_steps):\n",
    "            # Generate refined step\n",
    "            refined_step = self.refine_step(context, step)\n",
    "            refined_steps.append(refined_step)\n",
    "            \n",
    "            # Update context for next step\n",
    "            context += f\"\\nStep {i+1}: {refined_step}\"\n",
    "        \n",
    "        return refined_steps\n",
    "    \n",
    "    def evaluate_refinement(self, examples, reflection_module=None):\n",
    "        \"\"\"Evaluate refinement quality\"\"\"\n",
    "        if not examples:\n",
    "            return {\"success_rate\": 0, \"average_improvement\": 0}\n",
    "        \n",
    "        success_count = 0\n",
    "        total_improvement = 0\n",
    "        \n",
    "        for example in examples:\n",
    "            # Get original steps\n",
    "            question = example['question']\n",
    "            original_steps = example['steps']\n",
    "            \n",
    "            # Generate refined steps\n",
    "            refined_steps = self.refine_reasoning_steps(question, original_steps)\n",
    "            \n",
    "            # Evaluate with reflection module if available\n",
    "            if reflection_module:\n",
    "                original_score = reflection_module.evaluate_reasoning(\n",
    "                    question, original_steps\n",
    "                )['scores']['overall_score']\n",
    "                \n",
    "                refined_score = reflection_module.evaluate_reasoning(\n",
    "                    question, refined_steps\n",
    "                )['scores']['overall_score']\n",
    "                \n",
    "                improvement = refined_score - original_score\n",
    "                total_improvement += improvement\n",
    "                \n",
    "                if improvement > 0:\n",
    "                    success_count += 1\n",
    "            else:\n",
    "                # Simple evaluation (just count non-identical refinements as success)\n",
    "                changes = sum(1 for orig, ref in zip(original_steps, refined_steps) if orig != ref)\n",
    "                if changes > 0:\n",
    "                    success_count += 1\n",
    "        \n",
    "        success_rate = success_count / len(examples) * 100\n",
    "        avg_improvement = total_improvement / len(examples)\n",
    "        \n",
    "        return {\n",
    "            \"success_rate\": success_rate,\n",
    "            \"average_improvement\": avg_improvement\n",
    "        }\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        \"\"\"Save the model\"\"\"\n",
    "        self.model.save_pretrained(path)\n",
    "        self.tokenizer.save_pretrained(path)\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        \"\"\"Load the model\"\"\"\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "        self.model.to(self.config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Dual Reward Function and RL Training\n",
    "\n",
    "class RewardFunction:\n",
    "    \"\"\"Combines outcome and process rewards for RL training\"\"\"\n",
    "    def __init__(self, reflection_module, config):\n",
    "        self.reflection_module = reflection_module\n",
    "        self.config = config\n",
    "        \n",
    "        # For outcome verification\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "    def calculate_outcome_reward(self, predicted_answer, ground_truth):\n",
    "        \"\"\"Calculate reward based on correctness of final answer\"\"\"\n",
    "        # For ScienceQA, we can do a simple match check\n",
    "        # In a real system, you might use more sophisticated answer verification\n",
    "        predicted_normalized = predicted_answer.strip().lower()\n",
    "        ground_truth_normalized = ground_truth.strip().lower()\n",
    "        \n",
    "        # Check for exact match\n",
    "        if predicted_normalized == ground_truth_normalized:\n",
    "            return 1.0\n",
    "        \n",
    "        # Check for partial match (for longer answers)\n",
    "        if len(ground_truth_normalized) > 10:\n",
    "            # Using simple token overlap as a metric\n",
    "            pred_tokens = set(self.tokenizer.tokenize(predicted_normalized))\n",
    "            truth_tokens = set(self.tokenizer.tokenize(ground_truth_normalized))\n",
    "            \n",
    "            if not truth_tokens:\n",
    "                return 0.0\n",
    "                \n",
    "            overlap = len(pred_tokens.intersection(truth_tokens)) / len(truth_tokens)\n",
    "            return max(0.0, overlap - 0.3)  # Only reward significant overlap\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "    def calculate_process_reward(self, question, steps, original_steps=None):\n",
    "        \"\"\"Calculate reward based on quality of reasoning process\"\"\"\n",
    "        # Get reflection scores\n",
    "        reflection_result = self.reflection_module.evaluate_reasoning(question, steps)\n",
    "        process_score = reflection_result['scores']['overall_score']\n",
    "        \n",
    "        # If we have original steps, reward improvement\n",
    "        if original_steps:\n",
    "            original_result = self.reflection_module.evaluate_reasoning(question, original_steps)\n",
    "            original_score = original_result['scores']['overall_score']\n",
    "            \n",
    "            # Reward improvement, penalize degradation\n",
    "            improvement = process_score - original_score\n",
    "            if improvement > 0:\n",
    "                process_score += 0.2 * improvement  # Bonus for improvement\n",
    "            else:\n",
    "                process_score += 0.1 * improvement  # Smaller penalty for degradation\n",
    "        \n",
    "        return process_score\n",
    "    \n",
    "    def calculate_combined_reward(self, sample, ground_truth, original_steps=None):\n",
    "        \"\"\"Calculate combined reward from outcome and process\"\"\"\n",
    "        question = sample['question']\n",
    "        steps = sample['steps']\n",
    "        predicted_answer = sample['answer']\n",
    "        \n",
    "        # Calculate component rewards\n",
    "        outcome_reward = self.calculate_outcome_reward(predicted_answer, ground_truth)\n",
    "        process_reward = self.calculate_process_reward(question, steps, original_steps)\n",
    "        \n",
    "        # Combine rewards\n",
    "        # The balance between outcome and process rewards is important\n",
    "        # In this implementation, we favor process for ScienceQA\n",
    "        combined_reward = 0.4 * outcome_reward + 0.6 * process_reward\n",
    "        \n",
    "        return {\n",
    "            'combined': combined_reward,\n",
    "            'outcome': outcome_reward,\n",
    "            'process': process_reward,\n",
    "            'details': {\n",
    "                'answer_correct': outcome_reward > 0.9,\n",
    "                'reasoning_quality': process_reward\n",
    "            }\n",
    "        }\n",
    "\n",
    "class PPOTrainer:\n",
    "    \"\"\"PPO-based RL trainer for the reasoning model\"\"\"\n",
    "    def __init__(self, cot_generator, reward_function, transformer_refiner, config):\n",
    "        self.cot_generator = cot_generator\n",
    "        self.reward_function = reward_function\n",
    "        self.transformer_refiner = transformer_refiner  # Changed from GAN to transformer\n",
    "        self.config = config\n",
    "        \n",
    "        # Create a reference model for KL penalty\n",
    "        self.ref_model = AutoModelForCausalLM.from_pretrained(config.model_name)\n",
    "        self.ref_model.to(config.device)\n",
    "        self.ref_model.eval()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = AdamW(\n",
    "            self.cot_generator.model.parameters(),\n",
    "            lr=config.learning_rate,\n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "        \n",
    "        # Initialize policy entropy and value losses\n",
    "        self.policy_loss = 0\n",
    "        self.value_losses = []\n",
    "        self.entropy_losses = []\n",
    "    \n",
    "    def train_step(self, batch, transformer_refiner=None):\n",
    "        \"\"\"Perform a single PPO training step\"\"\"\n",
    "        # Extract data\n",
    "        input_ids = batch['input_ids'].to(self.config.device)\n",
    "        attention_mask = batch['attention_mask'].to(self.config.device)\n",
    "        questions = batch['question']\n",
    "        ground_truth_answers = batch['answer']\n",
    "        \n",
    "        # Forward pass with current policy to get initial log probs and values\n",
    "        with torch.no_grad():\n",
    "            outputs = self.cot_generator.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                return_dict=True\n",
    "            )\n",
    "            old_logits = outputs.logits\n",
    "            \n",
    "            # Extract values (implicitly learned through the LM head)\n",
    "            # In a full implementation, you would have a separate value head\n",
    "            values = torch.mean(old_logits, dim=-1)  # Simplistic value estimation\n",
    "        \n",
    "        # Generate samples from current policy\n",
    "        generated_samples = []\n",
    "        for i in range(len(questions)):\n",
    "            sample = self.cot_generator.generate_step_by_step(questions[i], image=None)\n",
    "            generated_samples.append(sample)\n",
    "        \n",
    "        # Optionally refine with transformer\n",
    "        if transformer_refiner or self.transformer_refiner:\n",
    "            refiner = transformer_refiner if transformer_refiner else self.transformer_refiner\n",
    "            for i, sample in enumerate(generated_samples):\n",
    "                refined_steps = refiner.refine_reasoning_steps(\n",
    "                    sample['question'], \n",
    "                    sample['steps']\n",
    "                )\n",
    "                generated_samples[i]['refined_steps'] = refined_steps\n",
    "        \n",
    "        # Calculate rewards\n",
    "        rewards = []\n",
    "        for i, sample in enumerate(generated_samples):\n",
    "            steps_to_evaluate = sample.get('refined_steps', sample['steps'])\n",
    "            reward = self.reward_function.calculate_combined_reward(\n",
    "                {\n",
    "                    'question': sample['question'],\n",
    "                    'steps': steps_to_evaluate,\n",
    "                    'answer': sample['answer']\n",
    "                },\n",
    "                ground_truth_answers[i]\n",
    "            )\n",
    "            rewards.append(reward['combined'])\n",
    "        \n",
    "        rewards_tensor = torch.tensor(rewards, device=self.config.device)\n",
    "        \n",
    "        # PPO optimization loop\n",
    "        for _ in range(self.config.ppo_epochs):\n",
    "            # Forward pass with current policy\n",
    "            outputs = self.cot_generator.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                return_dict=True\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Calculate new log probabilities and values\n",
    "            new_values = torch.mean(logits, dim=-1)  # Simplistic value estimation\n",
    "            \n",
    "            # Compute KL divergence penalty\n",
    "            kl_div = self._compute_kl_divergence(old_logits, logits, attention_mask)\n",
    "            \n",
    "            # Compute policy loss (PPO clipped objective)\n",
    "            # In a full implementation, you would compute proper action probabilities\n",
    "            # For simplicity, we're using a proxy based on logits difference\n",
    "            logit_diff = torch.sum(torch.abs(logits - old_logits), dim=-1)\n",
    "            policy_ratio = torch.exp(-logit_diff * 0.01)  # Proxy for probability ratio\n",
    "            \n",
    "            clipped_ratio = torch.clamp(\n",
    "                policy_ratio, \n",
    "                1.0 - self.config.clip_param, \n",
    "                1.0 + self.config.clip_param\n",
    "            )\n",
    "            \n",
    "            policy_reward = rewards_tensor.unsqueeze(-1).expand_as(policy_ratio)\n",
    "            policy_loss = -torch.min(\n",
    "                policy_ratio * policy_reward,\n",
    "                clipped_ratio * policy_reward\n",
    "            ).mean()\n",
    "            \n",
    "            # Value loss\n",
    "            value_loss = F.mse_loss(new_values, rewards_tensor.unsqueeze(-1).expand_as(new_values))\n",
    "            \n",
    "            # Entropy for exploration\n",
    "            # Simplified entropy calculation\n",
    "            entropy = torch.mean(torch.std(logits, dim=-1))\n",
    "            entropy_loss = -self.config.entropy_coef * entropy\n",
    "            \n",
    "            # Total loss\n",
    "            loss = policy_loss + self.config.value_loss_coef * value_loss + entropy_loss + 0.01 * kl_div\n",
    "            \n",
    "            # Backward and optimize\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.cot_generator.model.parameters(), self.config.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Store metrics\n",
    "            self.policy_loss = policy_loss.item()\n",
    "            self.value_losses.append(value_loss.item())\n",
    "            self.entropy_losses.append(entropy_loss.item())\n",
    "        \n",
    "        return {\n",
    "            'policy_loss': self.policy_loss,\n",
    "            'value_loss': sum(self.value_losses) / len(self.value_losses),\n",
    "            'entropy_loss': sum(self.entropy_losses) / len(self.entropy_losses),\n",
    "            'mean_reward': rewards_tensor.mean().item(),\n",
    "            'transformer_refinement': transformer_refiner is not None or self.transformer_refiner is not None\n",
    "        }\n",
    "    \n",
    "    def _compute_kl_divergence(self, old_logits, new_logits, attention_mask):\n",
    "        \"\"\"Compute KL divergence between old and new policies\"\"\"\n",
    "        old_probs = F.softmax(old_logits, dim=-1)\n",
    "        new_probs = F.softmax(new_logits, dim=-1)\n",
    "        \n",
    "        # KL divergence\n",
    "        kl = old_probs * (torch.log(old_probs) - torch.log(new_probs))\n",
    "        kl = kl.sum(-1)\n",
    "        \n",
    "        # Apply attention mask\n",
    "        kl = kl * attention_mask.float()\n",
    "        \n",
    "        # Average over non-masked tokens\n",
    "        kl = kl.sum() / attention_mask.float().sum()\n",
    "        \n",
    "        return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Self-Training and Distillation\n",
    "\n",
    "class SelfTrainer:\n",
    "    \"\"\"Self-training through iterative pseudo-labeling\"\"\"\n",
    "    def __init__(self, cot_generator, config):\n",
    "        self.cot_generator = cot_generator\n",
    "        self.config = config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n",
    "        \n",
    "        # Optimizer for fine-tuning\n",
    "        self.optimizer = AdamW(\n",
    "            self.cot_generator.model.parameters(),\n",
    "            lr=config.learning_rate,\n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "        \n",
    "        # LR scheduler\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=config.warmup_steps,\n",
    "            num_training_steps=1000  # Will be updated when dataset size is known\n",
    "        )\n",
    "    \n",
    "    def generate_pseudo_labels(self, unlabeled_data, ground_truth_answers=None):\n",
    "        \"\"\"Generate pseudo-labels for unlabeled data\"\"\"\n",
    "        pseudo_labeled_data = []\n",
    "        \n",
    "        for i, sample in enumerate(unlabeled_data):\n",
    "            question = sample['question']\n",
    "            \n",
    "            # Generate reasoning steps and answer\n",
    "            generated = self.cot_generator.generate_step_by_step(question, image=None)\n",
    "            \n",
    "            # Check if the answer is correct (if ground truth is available)\n",
    "            is_correct = False\n",
    "            if ground_truth_answers is not None:\n",
    "                ground_truth = ground_truth_answers[i]\n",
    "                predicted = generated['answer'].strip().lower()\n",
    "                ground_truth = ground_truth.strip().lower()\n",
    "                is_correct = predicted == ground_truth\n",
    "            \n",
    "            # Only include correct answers or all if no ground truth\n",
    "            if is_correct or ground_truth_answers is None:\n",
    "                pseudo_labeled_data.append({\n",
    "                    'question': question,\n",
    "                    'steps': generated['steps'],\n",
    "                    'answer': generated['answer'],\n",
    "                    'confidence': 1.0  # In a real implementation, you'd use model confidence\n",
    "                })\n",
    "        \n",
    "        return pseudo_labeled_data\n",
    "    \n",
    "    def finetune_on_pseudo_labels(self, pseudo_labeled_data, epochs=None):\n",
    "        \"\"\"Fine-tune model on pseudo-labeled data\"\"\"\n",
    "        if epochs is None:\n",
    "            epochs = self.config.epochs\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = self._create_dataset_from_samples(pseudo_labeled_data)\n",
    "        \n",
    "        # Adjust scheduler\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=self.config.warmup_steps,\n",
    "            num_training_steps=epochs * len(dataset)\n",
    "        )\n",
    "        \n",
    "        # Fine-tuning loop\n",
    "        self.cot_generator.model.train()\n",
    "        total_loss = 0\n",
    "        global_step = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            logger.info(f\"Starting epoch {epoch+1}/{epochs}\")\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            for batch in dataset:\n",
    "                # Move batch to device\n",
    "                batch = {k: v.to(self.config.device) if isinstance(v, torch.Tensor) else v \n",
    "                         for k, v in batch.items()}\n",
    "                \n",
    "                # Forward and backward\n",
    "                outputs = self.cot_generator.model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['labels']\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.cot_generator.model.parameters(), \n",
    "                    self.config.max_grad_norm\n",
    "                )\n",
    "                \n",
    "                # Update weights\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                global_step += 1\n",
    "                \n",
    "                # Log progress\n",
    "                if global_step % 50 == 0:\n",
    "                    logger.info(f\"Step {global_step}: loss = {loss.item()}\")\n",
    "            \n",
    "            avg_epoch_loss = epoch_loss / len(dataset)\n",
    "            total_loss += avg_epoch_loss\n",
    "            logger.info(f\"Epoch {epoch+1} completed: Average loss = {avg_epoch_loss}\")\n",
    "        \n",
    "        avg_loss = total_loss / epochs\n",
    "        logger.info(f\"Fine-tuning completed: Average loss = {avg_loss}\")\n",
    "        \n",
    "        return avg_loss\n",
    "    \n",
    "    def _create_dataset_from_samples(self, samples):\n",
    "        \"\"\"Create a dataset from generated samples\"\"\"\n",
    "        dataset = []\n",
    "        \n",
    "        for sample in samples:\n",
    "            question = sample['question']\n",
    "            steps = sample['steps']\n",
    "            answer = sample['answer']\n",
    "            \n",
    "            # Prepare input text\n",
    "            input_text = f\"Let's think step by step! {question}\\n\"\n",
    "            for i, step in enumerate(steps):\n",
    "                input_text += f\"{i+1}. {step}\\n\"\n",
    "            input_text += f\"Therefore, the answer is {answer}\"\n",
    "            \n",
    "            # Tokenize\n",
    "            encodings = self.tokenizer(\n",
    "                input_text,\n",
    "                truncation=True,\n",
    "                max_length=self.config.max_seq_length,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Create labels for causal LM training\n",
    "            input_ids = encodings['input_ids'][0]\n",
    "            attention_mask = encodings['attention_mask'][0]\n",
    "            labels = input_ids.clone()\n",
    "            \n",
    "            # Mask question part in labels\n",
    "            question_tokens = self.tokenizer.encode(\n",
    "                f\"Let's think step by step! {question}\",\n",
    "                add_special_tokens=True\n",
    "            )\n",
    "            labels[:len(question_tokens)] = -100\n",
    "            \n",
    "            dataset.append({\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'labels': labels\n",
    "            })\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def self_training_loop(self, labeled_data, unlabeled_data, num_iterations=3):\n",
    "        \"\"\"Run the complete self-training loop\"\"\"\n",
    "        for iteration in range(num_iterations):\n",
    "            logger.info(f\"Starting self-training iteration {iteration+1}/{num_iterations}\")\n",
    "            \n",
    "            # Generate pseudo-labels\n",
    "            pseudo_labels = self.generate_pseudo_labels(\n",
    "                unlabeled_data,\n",
    "                ground_truth_answers=[item['answer'] for item in unlabeled_data]\n",
    "            )\n",
    "            \n",
    "            if not pseudo_labels:\n",
    "                logger.warning(\"No pseudo-labels generated. Stopping self-training.\")\n",
    "                break\n",
    "            \n",
    "            logger.info(f\"Generated {len(pseudo_labels)} pseudo-labels\")\n",
    "            \n",
    "            # Combine with labeled data\n",
    "            combined_data = labeled_data + pseudo_labels\n",
    "            \n",
    "            # Fine-tune on combined data\n",
    "            loss = self.finetune_on_pseudo_labels(combined_data)\n",
    "            \n",
    "            logger.info(f\"Iteration {iteration+1} completed: loss = {loss}\")\n",
    "            \n",
    "            # Update labeled data for next iteration\n",
    "            labeled_data = combined_data\n",
    "        \n",
    "        logger.info(\"Self-training completed\")\n",
    "        \n",
    "        # Save the final model\n",
    "        self.cot_generator.model.save_pretrained(os.path.join(self.config.output_dir, \"self_trained_model\"))\n",
    "        self.tokenizer.save_pretrained(os.path.join(self.config.output_dir, \"self_trained_tokenizer\"))\n",
    "        \n",
    "        return labeled_data\n",
    "\n",
    "class ModelDistiller:\n",
    "    \"\"\"Knowledge distillation for creating smaller, efficient models\"\"\"\n",
    "    def __init__(self, teacher_model, config, student_model_name=\"distilbert-base-uncased\"):\n",
    "        self.teacher_model = teacher_model\n",
    "        self.config = config\n",
    "        \n",
    "        # Load smaller student model\n",
    "        self.student_tokenizer = AutoTokenizer.from_pretrained(student_model_name)\n",
    "        self.student_model = AutoModelForCausalLM.from_pretrained(student_model_name)\n",
    "        \n",
    "        # Add special tokens if needed\n",
    "        special_tokens = {\"pad_token\": \"[PAD]\"} if self.student_tokenizer.pad_token is None else {}\n",
    "        if special_tokens:\n",
    "            self.student_tokenizer.add_special_tokens(special_tokens)\n",
    "            self.student_model.resize_token_embeddings(len(self.student_tokenizer))\n",
    "        \n",
    "        # Move to device\n",
    "        self.student_model.to(config.device)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = AdamW(\n",
    "            self.student_model.parameters(),\n",
    "            lr=2e-5,  # Usually higher for distillation\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        # Temperature for softening distributions\n",
    "        self.temperature = 2.0\n",
    "    \n",
    "    def distill(self, dataset, epochs=3):\n",
    "        \"\"\"Distill knowledge from teacher to student\"\"\"\n",
    "        self.student_model.train()\n",
    "        self.teacher_model.model.eval()\n",
    "        \n",
    "        total_loss = 0\n",
    "        step_count = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            logger.info(f\"Starting distillation epoch {epoch+1}/{epochs}\")\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            for batch in dataset:\n",
    "                # Convert to student tokenization\n",
    "                student_inputs = self._convert_teacher_to_student_inputs(batch)\n",
    "                \n",
    "                # Move to device\n",
    "                student_inputs = {k: v.to(self.config.device) if isinstance(v, torch.Tensor) else v\n",
    "                                 for k, v in student_inputs.items()}\n",
    "                \n",
    "                # Get teacher predictions\n",
    "                with torch.no_grad():\n",
    "                    teacher_outputs = self.teacher_model.model(\n",
    "                        input_ids=batch['input_ids'].to(self.config.device),\n",
    "                        attention_mask=batch['attention_mask'].to(self.config.device)\n",
    "                    )\n",
    "                    \n",
    "                    # Apply temperature scaling to logits\n",
    "                    teacher_logits = teacher_outputs.logits / self.temperature\n",
    "                \n",
    "                # Student forward pass\n",
    "                student_outputs = self.student_model(**student_inputs)\n",
    "                student_logits = student_outputs.logits / self.temperature\n",
    "                \n",
    "                # Compute distillation loss\n",
    "                # Standard cross-entropy loss for task performance\n",
    "                task_loss = F.cross_entropy(\n",
    "                    student_logits.view(-1, student_logits.size(-1)),\n",
    "                    student_inputs['labels'].view(-1),\n",
    "                    ignore_index=-100\n",
    "                )\n",
    "                \n",
    "                # Distillation loss (KL divergence)\n",
    "                # We need to align teacher and student token representations\n",
    "                aligned_teacher_logits = self._align_teacher_student_representations(\n",
    "                    teacher_logits, \n",
    "                    student_logits,\n",
    "                    batch['attention_mask'].to(self.config.device),\n",
    "                    student_inputs['attention_mask']\n",
    "                )\n",
    "                \n",
    "                distillation_loss = F.kl_div(\n",
    "                    F.log_softmax(student_logits, dim=-1),\n",
    "                    F.softmax(aligned_teacher_logits, dim=-1),\n",
    "                    reduction='batchmean'\n",
    "                )\n",
    "                \n",
    "                # Combine losses\n",
    "                loss = 0.5 * task_loss + 0.5 * distillation_loss\n",
    "                \n",
    "                # Backward and optimize\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.student_model.parameters(), 1.0)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # Track losses\n",
    "                epoch_loss += loss.item()\n",
    "                step_count += 1\n",
    "                \n",
    "                # Log progress\n",
    "                if step_count % 100 == 0:\n",
    "                    logger.info(f\"Distillation step {step_count}: loss = {loss.item()}\")\n",
    "            \n",
    "            avg_epoch_loss = epoch_loss / len(dataset)\n",
    "            total_loss += avg_epoch_loss\n",
    "            logger.info(f\"Distillation epoch {epoch+1} completed: Average loss = {avg_epoch_loss}\")\n",
    "        \n",
    "        avg_loss = total_loss / epochs\n",
    "        logger.info(f\"Distillation completed: Average loss = {avg_loss}\")\n",
    "        \n",
    "        # Save distilled model\n",
    "        self.student_model.save_pretrained(os.path.join(self.config.output_dir, \"distilled_model\"))\n",
    "        self.student_tokenizer.save_pretrained(os.path.join(self.config.output_dir, \"distilled_tokenizer\"))\n",
    "        \n",
    "        return self.student_model\n",
    "    \n",
    "    def _convert_teacher_to_student_inputs(self, teacher_batch):\n",
    "        \"\"\"Convert teacher batch to student tokenization\"\"\"\n",
    "        # This is a placeholder - in practice, you would need to implement \n",
    "        # conversion between different tokenizers\n",
    "        return teacher_batch\n",
    "    \n",
    "    def _align_teacher_student_representations(self, teacher_logits, student_logits, \n",
    "                                              teacher_mask, student_mask):\n",
    "        \"\"\"Align teacher and student token representations\"\"\"\n",
    "        # This is a placeholder for token alignment\n",
    "        # In practice, you'd need to implement vocabulary mapping\n",
    "        return teacher_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Integration - Full Reasoning Pipeline\n",
    "class ReasoningPipeline:\n",
    "    \"\"\"Full reasoning pipeline integrating all components\"\"\"\n",
    "    def __init__(self, config, bert_model=None, bert_tokenizer=None, vision_processor=None):\n",
    "        self.config = config\n",
    "        \n",
    "        # Use provided models/tokenizers if available, otherwise initialize from config\n",
    "        if bert_tokenizer is not None:\n",
    "            self.tokenizer = bert_tokenizer\n",
    "        else:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n",
    "            \n",
    "        if vision_processor is not None:\n",
    "            self.vision_processor = vision_processor\n",
    "        else:\n",
    "            self.vision_processor = AutoProcessor.from_pretrained(config.vision_model_name)\n",
    "        \n",
    "        # Add special tokens if needed - FIXED: Make sure all necessary special tokens are present\n",
    "        special_tokens = {}\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            special_tokens[\"pad_token\"] = \"[PAD]\"\n",
    "        if self.tokenizer.eos_token is None:\n",
    "            special_tokens[\"eos_token\"] = \"[EOS]\"\n",
    "        if self.tokenizer.bos_token is None:\n",
    "            special_tokens[\"bos_token\"] = \"[BOS]\"\n",
    "        \n",
    "        if special_tokens:\n",
    "            self.tokenizer.add_special_tokens(special_tokens)\n",
    "        \n",
    "        # FIXED: Resize embeddings if special tokens were added\n",
    "        if bert_model is not None and special_tokens:\n",
    "            bert_model.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "        # Initialize components, passing the BERT model to CoT generator if provided\n",
    "        self.cot_generator = ChainOfThoughtGenerator(config, model=bert_model)\n",
    "        \n",
    "        # FIXED: Ensure the CoT model's embedding size matches the tokenizer\n",
    "        if hasattr(self.cot_generator, 'model'):\n",
    "            self.cot_generator.model.resize_token_embeddings(len(self.tokenizer))\n",
    "            \n",
    "        self.reflection_module = ReflectionModule(config)\n",
    "        self.retrieval_module = RetrievalModule(config)\n",
    "        \n",
    "        # Replace GAN with simple transformer refiner\n",
    "        self.transformer_refiner = SimpleTransformerRefiner(config)\n",
    "        \n",
    "        # Create reward function\n",
    "        self.reward_function = RewardFunction(self.reflection_module, config)\n",
    "        \n",
    "        # Create RL trainer\n",
    "        self.ppo_trainer = PPOTrainer(self.cot_generator, self.reward_function, self.transformer_refiner, config)\n",
    "        \n",
    "        # Create self-trainer\n",
    "        self.self_trainer = SelfTrainer(self.cot_generator, config)\n",
    "        \n",
    "        # Initialize model distiller\n",
    "        self.distiller = None\n",
    "        \n",
    "        # FIXED: Set the loss_type explicitly in the config\n",
    "        if not hasattr(self.config, 'loss_type') or self.config.loss_type is None:\n",
    "            self.config.loss_type = 'custom'\n",
    "    \n",
    "    def train(self, train_file, val_file, num_rl_updates=1000, num_self_training_iterations=3):\n",
    "        \"\"\"Train the full reasoning pipeline\"\"\"\n",
    "        # Load datasets\n",
    "        train_dataset = ScienceQADataset(\n",
    "            train_file, \n",
    "            self.tokenizer, \n",
    "            self.vision_processor, \n",
    "            self.config, \n",
    "            is_train=True\n",
    "        )\n",
    "        \n",
    "        val_dataset = ScienceQADataset(\n",
    "            val_file, \n",
    "            self.tokenizer, \n",
    "            self.vision_processor, \n",
    "            self.config, \n",
    "            is_train=False\n",
    "        )\n",
    "\n",
    "        # DEBUG: Run label debugging\n",
    "        print(\"Debugging labels in the dataset:\")\n",
    "        train_dataset.debug_label_issues()\n",
    "    \n",
    "        # Create data loaders\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.config.batch_size, \n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        val_dataloader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=self.config.batch_size, \n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        # Verify model initialization\n",
    "        loss_verification = self.verify_loss_calculation()\n",
    "        if not loss_verification:\n",
    "            print(\"The model is not producing non-zero loss on test inputs!\")\n",
    "            print(\"Check your model configuration and loss calculation.\")\n",
    "\n",
    "        # Analyze dataset\n",
    "        label_stats = self.analyze_labels(train_dataloader)\n",
    "        \n",
    "        # FIXED: Check and fix label distribution before training\n",
    "        if label_stats[\"non_ignored\"] / label_stats[\"total_labels\"] < 0.05:\n",
    "            print(\" WARNING: Less than 5% of labels are non-ignored. Checking dataset preparation...\")\n",
    "            self._check_and_fix_dataset_preparation(train_dataset)\n",
    "            \n",
    "            # Re-analyze after fixes\n",
    "            print(\"Re-analyzing labels after dataset fixes...\")\n",
    "            label_stats = self.analyze_labels(train_dataloader)\n",
    "        \n",
    "        # 1. Initial supervised fine-tuning\n",
    "        logger.info(\"Starting supervised fine-tuning\")\n",
    "        self._supervised_finetuning(train_dataloader, val_dataloader)\n",
    "        \n",
    "        # 2. Train refiner instead of GAN\n",
    "        logger.info(\"Training transformer refiner\")\n",
    "        self._train_refiner(train_dataloader)\n",
    "        \n",
    "        # 3. RL fine-tuning\n",
    "        logger.info(\"Starting RL fine-tuning\")\n",
    "        self._rl_finetuning(train_dataloader, num_rl_updates)\n",
    "        \n",
    "        # 4. Self-training loop\n",
    "        logger.info(\"Starting self-training\")\n",
    "        labeled_data = train_dataset.data[:100]  # Start with a small labeled subset\n",
    "        unlabeled_data = train_dataset.data[100:]  # The rest is unlabeled\n",
    "        final_labeled_data = self.self_trainer.self_training_loop(\n",
    "            labeled_data, \n",
    "            unlabeled_data, \n",
    "            num_iterations=num_self_training_iterations\n",
    "        )\n",
    "        \n",
    "        # 5. Distillation to smaller model\n",
    "        logger.info(\"Starting model distillation\")\n",
    "        self.distiller = ModelDistiller(self.cot_generator, self.config)\n",
    "        distilled_model = self.distiller.distill(train_dataloader, epochs=3)\n",
    "        \n",
    "        logger.info(\"Training complete\")\n",
    "        \n",
    "        return {\n",
    "            'cot_generator': self.cot_generator,\n",
    "            'reflection_module': self.reflection_module,\n",
    "            'retrieval_module': self.retrieval_module,\n",
    "            'refiner': self.refiner,\n",
    "            'distilled_model': distilled_model\n",
    "        }\n",
    "    \n",
    "    # FIXED: Add method to check and fix dataset preparation issues\n",
    "    def _check_and_fix_dataset_preparation(self, dataset):\n",
    "        \"\"\"Check and fix common dataset preparation issues\"\"\"\n",
    "        print(\"Performing dataset preparation checks and fixes...\")\n",
    "        \n",
    "        # Check if the dataset has a prepare_inputs method we can modify\n",
    "        if hasattr(dataset, 'prepare_inputs'):\n",
    "            original_prepare = dataset.prepare_inputs\n",
    "            \n",
    "            # Define fixed prepare_inputs method\n",
    "            def fixed_prepare_inputs(item):\n",
    "                # Call the original preparation\n",
    "                result = original_prepare(item)\n",
    "                \n",
    "                # FIXED: Ensure labels are properly set for causal language modeling\n",
    "                # For causal LM, typically labels are the same as input_ids but shifted\n",
    "                if 'input_ids' in result and 'labels' in result:\n",
    "                    # Clone input_ids for labels\n",
    "                    input_ids = result['input_ids']\n",
    "                    \n",
    "                    # Create labels by shifting input_ids right by one position\n",
    "                    labels = torch.full_like(input_ids, -100)  # Initialize with -100\n",
    "                    \n",
    "                    # For causal LM: labels are next tokens (shifted by 1)\n",
    "                    if len(input_ids.shape) > 1:  # For batched inputs\n",
    "                        labels[:, :-1] = input_ids[:, 1:].clone()\n",
    "                    else:  # For single inputs\n",
    "                        labels[:-1] = input_ids[1:].clone()\n",
    "                    \n",
    "                    # Keep only non-padding positions for loss computation\n",
    "                    if 'attention_mask' in result:\n",
    "                        # Only compute loss on positions with attention\n",
    "                        mask = result['attention_mask'] == 1\n",
    "                        # Apply mask to labels (keep attention positions, set others to -100)\n",
    "                        labels = labels * mask + (-100) * (~mask)\n",
    "                    \n",
    "                    result['labels'] = labels\n",
    "                \n",
    "                return result\n",
    "            \n",
    "            # Replace the dataset's prepare_inputs with our fixed version\n",
    "            dataset.prepare_inputs = fixed_prepare_inputs\n",
    "            print(\" Fixed dataset preparation method\")\n",
    "        else:\n",
    "            print(\" Could not fix dataset - no prepare_inputs method found\")\n",
    "            \n",
    "        # Add additional checks and fixes as needed\n",
    "        print(\"Dataset preparation checks complete\")\n",
    "    \n",
    "    def _supervised_finetuning(self, train_dataloader, val_dataloader, epochs=None):\n",
    "        \"\"\"Supervised fine-tuning on labeled data\"\"\"\n",
    "        if epochs is None:\n",
    "            epochs = self.config.epochs\n",
    "        \n",
    "        # FIXED: Force model to use our custom loss function\n",
    "        if hasattr(self.cot_generator.model, 'config'):\n",
    "            original_loss_config = getattr(self.cot_generator.model.config, 'loss_type', None)\n",
    "            self.cot_generator.model.config.loss_type = 'custom'\n",
    "            print(f\"Updated loss_type in model config from {original_loss_config} to 'custom'\")\n",
    "        \n",
    "        # Setup optimizer\n",
    "        optimizer = AdamW(\n",
    "            self.cot_generator.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay\n",
    "        )\n",
    "        \n",
    "        # Setup scheduler\n",
    "        total_steps = len(train_dataloader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.config.warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        self.cot_generator.model.train()\n",
    "        global_step = 0\n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            # Training with tqdm progress bar\n",
    "            pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=True)\n",
    "            for batch in pbar:\n",
    "                # Move batch to device\n",
    "                input_ids = batch['input_ids'].to(self.config.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.config.device)\n",
    "                labels = batch['labels'].to(self.config.device)\n",
    "                visual_features = None\n",
    "                if batch.get('visual_features') is not None:\n",
    "                    visual_features = batch['visual_features'].to(self.config.device)\n",
    "                \n",
    "                # FIXED: Ensure we're not accidentally using the default loss\n",
    "                # Explicitly disable built-in loss calculation in forward pass\n",
    "                outputs = self.cot_generator.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=None,  # Pass None to prevent internal loss calculation\n",
    "                    visual_features=visual_features\n",
    "                )\n",
    "                \n",
    "                # Get loss using our improved loss function\n",
    "                loss = self._calculate_improved_loss(outputs, labels)\n",
    "                \n",
    "                # Diagnostic information with less verbose output\n",
    "                non_ignored = (labels != -100).sum().item()\n",
    "                total_labels = labels.numel()\n",
    "                \n",
    "                # Only log detailed diagnostics if there's an issue or very occasionally\n",
    "                if non_ignored < 5 or global_step % 50 == 0:\n",
    "                    print(f\"Non-ignored labels: {non_ignored}/{total_labels} ({non_ignored/total_labels*100:.2f}%)\")\n",
    "                    print(f\"Raw loss value: {loss.item()}\")\n",
    "                    \n",
    "                    # Check if your labels have any values within the vocabulary range\n",
    "                    valid_range = (labels >= 0) & (labels < len(self.tokenizer))\n",
    "                    valid_count = (valid_range & (labels != -100)).sum().item()\n",
    "                    print(f\"Labels in valid vocab range: {valid_count}\")\n",
    "                \n",
    "                # Zero loss detection\n",
    "                if loss.item() == 0:\n",
    "                    print(\"\\n Zero loss detected in batch!\")\n",
    "                    # Inspect some samples from this batch\n",
    "                    sample_idx = 0  # Check the first sample in batch\n",
    "                    print(f\"Input shape: {input_ids.shape}, Labels shape: {labels.shape}\")\n",
    "                    print(f\"Sample input: {self.tokenizer.decode(input_ids[sample_idx])}\")\n",
    "                    \n",
    "                    # Check where we have non-ignored labels\n",
    "                    non_ignored_pos = (labels[sample_idx] != -100).nonzero().flatten()\n",
    "                    if len(non_ignored_pos) > 0:\n",
    "                        print(f\"First few non-ignored positions: {non_ignored_pos[:10].tolist()}\")\n",
    "                        for pos in non_ignored_pos[:5]:\n",
    "                            input_token = self.tokenizer.decode([input_ids[sample_idx, pos]])\n",
    "                            label_token = self.tokenizer.decode([labels[sample_idx, pos]])\n",
    "                            print(f\"  Pos {pos}: Input='{input_token}', Label='{label_token}'\")\n",
    "                    else:\n",
    "                        print(\"No non-ignored labels found in this sample!\")\n",
    "                        \n",
    "                    # FIXED: Try to recover with a simple loss if our custom loss fails\n",
    "                    if non_ignored > 0:\n",
    "                        print(\"Attempting to recover with simple cross-entropy loss...\")\n",
    "                        shifted_logits = outputs.logits[:, :-1, :].contiguous().view(-1, outputs.logits.size(-1))\n",
    "                        shifted_labels = labels[:, 1:].contiguous().view(-1)\n",
    "                        loss = torch.nn.CrossEntropyLoss(ignore_index=-100)(shifted_logits, shifted_labels)\n",
    "                        print(f\"Recovery loss: {loss.item()}\")\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.cot_generator.model.parameters(), \n",
    "                    self.config.max_grad_norm\n",
    "                )\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                global_step += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"step\": global_step, \"device\": self.config.device})\n",
    "            \n",
    "            avg_train_loss = epoch_loss / len(train_dataloader)\n",
    "            logger.info(f\"Epoch {epoch+1}/{epochs} completed: Average training loss = {avg_train_loss}\")\n",
    "            \n",
    "            # Validation with tqdm\n",
    "            val_loss = self._evaluate(val_dataloader)\n",
    "            logger.info(f\"Validation loss: {val_loss}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                self.cot_generator.model.save_pretrained(\n",
    "                    os.path.join(self.config.checkpoint_dir, \"best_model\")\n",
    "                )\n",
    "                logger.info(\"Saved new best model\")\n",
    "    \n",
    "    def _calculate_improved_loss(self, outputs, labels):\n",
    "        \"\"\"\n",
    "        Calculate improved loss that combines cross-entropy with auxiliary losses\n",
    "        to enhance training stability and reasoning capabilities\n",
    "        \"\"\"\n",
    "        # Get logits from model outputs\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Initialize weights for each loss component\n",
    "        weights = {\n",
    "            \"ce_loss\": 1.0,  # Cross-entropy loss weight\n",
    "            \"consistency_loss\": 0.2,  # Consistency loss weight\n",
    "            \"coverage_loss\": 0.1  # Coverage loss weight\n",
    "        }\n",
    "        \n",
    "        # FIXED: Improved handling of labels, with better checks and warnings\n",
    "        # First check that our inputs are valid\n",
    "        if labels is None:\n",
    "            logger.warning(\"Labels are None, cannot calculate loss\")\n",
    "            return torch.tensor(0.0, device=logits.device, requires_grad=True)\n",
    "            \n",
    "        if logits.size(0) != labels.size(0):\n",
    "            logger.warning(f\"Batch size mismatch: logits={logits.size(0)}, labels={labels.size(0)}\")\n",
    "            return torch.tensor(0.0, device=logits.device, requires_grad=True)\n",
    "            \n",
    "        if logits.size(1) != labels.size(1):\n",
    "            # FIXED: Handle sequence length mismatch for causal LM\n",
    "            logger.warning(f\"Sequence length mismatch: logits={logits.size(1)}, labels={labels.size(1)}\")\n",
    "            # Truncate to shorter length\n",
    "            min_len = min(logits.size(1), labels.size(1))\n",
    "            logits = logits[:, :min_len, :]\n",
    "            labels = labels[:, :min_len]\n",
    "        \n",
    "        # 1. Cross-entropy loss - standard training loss\n",
    "        # Create a tensor with -100 weight values for ignored positions\n",
    "        loss_weights = torch.ones_like(labels, dtype=torch.float)\n",
    "        loss_weights[labels == -100] = 0.0\n",
    "        \n",
    "        # Count non-ignored tokens for debugging\n",
    "        non_ignored = loss_weights.sum().item()\n",
    "        if non_ignored == 0:\n",
    "            logger.warning(\"No non-ignored labels found, returning zero loss\")\n",
    "            return torch.tensor(0.0, device=logits.device, requires_grad=True)\n",
    "        \n",
    "        # FIXED: If we're dealing with a causal LM, we need to shift labels and logits\n",
    "        if logits.size(1) > 1:  # Only perform shifts for sequence lengths > 1\n",
    "            # For causal language modeling:\n",
    "            # - predictions at position i should be for the token at position i+1\n",
    "            # - we want logits[:, :-1, :] and labels[:, 1:]\n",
    "            shifted_logits = logits[:, :-1, :].contiguous()\n",
    "            shifted_labels = labels[:, 1:].contiguous()\n",
    "            shifted_weights = loss_weights[:, 1:].contiguous()\n",
    "        else:\n",
    "            # For single token prediction, no need to shift\n",
    "            shifted_logits = logits\n",
    "            shifted_labels = labels\n",
    "            shifted_weights = loss_weights\n",
    "            \n",
    "        # Create a loss function with ignore_index=-100\n",
    "        ce_loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        flat_logits = shifted_logits.view(-1, shifted_logits.size(-1))\n",
    "        flat_labels = shifted_labels.view(-1)\n",
    "        flat_weights = shifted_weights.view(-1)\n",
    "        \n",
    "        # Calculate per-token cross-entropy loss\n",
    "        per_token_loss = ce_loss_fn(flat_logits, flat_labels)\n",
    "        \n",
    "        # Apply weights to ignore padding (-100) tokens\n",
    "        weighted_loss = per_token_loss * flat_weights\n",
    "        \n",
    "        # Get the mean loss over non-ignored tokens\n",
    "        non_ignored = flat_weights.sum()\n",
    "        ce_loss = weighted_loss.sum() / (non_ignored + 1e-8)\n",
    "        \n",
    "        # 2. Consistency loss - encourages logical coherence between steps\n",
    "        consistency_loss = torch.tensor(0.0, device=logits.device)\n",
    "        \n",
    "        # If we have enough tokens and non-zero tokens to calculate consistency\n",
    "        if non_ignored > 10:\n",
    "            # Simple consistency metric: adjacent tokens should have some correlation\n",
    "            # Get top predictions for each position\n",
    "            top_preds = logits.argmax(dim=-1)\n",
    "            \n",
    "            # Calculate consistency as prediction stability over sequences\n",
    "            for b in range(logits.size(0)):\n",
    "                # Check consecutive predictions excluding padding\n",
    "                valid_positions = (labels[b] != -100).nonzero().flatten()\n",
    "                if len(valid_positions) > 2:\n",
    "                    # Get embedding-based consistency\n",
    "                    token_embeddings = self.cot_generator.model.get_input_embeddings()(top_preds[b, valid_positions])\n",
    "                    similarities = torch.nn.functional.cosine_similarity(\n",
    "                        token_embeddings[:-1], token_embeddings[1:], dim=1\n",
    "                    )\n",
    "                    # Consistency loss: encourage smooth transitions (higher similarity)\n",
    "                    consistency_loss += (1.0 - similarities.mean())\n",
    "        \n",
    "            # Average across batch\n",
    "            consistency_loss /= logits.size(0)\n",
    "        \n",
    "        # 3. Coverage loss - encourages diverse vocabulary usage\n",
    "        coverage_loss = torch.tensor(0.0, device=logits.device)\n",
    "        \n",
    "        # If we have enough tokens to calculate coverage\n",
    "        if non_ignored > 5:\n",
    "            # Get token probability distribution averaged over sequence\n",
    "            token_probs = torch.softmax(logits.view(-1, logits.size(-1)), dim=-1)\n",
    "            mean_probs = token_probs.mean(dim=0)\n",
    "            \n",
    "            # Calculate negative entropy of this distribution\n",
    "            # Lower entropy = more concentrated on few tokens = bad\n",
    "            # Higher entropy = more diverse vocabulary = good\n",
    "            eps = 1e-8  # For numerical stability\n",
    "            entropy = -torch.sum(mean_probs * torch.log(mean_probs + eps))\n",
    "            coverage_loss = 1.0 / (entropy + eps)  # Inverse of entropy\n",
    "        \n",
    "        # Combine losses with weights\n",
    "        total_loss = (\n",
    "            weights[\"ce_loss\"] * ce_loss + \n",
    "            weights[\"consistency_loss\"] * consistency_loss + \n",
    "            weights[\"coverage_loss\"] * coverage_loss\n",
    "        )\n",
    "        \n",
    "        # Store loss components for logging - use fields that won't conflict with HF\n",
    "        outputs.ce_loss_value = ce_loss\n",
    "        outputs.consistency_loss_value = consistency_loss\n",
    "        outputs.coverage_loss_value = coverage_loss\n",
    "        outputs.total_loss_value = total_loss\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def _evaluate(self, dataloader):\n",
    "        \"\"\"Evaluate model on dataloader\"\"\"\n",
    "        self.cot_generator.model.eval()\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Evaluation with tqdm progress bar\n",
    "        pbar = tqdm(dataloader, desc=\"Evaluation\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for batch in pbar:\n",
    "                # Move batch to device\n",
    "                input_ids = batch['input_ids'].to(self.config.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.config.device)\n",
    "                labels = batch['labels'].to(self.config.device)\n",
    "                visual_features = None\n",
    "                if batch.get('visual_features') is not None:\n",
    "                    visual_features = batch['visual_features'].to(self.config.device)\n",
    "                \n",
    "                # FIXED: Consistent with training, disable internal loss calculation\n",
    "                outputs = self.cot_generator.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=None,  # Pass None to prevent internal loss calculation\n",
    "                    visual_features=visual_features\n",
    "                )\n",
    "                \n",
    "                # Calculate improved loss\n",
    "                loss = self._calculate_improved_loss(outputs, labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        # Set back to training mode\n",
    "        self.cot_generator.model.train()\n",
    "        \n",
    "        return total_loss / len(dataloader)\n",
    "    \n",
    "    def _train_refiner(self, dataloader, epochs=None):\n",
    "        \"\"\"Train the transformer refiner\"\"\"\n",
    "        if epochs is None:\n",
    "            epochs = self.config.refiner_epochs or 3\n",
    "        \n",
    "        # Create training examples\n",
    "        refiner_training_data = []\n",
    "        \n",
    "        # Sample batches for refiner training with progress bar\n",
    "        pbar = tqdm(dataloader, desc=\"Preparing refiner training data\", leave=True)\n",
    "        for batch in pbar:\n",
    "            questions = batch['question']\n",
    "            steps_list = batch['steps']\n",
    "            \n",
    "            for question, steps in zip(questions, steps_list):\n",
    "                for i in range(1, len(steps)):\n",
    "                    context = question + \" \" + \" \".join(steps[:i])\n",
    "                    refiner_training_data.append({\n",
    "                        'context': context,\n",
    "                        'step': steps[i]\n",
    "                    })\n",
    "                    \n",
    "                    # Break after a few examples per sample\n",
    "                    if i >= 3:\n",
    "                        break\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\"examples\": len(refiner_training_data)})\n",
    "            \n",
    "            # Limit training data size\n",
    "            if len(refiner_training_data) >= 1000:\n",
    "                break\n",
    "        \n",
    "        # Train the refiner\n",
    "        logger.info(f\"Training refiner for {epochs} epochs\")\n",
    "        self.refiner.train(refiner_training_data, epochs=epochs)\n",
    "        \n",
    "        # Evaluate the refiner\n",
    "        eval_results = self._evaluate_refiner(dataloader)\n",
    "        logger.info(f\"Refiner evaluation: Success rate = {eval_results['success_rate']:.1f}%, \"\n",
    "                   f\"Average improvement = {eval_results['average_improvement']:.3f}\")\n",
    "        \n",
    "        # Save refiner checkpoint\n",
    "        os.makedirs(os.path.join(self.config.checkpoint_dir, \"refiner\"), exist_ok=True)\n",
    "        self.refiner.save_model(\n",
    "            os.path.join(self.config.checkpoint_dir, \"refiner\")\n",
    "        )\n",
    "    \n",
    "    def _evaluate_refiner(self, dataloader):\n",
    "        \"\"\"Evaluate refiner quality\"\"\"\n",
    "        # Sample a few examples from validation set\n",
    "        examples = []\n",
    "        \n",
    "        # Use tqdm for sampling\n",
    "        pbar = tqdm(dataloader, desc=\"Sampling for refiner evaluation\", leave=False)\n",
    "        for batch in pbar:\n",
    "            questions = batch['question']\n",
    "            steps_list = batch['steps']\n",
    "            \n",
    "            for question, steps in zip(questions, steps_list):\n",
    "                if len(steps) >= 3:  # Ensure enough steps for evaluation\n",
    "                    examples.append({\n",
    "                        'question': question,\n",
    "                        'steps': steps\n",
    "                    })\n",
    "                \n",
    "                # Limit evaluation examples\n",
    "                if len(examples) >= 10:\n",
    "                    break\n",
    "            \n",
    "            if len(examples) >= 10:\n",
    "                break\n",
    "                \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\"samples\": len(examples)})\n",
    "        \n",
    "        # Evaluate refiner quality\n",
    "        return self.refiner.evaluate_refinement(examples, self.reflection_module)\n",
    "    \n",
    "    def _rl_finetuning(self, dataloader, num_updates):\n",
    "        \"\"\"Perform RL fine-tuning\"\"\"\n",
    "        # Setup learning rate scheduler\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.ppo_trainer.optimizer,\n",
    "            num_warmup_steps=int(0.1 * num_updates),\n",
    "            num_training_steps=num_updates\n",
    "        )\n",
    "        \n",
    "        # RL training loop\n",
    "        global_step = 0\n",
    "        best_reward = float('-inf')\n",
    "        \n",
    "        logger.info(f\"Starting RL fine-tuning: {num_updates} updates\")\n",
    "        \n",
    "        # Create progress bar for RL updates\n",
    "        pbar = tqdm(total=num_updates, desc=\"RL Fine-tuning\", leave=True)\n",
    "        \n",
    "        while global_step < num_updates:\n",
    "            # Sample batch from dataloader\n",
    "            for batch in dataloader:\n",
    "                # Perform PPO update with refiner instead of GAN\n",
    "                metrics = self.ppo_trainer.train_step(batch, self.refiner)\n",
    "                \n",
    "                # Update learning rate\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Update progress bar\n",
    "                global_step += 1\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\n",
    "                    \"policy_loss\": f\"{metrics['policy_loss']:.4f}\", \n",
    "                    \"value_loss\": f\"{metrics['value_loss']:.4f}\", \n",
    "                    \"reward\": f\"{metrics['mean_reward']:.4f}\"\n",
    "                })\n",
    "                \n",
    "                # Log metrics periodically\n",
    "                if global_step % 10 == 0:\n",
    "                    logger.info(\n",
    "                        f\"RL step {global_step}/{num_updates}: \"\n",
    "                        f\"Policy loss = {metrics['policy_loss']:.4f}, \"\n",
    "                        f\"Value loss = {metrics['value_loss']:.4f}, \"\n",
    "                        f\"Mean reward = {metrics['mean_reward']:.4f}\"\n",
    "                    )\n",
    "                \n",
    "                # Save best model\n",
    "                if metrics['mean_reward'] > best_reward:\n",
    "                    best_reward = metrics['mean_reward']\n",
    "                    self.cot_generator.model.save_pretrained(\n",
    "                        os.path.join(self.config.checkpoint_dir, \"best_rl_model\")\n",
    "                    )\n",
    "                    logger.info(f\"New best reward: {best_reward:.4f} - Saved model\")\n",
    "                \n",
    "                # Evaluate periodically\n",
    "                if global_step % 100 == 0:\n",
    "                    eval_reward = self._evaluate_rl()\n",
    "                    logger.info(f\"RL evaluation reward: {eval_reward:.4f}\")\n",
    "                    pbar.set_postfix({\n",
    "                        \"policy_loss\": f\"{metrics['policy_loss']:.4f}\", \n",
    "                        \"value_loss\": f\"{metrics['value_loss']:.4f}\", \n",
    "                        \"reward\": f\"{metrics['mean_reward']:.4f}\",\n",
    "                        \"eval\": f\"{eval_reward:.4f}\"\n",
    "                    })\n",
    "                \n",
    "                # Check if we've reached the target number of updates\n",
    "                if global_step >= num_updates:\n",
    "                    break\n",
    "        \n",
    "        # Close the progress bar\n",
    "        pbar.close()\n",
    "        logger.info(f\"RL fine-tuning complete: Best reward = {best_reward:.4f}\")\n",
    "    \n",
    "    def _evaluate_rl(self, num_samples=10):\n",
    "        \"\"\"Evaluate current policy\"\"\"\n",
    "        self.cot_generator.model.eval()\n",
    "        total_reward = 0.0\n",
    "        \n",
    "        # Sample examples for evaluation\n",
    "        examples = []\n",
    "        temp_data = self.self_trainer.generate_pseudo_labels(\n",
    "            [{'question': item} for item in self.config.eval_questions]\n",
    "        )\n",
    "        data_loader = DataLoader(temp_data, batch_size=1)\n",
    "        \n",
    "        # Use tqdm for sampling\n",
    "        pbar = tqdm(data_loader, desc=\"Sampling for RL evaluation\", leave=False)\n",
    "        for batch in pbar:\n",
    "            examples.append(batch)\n",
    "            if len(examples) >= num_samples:\n",
    "                break\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\"samples\": len(examples)})\n",
    "        \n",
    "        # Evaluate on examples with progress bar\n",
    "        eval_pbar = tqdm(examples, desc=\"RL Evaluation\", leave=False)\n",
    "        for example in eval_pbar:\n",
    "            # Generate reasoning\n",
    "            sample = self.cot_generator.generate_step_by_step(example['question'][0])\n",
    "            \n",
    "            # Calculate reward\n",
    "            reward = self.reward_function.calculate_combined_reward(\n",
    "                sample,\n",
    "                example['answer'][0]\n",
    "            )\n",
    "            \n",
    "            total_reward += reward['combined']\n",
    "            \n",
    "            # Update progress bar\n",
    "            eval_pbar.set_postfix({\"reward\": f\"{reward['combined']:.4f}\"})\n",
    "        \n",
    "        # Set back to training mode\n",
    "        self.cot_generator.model.train()\n",
    "        \n",
    "        # Calculate average reward\n",
    "        avg_reward = total_reward / len(examples)\n",
    "        return avg_reward\n",
    "    \n",
    "    def generate(self, question, image=None):\n",
    "        \"\"\"Generate reasoning for a question\"\"\"\n",
    "        logger.info(f\"Generating reasoning for: {question}\")\n",
    "        \n",
    "        # Lookup relevant information\n",
    "        logger.info(\"Retrieving context information...\")\n",
    "        retrieved_info = self.retrieval_module.retrieve(question)\n",
    "        \n",
    "        # Generate initial chain-of-thought\n",
    "        logger.info(\"Generating initial chain-of-thought...\")\n",
    "        result = self.cot_generator.generate_step_by_step(\n",
    "            question, \n",
    "            image=image,\n",
    "            retrieved_context=retrieved_info\n",
    "        )\n",
    "        \n",
    "        # Apply refiner\n",
    "        logger.info(\"Applying transformer refiner...\")\n",
    "        refined_steps = self.refiner.refine_reasoning_steps(\n",
    "            question,\n",
    "            result['steps']\n",
    "        )\n",
    "        result['refined_steps'] = refined_steps\n",
    "        \n",
    "        # Evaluate if refinement is better\n",
    "        logger.info(\"Evaluating refinement quality...\")\n",
    "        original_score = self.reflection_module.evaluate_reasoning(\n",
    "            question,\n",
    "            result['steps']\n",
    "        )['scores']['overall_score']\n",
    "        \n",
    "        refined_score = self.reflection_module.evaluate_reasoning(\n",
    "            question,\n",
    "            refined_steps\n",
    "        )['scores']['overall_score']\n",
    "        \n",
    "        # Use refined steps if better\n",
    "        if refined_score > original_score:\n",
    "            logger.info(f\"Using refined steps (score improved: {original_score:.2f} -> {refined_score:.2f})\")\n",
    "            result['steps'] = refined_steps\n",
    "            result['used_refinement'] = True\n",
    "        else:\n",
    "            logger.info(f\"Keeping original steps (refinement score: {refined_score:.2f} vs original: {original_score:.2f})\")\n",
    "        \n",
    "        # Get final reflection\n",
    "        logger.info(\"Generating final reflection...\")\n",
    "        reflection = self.reflection_module.evaluate_reasoning(\n",
    "            question,\n",
    "            result['steps']\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Reasoning generation complete\")\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'steps': result['steps'],\n",
    "            'answer': result['answer'],\n",
    "            'reflection': reflection,\n",
    "            'retrieved_context': retrieved_info\n",
    "        }\n",
    "    \n",
    "    def analyze_labels(self, dataloader, num_batches=5):\n",
    "        \"\"\"Analyze label distribution in the dataset\"\"\"\n",
    "        label_stats = {\n",
    "            \"total_labels\": 0,\n",
    "            \"non_ignored\": 0,\n",
    "            \"min_value\": float('inf'),\n",
    "            \"max_value\": -float('inf'),\n",
    "            \"label_counts\": {},\n",
    "            \"avg_length\": 0,\n",
    "            \"vocab_size\": len(self.tokenizer)\n",
    "        }\n",
    "        \n",
    "        print(f\"Analyzing labels from {num_batches} batches...\")\n",
    "        \n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "                \n",
    "            labels = batch['labels'].numpy().flatten()\n",
    "            \n",
    "            # Update statistics\n",
    "            label_stats[\"total_labels\"] += labels.size\n",
    "            label_stats[\"non_ignored\"] += (labels != -100).sum()\n",
    "            \n",
    "            # Update min/max (excluding -100)\n",
    "            valid_labels = labels[labels != -100]\n",
    "            if valid_labels.size > 0:\n",
    "                label_stats[\"min_value\"] = min(label_stats[\"min_value\"], valid_labels.min())\n",
    "                label_stats[\"max_value\"] = max(label_stats[\"max_value\"], valid_labels.max())\n",
    "                \n",
    "            # Count values\n",
    "            unique, counts = np.unique(labels, return_counts=True)\n",
    "            for val, count in zip(unique, counts):\n",
    "                if val not in label_stats[\"label_counts\"]:\n",
    "                    label_stats[\"label_counts\"][val] = 0\n",
    "                label_stats[\"label_counts\"][val] += count\n",
    "                \n",
    "            # Track average length of non-ignored sequences\n",
    "            for l in batch['labels']:\n",
    "                non_ignored_length = (l != -100).sum().item()\n",
    "                label_stats[\"avg_length\"] += non_ignored_length\n",
    "        \n",
    "        # Calculate average\n",
    "        if i > 0:\n",
    "            label_stats[\"avg_length\"] /= (i * batch['labels'].size(0))\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\nLabel Analysis Results:\")\n",
    "        print(f\"Vocabulary size: {label_stats['vocab_size']}\")\n",
    "        print(f\"Total labels: {label_stats['total_labels']}\")\n",
    "        print(f\"Non-ignored labels: {label_stats['non_ignored']} ({label_stats['non_ignored']/label_stats['total_labels']*100:.2f}%)\")\n",
    "        print(f\"Min value (excluding -100): {label_stats['min_value']}\")\n",
    "        print(f\"Max value: {label_stats['max_value']}\")\n",
    "        print(f\"Average non-ignored length: {label_stats['avg_length']:.1f} tokens\")\n",
    "        \n",
    "        # Check for out of vocabulary indices\n",
    "        if label_stats[\"max_value\"] >= label_stats[\"vocab_size\"]:\n",
    "            print(\"\\n WARNING: Some labels are outside the vocabulary range!\")\n",
    "            print(f\"Max label value ({label_stats['max_value']}) >= Vocabulary size ({label_stats['vocab_size']})\")\n",
    "        \n",
    "        # Value distribution excluding -100\n",
    "        print(\"\\nTop label values (excluding -100):\")\n",
    "        value_counts = {k: v for k, v in label_stats[\"label_counts\"].items() if k != -100}\n",
    "        top_values = sorted(value_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        for val, count in top_values:\n",
    "            token = self.tokenizer.decode([val])\n",
    "            print(f\"  {val} ('{token}'): {count} occurrences\")\n",
    "        \n",
    "        return label_stats\n",
    "    \n",
    "    def verify_loss_calculation(self):\n",
    "        \"\"\"Verify loss calculation with a simple input\"\"\"\n",
    "        print(\"\\nVerifying loss calculation with test input...\")\n",
    "        \n",
    "        # Create a simple input\n",
    "        text = \"This is a test sentence.\"\n",
    "        encoding = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        input_ids = encoding[\"input_ids\"].to(self.config.device)\n",
    "        attention_mask = encoding[\"attention_mask\"].to(self.config.device)\n",
    "        \n",
    "        # Create labels (shift input_ids right by one)\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        # Check if EOS token exists, use PAD token or a default token if not\n",
    "        eos_token_id = self.tokenizer.eos_token_id\n",
    "        if eos_token_id is None:\n",
    "            # Try pad token, or use a default token ID (usually 0 or 1)\n",
    "            eos_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else 0\n",
    "            print(f\"Warning: No EOS token found. Using alternative token ID: {eos_token_id}\")\n",
    "        \n",
    "        # Now create the shifted labels\n",
    "        labels = torch.cat([labels[:, 1:], torch.tensor([[eos_token_id]]).to(self.config.device)], dim=1)\n",
    "        \n",
    "        # Show the tokens and labels\n",
    "        print(\"Input tokens:\", self.tokenizer.convert_ids_to_tokens(input_ids[0]))\n",
    "        print(\"Label tokens:\", self.tokenizer.convert_ids_to_tokens(labels[0]))\n",
    "        \n",
    "        # Forward pass\n",
    "        self.cot_generator.model.train()  # Ensure in training mode\n",
    "        outputs = self.cot_generator(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        # Calculate our improved loss\n",
    "        loss = self._calculate_improved_loss(outputs, labels)\n",
    "        \n",
    "        print(f\"Test loss calculation result: {loss.item()}\")\n",
    "        \n",
    "        if loss.item() == 0:\n",
    "            print(\" WARNING: Loss is still zero on test input!\")\n",
    "            return False\n",
    "        else:\n",
    "            print(\" Loss calculation is working properly!\")\n",
    "            \n",
    "        # Check logits shape and activation\n",
    "        logits = outputs.logits\n",
    "        print(f\"Logits shape: {logits.shape}\")\n",
    "        print(f\"Logits mean: {logits.mean().item()}\")\n",
    "        print(f\"Logits std: {logits.std().item()}\")\n",
    "        \n",
    "        # Check predictions for expected tokens\n",
    "        for i in range(min(3, input_ids.shape[1])):\n",
    "            next_token_logits = logits[0, i, :]\n",
    "            top_tokens = torch.topk(next_token_logits, 5)\n",
    "            print(f\"\\nTop predictions for position {i} (input: '{self.tokenizer.decode([input_ids[0, i]])}'):\")\n",
    "            for token_id, score in zip(top_tokens.indices, top_tokens.values):\n",
    "                token = self.tokenizer.decode([token_id])\n",
    "                print(f\"  {token} (ID: {token_id}): {score.item():.4f}\")\n",
    "        \n",
    "        return loss.item() > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 15:47:17,412 - __main__ - INFO - Configuration: {'model_name': 'bert-base-uncased', 'tokenizer_name': 'bert-base-uncased', 'vision_model_name': 'openai/clip-vit-base-patch32', 'file_path': '/Users/Viku/Datasets/ScienceQA', 'train_path': '/Users/Viku/Datasets/ScienceQA/train/train.json', 'val_path': '/Users/Viku/Datasets/ScienceQA/val/val.json', 'max_seq_length': 512, 'batch_size': 4, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'epochs': 3, 'warmup_steps': 100, 'max_grad_norm': 1.0, 'gradient_accumulation_steps': 8, 'ppo_epochs': 4, 'reward_scale': 0.01, 'clip_param': 0.2, 'value_loss_coef': 0.5, 'entropy_coef': 0.01, 'refiner_model_name': 'bert-base-uncased', 'refiner_learning_rate': 2e-05, 'refiner_weight_decay': 0.01, 'refiner_batch_size': 16, 'refiner_epochs': 2, 'refiner_max_seq_length': 256, 'retrieval_top_k': 3, 'embedding_dim': 768, 'reflection_threshold': 0.7, 'output_dir': 'outputs/', 'checkpoint_dir': 'checkpoints/', 'exemplar_path': 'data/exemplars.json', 'device': device(type='mps'), 'max_answer_length': 64, 'rl_updates': 1000, 'self_training_iterations': 3}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChainOfThoughtGenerator initialized with:\n",
      "  Tokenizer: bert-base-uncased\n",
      "  Model: bert-base-uncased\n",
      "  Vision Model: openai/clip-vit-base-patch32\n",
      "  Device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 15:47:29,806 - __main__ - WARNING - Exemplar file data/exemplars.json not found, using empty exemplars\n",
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "/Users/Viku/GitHub/ReCoT/.venv/lib/python3.10/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "2025-03-06 15:47:38,965 - __main__ - INFO - Loading ScienceQA data from /Users/Viku/Datasets/ScienceQA/train/train.json\n",
      "2025-03-06 15:48:07,531 - __main__ - INFO - Processed 12726 ScienceQA examples\n",
      "2025-03-06 15:48:07,538 - __main__ - INFO - Loading ScienceQA data from /Users/Viku/Datasets/ScienceQA/val/val.json\n",
      "2025-03-06 15:48:12,721 - __main__ - INFO - Processed 4241 ScienceQA examples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging labels in the dataset:\n",
      "\n",
      "=== SAMPLE 0 ===\n",
      "Total length: 512\n",
      "Non-padded tokens: 48\n",
      "Padded tokens: 464\n",
      "Non-ignored labels: 21\n",
      "Non-ignored percentage: 4.10%\n",
      "Non-ignored / Non-padded ratio: 43.75%\n",
      "\n",
      "Sample tokens (first 10):\n",
      "Pos 0: Input='[CLS]' | Label='IGNORED' | Mask=1\n",
      "Pos 1: Input='let' | Label='[CLS]' | Mask=1\n",
      "Pos 2: Input=''' | Label='let' | Mask=1\n",
      "Pos 3: Input='s' | Label=''' | Mask=1\n",
      "Pos 4: Input='think' | Label='s' | Mask=1\n",
      "Pos 5: Input='step' | Label='IGNORED' | Mask=1\n",
      "Pos 6: Input='by' | Label='IGNORED' | Mask=1\n",
      "Pos 7: Input='step' | Label='IGNORED' | Mask=1\n",
      "Pos 8: Input='!' | Label='IGNORED' | Mask=1\n",
      "Pos 9: Input='context' | Label='IGNORED' | Mask=1\n",
      "\n",
      "Ignored label positions:\n",
      "[0, 5, 6, 7, 8, 9, 10, 11, 12, 13] ... [502, 503, 504, 505, 506, 507, 508, 509, 510, 511]\n",
      "\n",
      "=== SAMPLE 1 ===\n",
      "Total length: 512\n",
      "Non-padded tokens: 88\n",
      "Padded tokens: 424\n",
      "Non-ignored labels: 31\n",
      "Non-ignored percentage: 6.05%\n",
      "Non-ignored / Non-padded ratio: 35.23%\n",
      "\n",
      "Sample tokens (first 10):\n",
      "Pos 0: Input='[CLS]' | Label='IGNORED' | Mask=1\n",
      "Pos 1: Input='let' | Label='[CLS]' | Mask=1\n",
      "Pos 2: Input=''' | Label='let' | Mask=1\n",
      "Pos 3: Input='s' | Label=''' | Mask=1\n",
      "Pos 4: Input='think' | Label='s' | Mask=1\n",
      "Pos 5: Input='step' | Label='IGNORED' | Mask=1\n",
      "Pos 6: Input='by' | Label='IGNORED' | Mask=1\n",
      "Pos 7: Input='step' | Label='IGNORED' | Mask=1\n",
      "Pos 8: Input='!' | Label='IGNORED' | Mask=1\n",
      "Pos 9: Input='context' | Label='IGNORED' | Mask=1\n",
      "\n",
      "Ignored label positions:\n",
      "[0, 5, 6, 7, 8, 9, 10, 11, 12, 13] ... [502, 503, 504, 505, 506, 507, 508, 509, 510, 511]\n",
      "\n",
      "=== SAMPLE 2 ===\n",
      "Total length: 512\n",
      "Non-padded tokens: 101\n",
      "Padded tokens: 411\n",
      "Non-ignored labels: 34\n",
      "Non-ignored percentage: 6.64%\n",
      "Non-ignored / Non-padded ratio: 33.66%\n",
      "\n",
      "Sample tokens (first 10):\n",
      "Pos 0: Input='[CLS]' | Label='IGNORED' | Mask=1\n",
      "Pos 1: Input='let' | Label='[CLS]' | Mask=1\n",
      "Pos 2: Input=''' | Label='let' | Mask=1\n",
      "Pos 3: Input='s' | Label=''' | Mask=1\n",
      "Pos 4: Input='think' | Label='s' | Mask=1\n",
      "Pos 5: Input='step' | Label='IGNORED' | Mask=1\n",
      "Pos 6: Input='by' | Label='IGNORED' | Mask=1\n",
      "Pos 7: Input='step' | Label='IGNORED' | Mask=1\n",
      "Pos 8: Input='!' | Label='IGNORED' | Mask=1\n",
      "Pos 9: Input='context' | Label='IGNORED' | Mask=1\n",
      "\n",
      "Ignored label positions:\n",
      "[0, 5, 6, 7, 8, 9, 10, 11, 12, 13] ... [502, 503, 504, 505, 506, 507, 508, 509, 510, 511]\n",
      "\n",
      "Verifying loss calculation with test input...\n",
      "Input tokens: ['[CLS]', 'this', 'is', 'a', 'test', 'sentence', '.', '[SEP]']\n",
      "Label tokens: ['this', 'is', 'a', 'test', 'sentence', '.', '[SEP]', '[EOS]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 15:48:18,033 - __main__ - INFO - Starting supervised fine-tuning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss calculation result: 9.087302207946777\n",
      " Loss calculation is working properly!\n",
      "Logits shape: torch.Size([1, 8, 30524])\n",
      "Logits mean: -7.146275520324707\n",
      "Logits std: 2.88424015045166\n",
      "\n",
      "Top predictions for position 0 (input: '[CLS]'):\n",
      "  and (ID: 1998): 7.7626\n",
      "  of (ID: 1997): 6.7241\n",
      "  in (ID: 1999): 6.5357\n",
      "  , (ID: 1010): 6.2768\n",
      "  . (ID: 1012): 6.2642\n",
      "\n",
      "Top predictions for position 1 (input: 'this'):\n",
      "  and (ID: 1998): 7.9312\n",
      "  is (ID: 2003): 5.9540\n",
      "  in (ID: 1999): 5.9331\n",
      "  , (ID: 1010): 5.6260\n",
      "  with (ID: 2007): 4.9781\n",
      "\n",
      "Top predictions for position 2 (input: 'is'):\n",
      "  is (ID: 2003): 5.5263\n",
      "  and (ID: 1998): 4.9881\n",
      "  , (ID: 1010): 4.7009\n",
      "  act (ID: 2552): 4.6559\n",
      "  formed (ID: 2719): 4.5885\n",
      "Analyzing labels from 5 batches...\n",
      "\n",
      "Label Analysis Results:\n",
      "Vocabulary size: 30524\n",
      "Total labels: 10240\n",
      "Non-ignored labels: 483 (4.72%)\n",
      "Min value (excluding -100): 101\n",
      "Max value: 28290\n",
      "Average non-ignored length: 24.1 tokens\n",
      "\n",
      "Top label values (excluding -100):\n",
      "  1007 (')'): 33 occurrences\n",
      "  1006 ('('): 30 occurrences\n",
      "  1010 (','): 28 occurrences\n",
      "  1996 ('the'): 27 occurrences\n",
      "  1005 ('''): 21 occurrences\n",
      "  1055 ('s'): 21 occurrences\n",
      "  3568 ('therefore'): 21 occurrences\n",
      "  101 ('[CLS]'): 20 occurrences\n",
      "  102 ('[SEP]'): 20 occurrences\n",
      "  2003 ('is'): 20 occurrences\n",
      " WARNING: Less than 5% of labels are non-ignored. Checking dataset preparation...\n",
      "Performing dataset preparation checks and fixes...\n",
      " Could not fix dataset - no prepare_inputs method found\n",
      "Dataset preparation checks complete\n",
      "Re-analyzing labels after dataset fixes...\n",
      "Analyzing labels from 5 batches...\n",
      "\n",
      "Label Analysis Results:\n",
      "Vocabulary size: 30524\n",
      "Total labels: 10240\n",
      "Non-ignored labels: 467 (4.56%)\n",
      "Min value (excluding -100): 101\n",
      "Max value: 29153\n",
      "Average non-ignored length: 23.4 tokens\n",
      "\n",
      "Top label values (excluding -100):\n",
      "  1007 (')'): 37 occurrences\n",
      "  1996 ('the'): 35 occurrences\n",
      "  1006 ('('): 31 occurrences\n",
      "  1010 (','): 21 occurrences\n",
      "  101 ('[CLS]'): 20 occurrences\n",
      "  102 ('[SEP]'): 20 occurrences\n",
      "  1005 ('''): 20 occurrences\n",
      "  1055 ('s'): 20 occurrences\n",
      "  2003 ('is'): 20 occurrences\n",
      "  2292 ('let'): 20 occurrences\n",
      "Updated loss_type in model config from None to 'custom'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 [Train]:   0%|          | 0/3182 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-ignored labels: 151/2048 (7.37%)\n",
      "Raw loss value: 3.2973084449768066\n",
      "Labels in valid vocab range: 151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 [Train]:   0%|          | 2/3182 [01:17<34:02:28, 38.54s/it, loss=4.1196, step=2, device=mps]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m ReasoningPipeline(config, bert_model, bert_tokenizer, vision_processor)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Train pipeline\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m trained_components \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_rl_updates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrl_updates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_self_training_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_training_iterations\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Evaluate final model\u001b[39;00m\n\u001b[1;32m     49\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating final model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[45], line 119\u001b[0m, in \u001b[0;36mReasoningPipeline.train\u001b[0;34m(self, train_file, val_file, num_rl_updates, num_self_training_iterations)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# 1. Initial supervised fine-tuning\u001b[39;00m\n\u001b[1;32m    118\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting supervised fine-tuning\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_supervised_finetuning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# 2. Train refiner instead of GAN\u001b[39;00m\n\u001b[1;32m    122\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining transformer refiner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[45], line 303\u001b[0m, in \u001b[0;36mReasoningPipeline._supervised_finetuning\u001b[0;34m(self, train_dataloader, val_dataloader, epochs)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m    302\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 303\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcot_generator\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_grad_norm\n\u001b[1;32m    307\u001b[0m )\n\u001b[1;32m    308\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create output directories\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Setup logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(config.output_dir, \"train.log\")),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger.info(f\"Configuration: {vars(config)}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Sample evaluation questions\n",
    "config.eval_questions = [\n",
    "    \"What happens when water boils?\",\n",
    "    \"How does gravity work?\",\n",
    "    \"Why does the moon have phases?\",\n",
    "    \"What is photosynthesis?\",\n",
    "    \"How do magnets work?\"\n",
    "]\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = ReasoningPipeline(config, bert_model, bert_tokenizer, vision_processor)\n",
    "\n",
    "# Train pipeline\n",
    "trained_components = pipeline.train(\n",
    "    config.train_path,\n",
    "    config.val_path,\n",
    "    num_rl_updates=config.rl_updates,\n",
    "    num_self_training_iterations=config.self_training_iterations\n",
    ")\n",
    "\n",
    "# Evaluate final model\n",
    "logger.info(\"Evaluating final model\")\n",
    "\n",
    "# Create test dataloader\n",
    "test_dataset = ScienceQADataset(\n",
    "    config.val_path,\n",
    "    pipeline.tokenizer,\n",
    "    pipeline.vision_processor,\n",
    "    config,\n",
    "    is_train=False\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Calculate validation metrics\n",
    "val_loss = pipeline._evaluate(test_dataloader)\n",
    "logger.info(f\"Final validation loss: {val_loss:.4f}\")\n",
    "\n",
    "# Generate examples for qualitative evaluation\n",
    "logger.info(\"Generating example outputs\")\n",
    "for question in config.eval_questions:\n",
    "    result = pipeline.generate(question)\n",
    "    logger.info(f\"Question: {question}\")\n",
    "    logger.info(f\"Steps:\")\n",
    "    for i, step in enumerate(result['steps']):\n",
    "        logger.info(f\" {i+1}. {step}\")\n",
    "    logger.info(f\"Answer: {result['answer']}\")\n",
    "    logger.info(f\"Reflection score: {result['reflection']['scores']['overall_score']:.2f}\")\n",
    "    logger.info(\"---\")\n",
    "\n",
    "logger.info(\"Training and evaluation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
