{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T17:40:31.648736Z",
     "iopub.status.busy": "2025-03-11T17:40:31.648405Z",
     "iopub.status.idle": "2025-03-11T17:40:31.653314Z",
     "shell.execute_reply": "2025-03-11T17:40:31.652594Z",
     "shell.execute_reply.started": "2025-03-11T17:40:31.648711Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer\n",
    ")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from torch.optim import Adam\n",
    "import random\n",
    "from datetime import datetime\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T17:40:32.283747Z",
     "iopub.status.busy": "2025-03-11T17:40:32.283388Z",
     "iopub.status.idle": "2025-03-11T17:40:32.289299Z",
     "shell.execute_reply": "2025-03-11T17:40:32.288593Z",
     "shell.execute_reply.started": "2025-03-11T17:40:32.283719Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=11):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T17:40:32.855072Z",
     "iopub.status.busy": "2025-03-11T17:40:32.854780Z",
     "iopub.status.idle": "2025-03-11T17:40:32.860001Z",
     "shell.execute_reply": "2025-03-11T17:40:32.859089Z",
     "shell.execute_reply.started": "2025-03-11T17:40:32.855049Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple M1 MPS device\n"
     ]
    }
   ],
   "source": [
    "# Device configuration - M1 Mac specific\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple M1 MPS device\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T17:40:33.531286Z",
     "iopub.status.busy": "2025-03-11T17:40:33.530968Z",
     "iopub.status.idle": "2025-03-11T17:40:33.535283Z",
     "shell.execute_reply": "2025-03-11T17:40:33.534540Z",
     "shell.execute_reply.started": "2025-03-11T17:40:33.531259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 768\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 3\n",
    "COT_PROMPT = \"Let's solve this step-by-step. To find the answer, I'll break down the problem into smaller parts.\"\n",
    "MODEL_NAME = \"t5-base\"\n",
    "MAX_SAMPLES = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T17:40:34.093742Z",
     "iopub.status.busy": "2025-03-11T17:40:34.093360Z",
     "iopub.status.idle": "2025-03-11T17:40:34.101190Z",
     "shell.execute_reply": "2025-03-11T17:40:34.100431Z",
     "shell.execute_reply.started": "2025-03-11T17:40:34.093711Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Utility functions for extracting final answers and CoT steps\n",
    "def extract_final_answer(text):\n",
    "    \"\"\"Extract the final answer from generated text\"\"\"\n",
    "    # Look for common patterns that indicate the final answer\n",
    "    patterns = [\n",
    "        r\"answer\\s+is\\s+([\\d\\.\\-\\+\\/\\*]+)\",  # \"answer is 42\"\n",
    "        r\"answer\\s*:\\s*([\\d\\.\\-\\+\\/\\*]+)\",   # \"answer: 42\"\n",
    "        r\"final\\s+answer\\s*[:\\s]\\s*([\\d\\.\\-\\+\\/\\*]+)\", # \"final answer: 42\" \n",
    "        r\"therefore[,\\s]+(?:the\\s+)?(?:answer\\s+is\\s+)?([\\d\\.\\-\\+\\/\\*]+)\",  # \"therefore, 42\"\n",
    "        r\"thus[,\\s]+(?:the\\s+)?(?:answer\\s+is\\s+)?([\\d\\.\\-\\+\\/\\*]+)\",      # \"thus, 42\"\n",
    "        r\"=\\s*([\\d\\.\\-\\+\\/\\*]+)(?:\\s*$|\\s*\\.\\s*$)\"  # \"= 42\" at the end\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    \n",
    "    # Look for the last number in the text as a fallback\n",
    "    numbers = re.findall(r\"(\\d+(?:\\.\\d+)?)\", text)\n",
    "    if numbers:\n",
    "        return numbers[-1]\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def extract_cot_steps(answer_text):\n",
    "    # Remove the final answer part\n",
    "    final_answer_match = re.search(r\"The answer is(.*?)$\", answer_text, re.DOTALL)\n",
    "    if final_answer_match:\n",
    "        cot_text = answer_text[:final_answer_match.start()].strip()\n",
    "    else:\n",
    "        # If no \"The answer is\" pattern, assume the last line is the answer\n",
    "        lines = answer_text.strip().split(\"\\n\")\n",
    "        cot_text = \"\\n\".join(lines[:-1]) if len(lines) > 1 else \"\"\n",
    "    \n",
    "    # Split into steps\n",
    "    steps = [step.strip() for step in cot_text.split(\"\\n\") if step.strip()]\n",
    "    return steps\n",
    "\n",
    "# Safe device transfer function for M1 MPS\n",
    "def to_device(tensor_or_module):\n",
    "    \"\"\"Safely move tensors or modules to the selected device\"\"\"\n",
    "    if tensor_or_module is None:\n",
    "        return None\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                         \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    \n",
    "    try:\n",
    "        return tensor_or_module.to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not move to {device}: {e}\")\n",
    "        return tensor_or_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T17:40:34.631252Z",
     "iopub.status.busy": "2025-03-11T17:40:34.630925Z",
     "iopub.status.idle": "2025-03-11T17:40:34.639995Z",
     "shell.execute_reply": "2025-03-11T17:40:34.639251Z",
     "shell.execute_reply.started": "2025-03-11T17:40:34.631224Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GSM8KDataset(Dataset):\n",
    "    def __init__(self, split=\"train\", tokenizer=None, max_length=768, max_samples=None):\n",
    "        self.data = load_dataset(\"gsm8k\", \"main\")[split]\n",
    "        if max_samples:\n",
    "            self.data = self.data.select(range(min(max_samples, len(self.data))))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.processed_data = self.preprocess_data()\n",
    "        \n",
    "    def preprocess_data(self):\n",
    "        processed = []\n",
    "        for item in tqdm(self.data, desc=\"Preprocessing data\"):\n",
    "            question = item[\"question\"]\n",
    "            answer_with_cot = item[\"answer\"]\n",
    "            \n",
    "            # Extract the CoT steps and the final answer\n",
    "            final_answer = extract_final_answer(answer_with_cot)\n",
    "            cot_steps = extract_cot_steps(answer_with_cot)\n",
    "            \n",
    "            # Format for T5 training - improved prompt to guide the model\n",
    "            formatted_question = f\"Solve this math problem step-by-step: {question} {COT_PROMPT}\"\n",
    "            \n",
    "            processed.append({\n",
    "                \"question\": question,\n",
    "                \"formatted_question\": formatted_question,\n",
    "                \"cot_steps\": cot_steps,\n",
    "                \"final_answer\": final_answer,\n",
    "                \"full_answer\": answer_with_cot\n",
    "            })\n",
    "        return processed\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.processed_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.processed_data[idx]\n",
    "        \n",
    "        if self.tokenizer:\n",
    "            # Prepare input with task-specific prefix for T5\n",
    "            input_text = item[\"formatted_question\"]\n",
    "            target_text = item[\"full_answer\"]\n",
    "            \n",
    "            # Improved tokenization with more balanced token allocation\n",
    "            try:\n",
    "                inputs = self.tokenizer(\n",
    "                    input_text,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=self.max_length // 3,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                targets = self.tokenizer(\n",
    "                    target_text,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=self.max_length * 2 // 3,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                # Ensure raw_answer is not None or empty\n",
    "                raw_answer = item[\"final_answer\"]\n",
    "                if not raw_answer:\n",
    "                    # Try to extract it from full answer\n",
    "                    raw_answer = extract_final_answer(item[\"full_answer\"])\n",
    "                    if not raw_answer:\n",
    "                        raw_answer = \"unknown\"  # Fallback\n",
    "                \n",
    "                return {\n",
    "                    \"input_ids\": inputs.input_ids.squeeze(),\n",
    "                    \"attention_mask\": inputs.attention_mask.squeeze(),\n",
    "                    \"labels\": targets.input_ids.squeeze(),\n",
    "                    \"raw_question\": item[\"question\"],\n",
    "                    \"raw_cot\": item[\"cot_steps\"],\n",
    "                    \"raw_answer\": raw_answer\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"Error tokenizing item {idx}: {e}\")\n",
    "                # Improved error handling with logging\n",
    "                dummy_tensor = torch.zeros(self.max_length, dtype=torch.long)\n",
    "                return {\n",
    "                    \"input_ids\": dummy_tensor,\n",
    "                    \"attention_mask\": dummy_tensor,\n",
    "                    \"labels\": dummy_tensor,\n",
    "                    \"raw_question\": item[\"question\"],\n",
    "                    \"raw_cot\": [],\n",
    "                    \"raw_answer\": \"unknown\"\n",
    "                }\n",
    "        else:\n",
    "            return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T17:40:35.192142Z",
     "iopub.status.busy": "2025-03-11T17:40:35.191846Z",
     "iopub.status.idle": "2025-03-11T17:40:35.217568Z",
     "shell.execute_reply": "2025-03-11T17:40:35.216731Z",
     "shell.execute_reply.started": "2025-03-11T17:40:35.192120Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CoTGenerator:\n",
    "    def __init__(self, model_name=\"t5-base\", local_dir=\"./models/t5_base_cache\"):\n",
    "        self.model_name = model_name\n",
    "        self.local_dir = local_dir\n",
    "        \n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(self.local_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"Loading model {model_name}...\")\n",
    "        \n",
    "        # Check if model is already saved locally\n",
    "        if os.path.exists(os.path.join(self.local_dir, \"pytorch_model.bin\")) and \\\n",
    "           os.path.exists(os.path.join(self.local_dir, \"tokenizer_config.json\")):\n",
    "            print(f\"Found existing model at {self.local_dir}. Loading locally...\")\n",
    "            self._load_local_model()\n",
    "        else:\n",
    "            print(f\"Model not found locally. Downloading {model_name}...\")\n",
    "            self._download_model()\n",
    "    \n",
    "    def _download_model(self):\n",
    "        try:\n",
    "            # Download tokenizer and save it immediately\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(\n",
    "                self.model_name,\n",
    "                cache_dir=self.local_dir,\n",
    "                use_fast=True\n",
    "            )\n",
    "            print(f\"Tokenizer downloaded and saved to {self.local_dir}\")\n",
    "            \n",
    "            # Download model and save it immediately\n",
    "            try:\n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                                    \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "                self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    cache_dir=self.local_dir,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                    torch_dtype=torch.float16 if device.type != \"cpu\" else torch.float32\n",
    "                )\n",
    "                self.model = self.model.to(device)\n",
    "                print(f\"Model downloaded and moved to {device}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model to device: {e}\")\n",
    "                print(\"Falling back to CPU\")\n",
    "                self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    cache_dir=self.local_dir\n",
    "                )\n",
    "                print(f\"Model downloaded (CPU version)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading model: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def _load_local_model(self):\n",
    "        try:\n",
    "            # Load locally saved tokenizer\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(self.local_dir)\n",
    "            \n",
    "            # Load locally saved model\n",
    "            try:\n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                                    \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "                self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "                    self.local_dir,\n",
    "                    torch_dtype=torch.float16 if device.type != \"cpu\" else torch.float32\n",
    "                )\n",
    "                self.model = self.model.to(device)\n",
    "                print(f\"Model loaded from {self.local_dir} and moved to {device}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model to device: {e}\")\n",
    "                print(\"Falling back to CPU\")\n",
    "                self.model = T5ForConditionalGeneration.from_pretrained(self.local_dir)\n",
    "                print(f\"Model loaded from {self.local_dir} (CPU version)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading local model: {e}\")\n",
    "            print(\"Will attempt to download from source...\")\n",
    "            self._download_model()\n",
    "    \n",
    "    def generate_step(self, question, previous_steps=None, max_length=256):\n",
    "        \"\"\"Generate a single reasoning step based on the question and previous steps\"\"\"\n",
    "        if previous_steps is None:\n",
    "            previous_steps = []\n",
    "        \n",
    "        # Construct prompt with previous steps - REVISED\n",
    "        previous_steps_text = \"\"\n",
    "        if previous_steps:\n",
    "            previous_steps_text = \"Steps so far:\\n\" + \"\\n\".join([f\"Step {i+1}: {step}\" for i, step in enumerate(previous_steps)])\n",
    "            previous_steps_text += \"\\nNext step:\"\n",
    "        \n",
    "        # Build the full prompt - REVISED\n",
    "        if previous_steps:\n",
    "            input_text = f\"Solve this math problem: {question}\\n{previous_steps_text}\"\n",
    "        else:\n",
    "            input_text = f\"Solve this math problem: {question}\\nStart step-by-step reasoning:\"\n",
    "        \n",
    "        try:\n",
    "            inputs = self.tokenizer(\n",
    "                input_text, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True,\n",
    "                max_length=max_length // 2\n",
    "            )\n",
    "            device = self.model.device\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Generate with clear stopping criteria - REVISED\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    max_length=max_length,\n",
    "                    num_return_sequences=1,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    top_k=40,\n",
    "                    num_beams=3,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "            \n",
    "            # Decode and clean up the output - REVISED\n",
    "            decoded_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Only take content after the prompt\n",
    "            prompt_end = input_text.split(\"\\n\")[-1]\n",
    "            if prompt_end in decoded_output:\n",
    "                step = decoded_output.split(prompt_end, 1)[1].strip()\n",
    "            else:\n",
    "                step = decoded_output.strip()\n",
    "            \n",
    "            # Check if this is a final answer step\n",
    "            is_final_step = (\"answer is\" in step.lower() or \n",
    "                             \"therefore\" in step.lower() or \n",
    "                             \"thus\" in step.lower() or\n",
    "                             \"final answer\" in step.lower())\n",
    "            \n",
    "            return {\n",
    "                \"step\": step,\n",
    "                \"is_final_step\": is_final_step\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating step: {e}\")\n",
    "            return {\n",
    "                \"step\": \"Error generating step\",\n",
    "                \"is_final_step\": True\n",
    "            }\n",
    "    \n",
    "    def evaluate_step(self, step, question, reflection_module=None):\n",
    "        \"\"\"Evaluate if a generated step is good or needs refinement\"\"\"\n",
    "        if reflection_module:\n",
    "            # Use the reflection module if provided\n",
    "            score = reflection_module.evaluate_step(step, question)\n",
    "            return score >= 0.5  # Return True if score is good enough\n",
    "        \n",
    "        # Basic heuristic evaluation if no reflection module\n",
    "        # Check if the step contains numbers and mathematical operations\n",
    "        has_numbers = bool(re.search(r'\\d+', step))\n",
    "        has_math_ops = bool(re.search(r'[+\\-*/=]', step))\n",
    "        reasonable_length = 10 <= len(step.split()) <= 100\n",
    "        \n",
    "        return has_numbers and has_math_ops and reasonable_length\n",
    "    \n",
    "    def refine_step(self, question, step, previous_steps=None):\n",
    "        \"\"\"Refine a step that didn't pass evaluation\"\"\"\n",
    "        if previous_steps is None:\n",
    "            previous_steps = []\n",
    "        \n",
    "        # Create a prompt asking for improvement\n",
    "        previous_steps_text = \"\"\n",
    "        if previous_steps:\n",
    "            previous_steps_text = \"Steps so far:\\n\" + \"\\n\".join([f\"{i+1}. {step}\" for i, step in enumerate(previous_steps)])\n",
    "        \n",
    "        input_text = (f\"Solve this math problem step-by-step: {question}\\n\"\n",
    "                     f\"{previous_steps_text}\\n\"\n",
    "                     f\"The following step needs improvement: {step}\\n\"\n",
    "                     f\"Improved step:\")\n",
    "        \n",
    "        try:\n",
    "            inputs = self.tokenizer(\n",
    "                input_text, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            )\n",
    "            device = self.model.device\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    max_length=256,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.6\n",
    "                )\n",
    "            \n",
    "            refined_step = self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "            return refined_step\n",
    "        except Exception as e:\n",
    "            print(f\"Error refining step: {e}\")\n",
    "            return step  # Return original if refinement fails\n",
    "    \n",
    "    def generate(self, question, max_length=768, cot_prompt=None, reflection_module=None, max_steps=8):\n",
    "        \"\"\"Generate a full chain-of-thought reasoning process step by step\"\"\"\n",
    "        # Improved initial prompt to guide better reasoning\n",
    "        if not cot_prompt:\n",
    "            cot_prompt = (\n",
    "                \"Think step-by-step to solve this math problem. \"\n",
    "                \"Write one reasoning step at a time. Each step should build on previous steps. \"\n",
    "                \"Identify the relevant information, set up equations when needed, and solve them correctly.\"\n",
    "            )\n",
    "        \n",
    "        enhanced_question = f\"{question}\\n{cot_prompt}\"\n",
    "        cot_steps = []\n",
    "        final_answer = None\n",
    "        \n",
    "        # Generate steps iteratively\n",
    "        for step_num in range(max_steps):\n",
    "            # Generate the next reasoning step\n",
    "            step_result = self.generate_step(question, cot_steps)\n",
    "            current_step = step_result[\"step\"]\n",
    "            \n",
    "            # Evaluate the step quality\n",
    "            step_is_good = self.evaluate_step(current_step, question, reflection_module)\n",
    "            \n",
    "            # Refine the step if needed (up to 2 attempts)\n",
    "            refinement_attempts = 0\n",
    "            while not step_is_good and refinement_attempts < 2:\n",
    "                refined_step = self.refine_step(question, current_step, cot_steps)\n",
    "                current_step = refined_step\n",
    "                step_is_good = self.evaluate_step(current_step, question, reflection_module)\n",
    "                refinement_attempts += 1\n",
    "            \n",
    "            # Add the step to our chain of thought\n",
    "            cot_steps.append(current_step)\n",
    "            \n",
    "            # Check if this is the final step\n",
    "            if step_result[\"is_final_step\"] or step_num == max_steps - 1:\n",
    "                break\n",
    "        \n",
    "        # Generate a final answer if needed\n",
    "        if not any(\"answer is\" in step.lower() for step in cot_steps):\n",
    "            final_step_result = self.generate_final_answer(question, cot_steps)\n",
    "            cot_steps.append(final_step_result[\"step\"])\n",
    "        \n",
    "        # Extract the final answer using the dedicated function\n",
    "        full_output = \"\\n\".join(cot_steps)\n",
    "        final_answer = extract_final_answer(full_output)\n",
    "        \n",
    "        return {\n",
    "            \"cot_steps\": cot_steps,\n",
    "            \"final_answer\": final_answer,\n",
    "            \"full_output\": full_output\n",
    "        }\n",
    "    \n",
    "    def generate_final_answer(self, question, cot_steps):\n",
    "        \"\"\"Generate a final answer step based on previous reasoning steps\"\"\"\n",
    "        steps_text = \"\\n\".join([f\"{i+1}. {step}\" for i, step in enumerate(cot_steps)])\n",
    "        \n",
    "        input_text = (f\"Solve this math problem step-by-step: {question}\\n\"\n",
    "                     f\"Steps so far:\\n{steps_text}\\n\"\n",
    "                     f\"Final answer:\")\n",
    "        \n",
    "        try:\n",
    "            inputs = self.tokenizer(\n",
    "                input_text, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            )\n",
    "            device = self.model.device\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    max_length=128,\n",
    "                    do_sample=False,  # More deterministic for final answer\n",
    "                    num_beams=3\n",
    "                )\n",
    "            \n",
    "            final_step = self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "            \n",
    "            # Make sure it starts with \"The answer is\" if it doesn't already\n",
    "            if not any(phrase in final_step.lower() for phrase in [\"answer is\", \"therefore\", \"thus\", \"final answer\"]):\n",
    "                final_step = \"The answer is \" + final_step\n",
    "                \n",
    "            return {\n",
    "                \"step\": final_step,\n",
    "                \"is_final_step\": True\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating final answer: {e}\")\n",
    "            return {\n",
    "                \"step\": \"The answer is unknown due to an error.\",\n",
    "                \"is_final_step\": True\n",
    "            }\n",
    "    \n",
    "    def save(self, path=None):\n",
    "        save_path = path if path else self.local_dir\n",
    "        try:\n",
    "            # Move to CPU before saving to avoid device-specific tensors in saved model\n",
    "            cpu_model = self.model.to(\"cpu\")\n",
    "            cpu_model.save_pretrained(save_path)\n",
    "            self.tokenizer.save_pretrained(save_path)\n",
    "            print(f\"Model saved to {save_path}\")\n",
    "            # Move back to device\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                               \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "            self.model = self.model.to(device)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {e}\")\n",
    "    \n",
    "    def load(self, path=None):\n",
    "        load_path = path if path else self.local_dir\n",
    "        try:\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(load_path)\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                               \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(load_path)\n",
    "            self.model = self.model.to(device)\n",
    "            print(f\"Model loaded from {load_path} and moved to {device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T17:40:35.322084Z",
     "iopub.status.busy": "2025-03-11T17:40:35.321793Z",
     "iopub.status.idle": "2025-03-11T17:40:35.338749Z",
     "shell.execute_reply": "2025-03-11T17:40:35.337855Z",
     "shell.execute_reply.started": "2025-03-11T17:40:35.322061Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ReflectionModule(nn.Module):\n",
    "    def __init__(self, model_name=MODEL_NAME, embedding_dim=768):\n",
    "        super(ReflectionModule, self).__init__()\n",
    "        try:\n",
    "            # Set device property on self first\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                               \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "                               \n",
    "            # Use pre-trained model for better embeddings instead of random\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.base_model = AutoModel.from_pretrained(model_name)\n",
    "            \n",
    "            # Freeze base model for stability\n",
    "            for param in self.base_model.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "            # Add trainable layers on top for step quality assessment\n",
    "            self.encoder = nn.TransformerEncoder(\n",
    "                nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=8, dropout=0.1, batch_first=True),\n",
    "                num_layers=2\n",
    "            )\n",
    "            self.fc1 = nn.Linear(embedding_dim, 256)\n",
    "            self.fc2 = nn.Linear(256, 64)\n",
    "            self.fc3 = nn.Linear(64, 1)\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "            \n",
    "            # Rule-based heuristics lookup (for early training stability)\n",
    "            self.heuristics = {\n",
    "                r'\\d+\\s*[\\+\\-\\*/]\\s*\\d+': 0.7,  # Mathematical operations\n",
    "                r'therefore|thus|so': 0.6,      # Logical connections\n",
    "                r'first|second|third|next': 0.5, # Sequential reasoning\n",
    "                r'=\\s*\\d+': 0.6,               # Equation results\n",
    "                r'equation|formula': 0.5,       # Mathematical concepts\n",
    "                r'calculate|compute': 0.5,      # Calculation indicators\n",
    "            }\n",
    "            \n",
    "            # Move model to device\n",
    "            self.to(self.device)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing reflection module: {e}\")\n",
    "            self.tokenizer = None\n",
    "            self.base_model = None\n",
    "            # Ensure device is set even in error case\n",
    "            self.device =torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                         \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        try:\n",
    "            # Get embeddings from base model\n",
    "            with torch.no_grad():\n",
    "                outputs = self.base_model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    return_dict=True\n",
    "                )\n",
    "                embeddings = outputs.last_hidden_state\n",
    "            \n",
    "            # Process through transformer encoder\n",
    "            encoded = self.encoder(embeddings)\n",
    "            \n",
    "            # Pool and process through FC layers\n",
    "            pooled = encoded.mean(dim=1)\n",
    "            x = F.relu(self.fc1(pooled))\n",
    "            x = self.dropout(x)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.dropout(x)\n",
    "            score = torch.sigmoid(self.fc3(x))\n",
    "            \n",
    "            return score\n",
    "        except Exception as e:\n",
    "            print(f\"Error in forward pass: {e}\")\n",
    "            return torch.tensor([[0.5]]).to(self.device)  # Default neutral score\n",
    "    \n",
    "    def evaluate_step(self, step_text, question_context):\n",
    "        \"\"\"Evaluate a single reasoning step with combined ML and rule-based approach\"\"\"\n",
    "        try:\n",
    "            # Apply rule-based heuristics as a fallback/supplement\n",
    "            heuristic_score = self._apply_heuristics(step_text)\n",
    "            \n",
    "            # For very short or empty steps, rely more on heuristics\n",
    "            if len(step_text.strip()) < 10:\n",
    "                return max(0.2, heuristic_score * 0.8)  # Penalize very short steps but not too harshly\n",
    "                \n",
    "            # Prepare input for model\n",
    "            combined_text = f\"Question: {question_context} Step: {step_text}\"\n",
    "            \n",
    "            # Tokenize\n",
    "            tokens = self.tokenizer(\n",
    "                combined_text, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True,\n",
    "                max_length=MAX_LENGTH\n",
    "            )\n",
    "            tokens = {k: v.to(self.device) for k, v in tokens.items()}\n",
    "            \n",
    "            # Get score from model\n",
    "            with torch.no_grad():\n",
    "                score = self.forward(tokens[\"input_ids\"], tokens[\"attention_mask\"])\n",
    "            \n",
    "            # Blend ML score with heuristic score for robustness\n",
    "            ml_score = score.item()\n",
    "            blended_score = 0.7 * ml_score + 0.3 * heuristic_score\n",
    "            \n",
    "            return blended_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in evaluate_step: {e}\")\n",
    "            return self._apply_heuristics(step_text)  # Fall back to heuristics\n",
    "    \n",
    "    def _apply_heuristics(self, text):\n",
    "        \"\"\"Apply rule-based heuristics for evaluation\"\"\"\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Start with a base score\n",
    "        score = 0.4\n",
    "        \n",
    "        # Check for various quality indicators\n",
    "        for pattern, bonus in self.heuristics.items():\n",
    "            if re.search(pattern, text):\n",
    "                score = max(score, bonus)\n",
    "        \n",
    "        # Penalize extremely short steps\n",
    "        if len(text.strip()) < 5:\n",
    "            score *= 0.5\n",
    "            \n",
    "        # Bonus for using numbers (important in math reasoning)\n",
    "        if re.search(r'\\d+', text):\n",
    "            score = min(1.0, score + 0.1)\n",
    "            \n",
    "        # Highest bonus for complete calculations with result\n",
    "        if re.search(r'\\d+\\s*[\\+\\-\\*/]\\s*\\d+\\s*=\\s*\\d+', text):\n",
    "            score = min(1.0, score + 0.2)\n",
    "            \n",
    "        return score\n",
    "    \n",
    "    def train_step(self, batch, optimizer):\n",
    "        try:\n",
    "            self.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Move batch to device\n",
    "            input_ids = batch[\"input_ids\"].to(self.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "            labels = batch[\"labels\"].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            scores = self.forward(input_ids, attention_mask)\n",
    "            \n",
    "            # Calculate loss (MSE against target quality scores)\n",
    "            loss = F.mse_loss(scores.squeeze(), labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            return loss.item()\n",
    "        except Exception as e:\n",
    "            print(f\"Error in train_step: {e}\")\n",
    "            return 1.0  # High loss as default\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save the reflection module\"\"\"\n",
    "        try:\n",
    "            torch.save({\n",
    "                'encoder_state_dict': self.encoder.state_dict(),\n",
    "                'fc1_state_dict': self.fc1.state_dict(),\n",
    "                'fc2_state_dict': self.fc2.state_dict(),\n",
    "                'fc3_state_dict': self.fc3.state_dict(),\n",
    "            }, path)\n",
    "            print(f\"Reflection module saved to {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving reflection module: {e}\")\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Load the reflection module\"\"\"\n",
    "        try:\n",
    "            checkpoint = torch.load(path)\n",
    "            self.encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "            self.fc1.load_state_dict(checkpoint['fc1_state_dict'])\n",
    "            self.fc2.load_state_dict(checkpoint['fc2_state_dict'])\n",
    "            self.fc3.load_state_dict(checkpoint['fc3_state_dict'])\n",
    "            print(f\"Reflection module loaded from {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading reflection module: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T17:40:35.555780Z",
     "iopub.status.busy": "2025-03-11T17:40:35.555426Z",
     "iopub.status.idle": "2025-03-11T17:40:35.577910Z",
     "shell.execute_reply": "2025-03-11T17:40:35.576854Z",
     "shell.execute_reply.started": "2025-03-11T17:40:35.555753Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RetrievalModule:\n",
    "    def __init__(self, embedding_model_name=MODEL_NAME):\n",
    "        try:\n",
    "            # Set device property on self first\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                               \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "            \n",
    "            # Initialize embedding cache before any usage\n",
    "            self.embedding_cache = {}\n",
    "            \n",
    "            # Use a better embedding model if available\n",
    "            # Switch to using a encoder-only model instead of seq2seq\n",
    "            # This will prevent the \"decoder_input_ids\" error\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "            \n",
    "            # Use AutoModelForPreTraining or AutoModelForMaskedLM instead of AutoModel\n",
    "            # to ensure we get an encoder-only model\n",
    "            try:\n",
    "                # First try loading as an encoder model explicitly\n",
    "                self.model = AutoModel.from_pretrained(\n",
    "                    embedding_model_name,\n",
    "                    is_decoder=False,  # Explicitly specify this is not a decoder\n",
    "                    add_cross_attention=False  # Ensure no cross-attention is expected\n",
    "                )\n",
    "            except:\n",
    "                # Fallback to a model we know works for embeddings\n",
    "                print(f\"Failed to load {embedding_model_name} as encoder-only, falling back to bert-base-uncased\")\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "                self.model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "            \n",
    "            self.model.to(self.device)\n",
    "            \n",
    "            # Keep model in eval mode\n",
    "            self.model.eval()\n",
    "            \n",
    "            # Exemplar bank with metadata for better filtering and retrieval\n",
    "            # (question, embedding, cot_steps, metadata)\n",
    "            self.exemplar_bank = []\n",
    "            \n",
    "            # Domain-specific keyword weights (math-focused)\n",
    "            self.keyword_weights = {\n",
    "                'equation': 1.5,\n",
    "                'equal': 1.5,\n",
    "                'solve': 1.3,\n",
    "                'calculate': 1.3,\n",
    "                'find': 1.2,\n",
    "                'value': 1.2,\n",
    "                'total': 1.2,\n",
    "                'average': 1.2,\n",
    "                'mean': 1.2,\n",
    "                'median': 1.2,\n",
    "                'probability': 1.5,\n",
    "                'percent': 1.3,\n",
    "                'increase': 1.2,\n",
    "                'decrease': 1.2,\n",
    "                'rate': 1.2,\n",
    "                'ratio': 1.5,\n",
    "                'proportion': 1.5,\n",
    "                'fraction': 1.5,\n",
    "                'decimal': 1.3\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing retrieval module: {e}\")\n",
    "            self.tokenizer = None\n",
    "            self.model = None\n",
    "            self.exemplar_bank = []\n",
    "            # Ensure embedding_cache is set even in error case\n",
    "            self.embedding_cache = {}\n",
    "            # Ensure device is set even in error case\n",
    "            self.device = torch.device(\"cpu\")\n",
    "    \n",
    "    def compute_embedding(self, text):\n",
    "        \"\"\"Compute embedding for text using the model\"\"\"\n",
    "        try:\n",
    "            # Check cache first\n",
    "            cache_key = text[:100]  # Use first 100 chars as key to save memory\n",
    "            if cache_key in self.embedding_cache:\n",
    "                return self.embedding_cache[cache_key]\n",
    "                \n",
    "            if self.model is None or self.tokenizer is None:\n",
    "                return np.random.randn(768)  # Fallback\n",
    "                \n",
    "            # Preprocess text for embedding\n",
    "            tokens = self.tokenizer(\n",
    "                text, \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                max_length=MAX_LENGTH, \n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            tokens = {k: v.to(self.device) for k, v in tokens.items()}\n",
    "            \n",
    "            # Get embedding from model - using only encoder part\n",
    "            with torch.no_grad():\n",
    "                # Using model output without decoder inputs\n",
    "                outputs = self.model(**tokens, return_dict=True)\n",
    "                \n",
    "                # Use CLS token or mean pooling based on model architecture\n",
    "                if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "                    embedding = outputs.pooler_output.cpu().numpy()[0]\n",
    "                else:\n",
    "                    # Use mean of last hidden states as fallback (more universal)\n",
    "                    embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()[0]\n",
    "            \n",
    "            # Apply keyword weighting\n",
    "            weighted_embedding = self.apply_keyword_weighting(text, embedding)\n",
    "            \n",
    "            # Normalize\n",
    "            norm = np.linalg.norm(weighted_embedding)\n",
    "            if norm > 0:\n",
    "                weighted_embedding = weighted_embedding / norm\n",
    "                \n",
    "            # Cache the result\n",
    "            self.embedding_cache[cache_key] = weighted_embedding\n",
    "            \n",
    "            return weighted_embedding\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error computing embedding: {e}\")\n",
    "            return np.random.randn(768)  # Fallback\n",
    "    \n",
    "    def add_exemplar(self, question, cot_steps, metadata=None):\n",
    "        \"\"\"Add an exemplar to the retrieval bank with metadata\"\"\"\n",
    "        try:\n",
    "            if not question or not cot_steps:\n",
    "                return False\n",
    "                \n",
    "            # Create default metadata if none provided\n",
    "            if metadata is None:\n",
    "                metadata = {\n",
    "                    'quality': 1.0,  # Default high quality\n",
    "                    'topics': self.extract_topics(question),\n",
    "                    'num_steps': len(cot_steps) if isinstance(cot_steps, list) else 1,\n",
    "                    'date_added': datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "            # Compute embedding\n",
    "            embedding = self.compute_embedding(question)\n",
    "            \n",
    "            # Add to exemplar bank\n",
    "            self.exemplar_bank.append((question, embedding, cot_steps, metadata))\n",
    "            \n",
    "            # Limit size to prevent memory issues\n",
    "            if len(self.exemplar_bank) > 1000:\n",
    "                # Remove lowest quality exemplars\n",
    "                self.exemplar_bank = sorted(\n",
    "                    self.exemplar_bank, \n",
    "                    key=lambda x: x[3].get('quality', 0.0), \n",
    "                    reverse=True\n",
    "                )[:1000]\n",
    "                \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding exemplar: {e}\")\n",
    "            return False\n",
    "\n",
    "    def apply_keyword_weighting(self, text, embedding):\n",
    "        \"\"\"Apply domain-specific keyword weighting to the embedding\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        weighted = embedding.copy()\n",
    "        \n",
    "        weight_multiplier = 1.0\n",
    "        for keyword, weight in self.keyword_weights.items():\n",
    "            if keyword in text_lower:\n",
    "                weight_multiplier += (weight - 1.0) * 0.1  # Gradual effect\n",
    "        \n",
    "        return weighted * weight_multiplier\n",
    "    \n",
    "    def extract_topics(self, text):\n",
    "        \"\"\"Extract likely math topics from the text\"\"\"\n",
    "        topics = []\n",
    "        topic_patterns = {\n",
    "            'algebra': r'equation|variable|solve for|unknown|linear|quadratic',\n",
    "            'geometry': r'triangle|circle|angle|area|volume|perimeter',\n",
    "            'probability': r'probability|chance|likelihood|random|odds',\n",
    "            'statistics': r'average|mean|median|mode|standard deviation|variance',\n",
    "            'calculus': r'derivative|integral|rate of change|maximum|minimum',\n",
    "            'arithmetic': r'add|subtract|multiply|divide|sum|difference|product|quotient',\n",
    "            'word_problem': r'train|distance|time|speed|mixture|percent|increase|decrease'\n",
    "        }\n",
    "        \n",
    "        for topic, pattern in topic_patterns.items():\n",
    "            if re.search(pattern, text, re.IGNORECASE):\n",
    "                topics.append(topic)\n",
    "                \n",
    "        return topics\n",
    "    \n",
    "    def retrieve_similar_exemplars(self, question, k=5, filter_criteria=None):\n",
    "        \"\"\"Retrieve similar exemplars with filtering options\"\"\"\n",
    "        try:\n",
    "            if not self.exemplar_bank:\n",
    "                return []\n",
    "                \n",
    "            query_embedding = self.compute_embedding(question)\n",
    "            query_topics = self.extract_topics(question)\n",
    "            \n",
    "            # Calculate similarities with metadata bonuses\n",
    "            candidates = []\n",
    "            for i, (exemplar_question, exemplar_embedding, cot_steps, metadata) in enumerate(self.exemplar_bank):\n",
    "                try:\n",
    "                    # Base similarity using cosine similarity\n",
    "                    sim = cosine_similarity([query_embedding], [exemplar_embedding])[0][0]\n",
    "                    \n",
    "                    # Apply topic matching bonus\n",
    "                    exemplar_topics = metadata.get('topics', [])\n",
    "                    matching_topics = set(query_topics).intersection(set(exemplar_topics))\n",
    "                    topic_bonus = len(matching_topics) * 0.05  # 5% bonus per matching topic\n",
    "                    \n",
    "                    # Apply quality bonus\n",
    "                    quality_bonus = (metadata.get('quality', 1.0) - 0.5) * 0.1  # Up to 5% bonus for quality\n",
    "                    \n",
    "                    # Apply appropriate length bonus\n",
    "                    steps_count = metadata.get('num_steps', 0)\n",
    "                    length_bonus = min(steps_count / 10, 0.05)  # Up to 5% bonus for longer examples\n",
    "                    \n",
    "                    # Final adjusted similarity\n",
    "                    adjusted_sim = sim + topic_bonus + quality_bonus + length_bonus\n",
    "                    \n",
    "                    # Apply filters if specified\n",
    "                    if filter_criteria:\n",
    "                        if 'min_quality' in filter_criteria and \\\n",
    "                           metadata.get('quality', 0) < filter_criteria['min_quality']:\n",
    "                            continue\n",
    "                        if 'min_steps' in filter_criteria and \\\n",
    "                           metadata.get('num_steps', 0) < filter_criteria['min_steps']:\n",
    "                            continue\n",
    "                    \n",
    "                    candidates.append((i, adjusted_sim, cot_steps))\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing exemplar {i}: {e}\")\n",
    "            \n",
    "            # Sort and get top-k\n",
    "            candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_k = candidates[:k]\n",
    "            \n",
    "            # Return the cot_steps for top matches\n",
    "            return [steps for _, _, steps in top_k]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving similar exemplars: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def initialize_from_dataset(self, dataset, max_exemplars=300):\n",
    "        \"\"\"Initialize the retrieval module with examples from a dataset with metadata\"\"\"\n",
    "        added_count = 0\n",
    "        for i, item in enumerate(tqdm(dataset, desc=\"Building exemplar bank\")):\n",
    "            if added_count >= max_exemplars:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                question = item.get(\"raw_question\", \"\")\n",
    "                cot = item.get(\"raw_cot\", [])\n",
    "                \n",
    "                # Create metadata\n",
    "                if isinstance(cot, list):\n",
    "                    num_steps = len(cot)\n",
    "                else:\n",
    "                    # If cot is a string, try to split into steps\n",
    "                    cot_list = re.split(r'Step \\d+:|^\\d+\\.', cot)\n",
    "                    cot_list = [s.strip() for s in cot_list if s.strip()]\n",
    "                    cot = cot_list\n",
    "                    num_steps = len(cot_list)\n",
    "                \n",
    "                metadata = {\n",
    "                    'quality': 1.0,  # Assume high quality for initial dataset\n",
    "                    'topics': self.extract_topics(question),\n",
    "                    'num_steps': num_steps,\n",
    "                    'date_added': datetime.now().isoformat(),\n",
    "                    'source': 'initial_dataset'\n",
    "                }\n",
    "                \n",
    "                success = self.add_exemplar(question, cot, metadata)\n",
    "                if success:\n",
    "                    added_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error adding exemplar {i} from dataset: {e}\")\n",
    "        \n",
    "        print(f\"Added {added_count} exemplars from dataset to retrieval module\")\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the embedding cache to free memory\"\"\"\n",
    "        self.embedding_cache = {}\n",
    "        \n",
    "    def save(self, path):\n",
    "        \"\"\"Save the retrieval module state\"\"\"\n",
    "        try:\n",
    "            state = {\n",
    "                'exemplar_bank': self.exemplar_bank,\n",
    "                'keyword_weights': self.keyword_weights\n",
    "            }\n",
    "            with open(path, 'wb') as f:\n",
    "                pickle.dump(state, f)\n",
    "            print(f\"Retrieval module saved to {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving retrieval module: {e}\")\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Load the retrieval module state\"\"\"\n",
    "        try:\n",
    "            with open(path, 'rb') as f:\n",
    "                state = pickle.load(f)\n",
    "            self.exemplar_bank = state['exemplar_bank']\n",
    "            self.keyword_weights = state.get('keyword_weights', self.keyword_weights)\n",
    "            print(f\"Retrieval module loaded from {path} with {len(self.exemplar_bank)} exemplars\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading retrieval module: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T17:40:35.806026Z",
     "iopub.status.busy": "2025-03-11T17:40:35.805708Z",
     "iopub.status.idle": "2025-03-11T17:40:35.821128Z",
     "shell.execute_reply": "2025-03-11T17:40:35.820298Z",
     "shell.execute_reply.started": "2025-03-11T17:40:35.805999Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TextRefinementTransformer:\n",
    "    def __init__(self, model_name=MODEL_NAME):\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                               \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "            self.model.to(self.device)\n",
    "            \n",
    "            # Pattern library for common errors in reasoning\n",
    "            self.error_patterns = {\n",
    "                r'(\\d+)\\s*\\+\\s*(\\d+)\\s*=\\s*(\\d+)': self._check_addition,\n",
    "                r'(\\d+)\\s*\\-\\s*(\\d+)\\s*=\\s*(\\d+)': self._check_subtraction,\n",
    "                r'(\\d+)\\s*\\*\\s*(\\d+)\\s*=\\s*(\\d+)': self._check_multiplication,\n",
    "                r'(\\d+)\\s*\\/\\s*(\\d+)\\s*=\\s*(\\d+)': self._check_division,\n",
    "            }\n",
    "            \n",
    "            # Template prompts for different refinement strategies\n",
    "            self.refinement_templates = {\n",
    "                'low_reward': \"Fix the following math solution by checking for calculation errors: {text}\",\n",
    "                'medium_reward': \"Improve this math solution with more detailed step-by-step reasoning: {text}\",\n",
    "                'high_reward': \"Further enhance this strong math solution with clearer explanations: {text}\",\n",
    "                'error_fix': \"The following math solution contains errors in {error_types}. Please fix these errors: {text}\",\n",
    "                'elaborate': \"This solution needs more steps. Please elaborate step {step_number} further: {text}\",\n",
    "                'conclude': \"This solution needs a clear final answer. Complete it based on the work shown: {text}\"\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing refinement module: {e}\")\n",
    "            self.tokenizer = None\n",
    "            self.model = None\n",
    "    \n",
    "    def _check_addition(self, match):\n",
    "        \"\"\"Check if addition is correct and return error if not\"\"\"\n",
    "        a, b, c = map(int, match.groups())\n",
    "        return a + b != c\n",
    "        \n",
    "    def _check_subtraction(self, match):\n",
    "        \"\"\"Check if subtraction is correct and return error if not\"\"\"\n",
    "        a, b, c = map(int, match.groups())\n",
    "        return a - b != c\n",
    "        \n",
    "    def _check_multiplication(self, match):\n",
    "        \"\"\"Check if multiplication is correct and return error if not\"\"\"\n",
    "        a, b, c = map(int, match.groups())\n",
    "        return a * b != c\n",
    "        \n",
    "    def _check_division(self, match):\n",
    "        \"\"\"Check if division is approximately correct and return error if not\"\"\"\n",
    "        a, b, c = map(int, match.groups())\n",
    "        if b == 0:\n",
    "            return True  # Division by zero is an error\n",
    "        return abs(a / b - c) > 0.01  # Allow for small floating point differences\n",
    "    \n",
    "    def detect_errors(self, text):\n",
    "        \"\"\"Detect specific calculation errors in the text\"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        for pattern, check_func in self.error_patterns.items():\n",
    "            for match in re.finditer(pattern, text):\n",
    "                if check_func(match):\n",
    "                    error_text = match.group(0)\n",
    "                    errors.append(error_text)\n",
    "        \n",
    "        return errors\n",
    "    \n",
    "    def select_refinement_strategy(self, text, reward, has_final_answer):\n",
    "        \"\"\"Select the best refinement strategy based on text analysis\"\"\"\n",
    "        errors = self.detect_errors(text)\n",
    "        \n",
    "        # If there are calculation errors, prioritize fixing them\n",
    "        if errors:\n",
    "            error_types = \"calculations (\" + \", \".join(errors[:2]) + \")\" if len(errors) > 0 else \"reasoning\"\n",
    "            return 'error_fix', {'error_types': error_types}\n",
    "            \n",
    "        # If no final answer, prioritize adding conclusion\n",
    "        if not has_final_answer:\n",
    "            return 'conclude', {}\n",
    "            \n",
    "        # Choose template based on reward level\n",
    "        if reward < 0.4:\n",
    "            return 'low_reward', {}\n",
    "        elif reward < 0.7:\n",
    "            return 'medium_reward', {}\n",
    "        else:\n",
    "            return 'high_reward', {}\n",
    "    \n",
    "    def refine_text(self, input_text, reward=0.5, max_length=MAX_LENGTH):\n",
    "        \"\"\"Refine the input text using the selected strategy\"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            return input_text  # Fallback\n",
    "            \n",
    "        try:\n",
    "            # Extract final answer to check if it exists\n",
    "            final_answer = extract_final_answer(input_text)\n",
    "            has_final_answer = bool(final_answer)\n",
    "            \n",
    "            # Select appropriate refinement strategy\n",
    "            strategy, params = self.select_refinement_strategy(input_text, reward, has_final_answer)\n",
    "            \n",
    "            # Prepare prompt based on strategy\n",
    "            template = self.refinement_templates[strategy]\n",
    "            params['text'] = input_text\n",
    "            prompt = template.format(**params)\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = self.tokenizer(\n",
    "                prompt, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True,\n",
    "                max_length=max_length // 2\n",
    "            )\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Generate improved text\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    max_length=max_length,\n",
    "                    num_return_sequences=1,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    no_repeat_ngram_size=3,\n",
    "                    num_beams=4\n",
    "                )\n",
    "            \n",
    "            refined_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Fallback to original if refinement failed or got much shorter\n",
    "            if len(refined_text) < len(input_text) * 0.5:\n",
    "                print(f\"Refinement produced much shorter text. Using original.\")\n",
    "                return input_text\n",
    "                \n",
    "            return refined_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error refining text: {e}\")\n",
    "            return input_text  # Fallback to original\n",
    "    \n",
    "    def batch_refine(self, text_list, rewards, max_length=MAX_LENGTH):\n",
    "        \"\"\"Refine a batch of texts in parallel\"\"\"\n",
    "        refined_texts = []\n",
    "        \n",
    "        for text, reward in zip(text_list, rewards):\n",
    "            refined = self.refine_text(text, reward, max_length)\n",
    "            refined_texts.append(refined)\n",
    "            \n",
    "        return refined_texts\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save the refinement module\"\"\"\n",
    "        try:\n",
    "            if self.model:\n",
    "                self.model.save_pretrained(path)\n",
    "                self.tokenizer.save_pretrained(path)\n",
    "                print(f\"Refinement module saved to {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving refinement module: {e}\")\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Load the refinement module\"\"\"\n",
    "        try:\n",
    "            self.model = AutoModelForSeq2SeqLM.from_pretrained(path)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "            self.model.to(self.device)\n",
    "            print(f\"Refinement module loaded from {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading refinement module: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T17:40:36.056477Z",
     "iopub.status.busy": "2025-03-11T17:40:36.056143Z",
     "iopub.status.idle": "2025-03-11T17:40:36.070946Z",
     "shell.execute_reply": "2025-03-11T17:40:36.070030Z",
     "shell.execute_reply.started": "2025-03-11T17:40:36.056447Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RewardFunction:\n",
    "    def __init__(self, reflection_module):\n",
    "        self.reflection_module = reflection_module\n",
    "        \n",
    "    def outcome_reward(self, predicted_answer, gold_answer):\n",
    "        \"\"\"Calculate a reward based on how close the predicted answer is to the gold answer\"\"\"\n",
    "        if not predicted_answer or not gold_answer:\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            # Clean and normalize answers\n",
    "            pred = self._normalize_answer(predicted_answer)\n",
    "            gold = self._normalize_answer(gold_answer)\n",
    "            \n",
    "            # Exact match\n",
    "            if pred == gold:\n",
    "                return 1.0\n",
    "            \n",
    "            # Try to convert to numbers for numerical comparison\n",
    "            try:\n",
    "                pred_num = float(pred.replace(',', ''))\n",
    "                gold_num = float(gold.replace(',', ''))\n",
    "                \n",
    "                # Calculate relative error\n",
    "                if abs(gold_num) > 1e-6:  # Avoid division by zero\n",
    "                    rel_error = abs(pred_num - gold_num) / abs(gold_num)\n",
    "                    if rel_error < 0.01:  # Within 1%\n",
    "                        return 0.9\n",
    "                    elif rel_error < 0.05:  # Within 5%\n",
    "                        return 0.7\n",
    "                    elif rel_error < 0.1:  # Within 10%\n",
    "                        return 0.5\n",
    "                    elif rel_error < 0.2:  # Within 20%\n",
    "                        return 0.3\n",
    "                \n",
    "                # If numbers are small, use absolute error\n",
    "                abs_error = abs(pred_num - gold_num)\n",
    "                if abs_error < 0.01:\n",
    "                    return 0.8\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Partial string match as fallback\n",
    "            if pred in gold or gold in pred:\n",
    "                return 0.3\n",
    "                \n",
    "            return 0.0\n",
    "        except Exception as e:\n",
    "            print(f\"Error in outcome_reward: {e}\")\n",
    "            return 0.0\n",
    "        \n",
    "    def _normalize_answer(self, answer):\n",
    "        \"\"\"Normalize an answer string for comparison\"\"\"\n",
    "        if not answer:\n",
    "            return \"\"\n",
    "            \n",
    "        # Convert to string if needed\n",
    "        answer = str(answer)\n",
    "        \n",
    "        # Remove punctuation and lowercase\n",
    "        answer = answer.lower().strip()\n",
    "        answer = re.sub(r'[^\\w\\s\\.\\-]', '', answer)\n",
    "        \n",
    "        # Remove units and common words\n",
    "        units = ['dollars', 'dollar', '$', 'cents', 'apples', 'people', 'hours', 'km', 'meters', 'years']\n",
    "        for unit in units:\n",
    "            answer = answer.replace(unit, '').strip()\n",
    "        \n",
    "        return answer.strip()\n",
    "    \n",
    "    def process_reward(self, cot_steps, question):\n",
    "        \"\"\"Improved reward based on the quality of the reasoning process\"\"\"\n",
    "        if not cot_steps:\n",
    "            return 0.2  # Small baseline reward even with no steps\n",
    "        \n",
    "        try:\n",
    "            # Check for math expressions, operations, and equations\n",
    "            has_math_expressions = False\n",
    "            for step in cot_steps:\n",
    "                if re.search(r'[=+\\-*/]', step) and re.search(r'\\d', step):\n",
    "                    has_math_expressions = True\n",
    "                    break\n",
    "            \n",
    "            # Check for relevant keywords from the question in the steps\n",
    "            question_words = set(re.findall(r'\\b\\w{4,}\\b', question.lower()))\n",
    "            step_text = ' '.join(cot_steps).lower()\n",
    "            relevant_word_count = sum(1 for word in question_words if word in step_text)\n",
    "            relevance_score = min(relevant_word_count / max(len(question_words), 1), 1.0)\n",
    "            \n",
    "            # Evaluate each step and take the mean\n",
    "            step_scores = []\n",
    "            for step in cot_steps:\n",
    "                score = self.reflection_module.evaluate_step(step, question)\n",
    "                step_scores.append(score)\n",
    "            \n",
    "            # More generous length bonus\n",
    "            length_bonus = min(len(step_scores) / 3, 1.0)  # Bonus for 3+ steps, capped at 1.0\n",
    "            \n",
    "            # Bonus for using numbers in steps\n",
    "            has_numbers = any(bool(re.search(r'\\d+', step)) for step in cot_steps)\n",
    "            number_bonus = 0.2 if has_numbers else 0.0\n",
    "            \n",
    "            # Bonus for mathematical expressions\n",
    "            math_bonus = 0.3 if has_math_expressions else 0.0\n",
    "            \n",
    "            # Return the mean score with bonuses\n",
    "            base_score = sum(step_scores) / len(step_scores) if step_scores else 0.3\n",
    "            return min(base_score * (1.0 + 0.3 * length_bonus + number_bonus + math_bonus) + 0.2 * relevance_score, 1.0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in process_reward: {e}\")\n",
    "            return 0.3  # More generous fallback\n",
    "    \n",
    "    def combined_reward(self, cot_steps, predicted_answer, true_answer, question, training_progress=0.0):\n",
    "        \"\"\"\n",
    "        Combine process and outcome rewards with adaptive weighting based on training progress\n",
    "        \n",
    "        Args:\n",
    "            cot_steps: List of reasoning steps\n",
    "            predicted_answer: Model's predicted answer\n",
    "            true_answer: Gold/reference answer\n",
    "            question: Original question text\n",
    "            training_progress: Float between 0-1 indicating training progress\n",
    "        \n",
    "        Returns:\n",
    "            Float: Combined reward score between 0-1\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Calculate component rewards\n",
    "            outcome = self.outcome_reward(predicted_answer, true_answer)\n",
    "            process = self.process_reward(cot_steps, question)\n",
    "            \n",
    "            # Dynamic alpha weighting that shifts focus towards outcome as training progresses\n",
    "            # Early training: focus more on process (reasoning quality)\n",
    "            # Late training: focus more on outcome (correct answers)\n",
    "            base_alpha = 0.7  # Same as original\n",
    "            min_alpha = 0.5   # Don't go below this to maintain outcome importance\n",
    "            \n",
    "            # Calculate adaptive alpha, increasing with training progress\n",
    "            # Start with lower alpha (more process focus) and gradually increase\n",
    "            alpha = min_alpha + (base_alpha - min_alpha) * training_progress\n",
    "            \n",
    "            # Apply confidence-based adjustment\n",
    "            # If outcome is very good but process is poor, we might have a lucky guess\n",
    "            # If process is very good but outcome is poor, reasoning might be correct but execution failed\n",
    "            confidence_adjustment = 0.0\n",
    "            \n",
    "            # If there's a big gap between process and outcome, adjust weighting\n",
    "            gap = abs(outcome - process)\n",
    "            if gap > 0.4:  # Significant disagreement between outcome and process\n",
    "                if outcome > process + 0.4:  # Much better outcome than process - lucky?\n",
    "                    confidence_adjustment = -0.1  # Reduce alpha slightly to trust process more\n",
    "                elif process > outcome + 0.4:  # Much better process than outcome - calculation error?\n",
    "                    confidence_adjustment = 0.1   # Increase alpha slightly to focus on outcomes\n",
    "            \n",
    "            # Apply adjustment but stay within bounds\n",
    "            adjusted_alpha = max(min_alpha, min(base_alpha, alpha + confidence_adjustment))\n",
    "            \n",
    "            # Combine rewards with adjusted alpha\n",
    "            combined = adjusted_alpha * outcome + (1 - adjusted_alpha) * process\n",
    "            \n",
    "            # For very short or empty reasoning chains with good outcomes, apply a penalty\n",
    "            if outcome > 0.8 and (not cot_steps or len(cot_steps) < 2):\n",
    "                combined *= 0.9  # Small penalty for correct answers with insufficient reasoning\n",
    "            \n",
    "            # Debug info with expanded metrics\n",
    "            if random.random() < 0.05:  # Only print for 5% of examples to avoid log flooding\n",
    "                print(f\"\\nQuestion: {question[:50]}...\")\n",
    "                print(f\"Predicted: {predicted_answer}\")\n",
    "                print(f\"True: {true_answer}\")\n",
    "                print(f\"Outcome reward: {outcome:.2f}, Process reward: {process:.2f}\")\n",
    "                print(f\"Training progress: {training_progress:.2f}, Alpha: {adjusted_alpha:.2f}\")\n",
    "                print(f\"Combined reward: {combined:.2f}\")\n",
    "                if cot_steps:\n",
    "                    print(f\"First reasoning step: {cot_steps[0][:50]}...\")\n",
    "                    print(f\"Step count: {len(cot_steps)}\")\n",
    "            \n",
    "            return combined\n",
    "        except Exception as e:\n",
    "            print(f\"Error in combined_reward: {e}\")\n",
    "            return 0.3  # More generous fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T17:40:36.866324Z",
     "iopub.status.busy": "2025-03-11T17:40:36.866018Z",
     "iopub.status.idle": "2025-03-11T17:40:36.916058Z",
     "shell.execute_reply": "2025-03-11T17:40:36.914938Z",
     "shell.execute_reply.started": "2025-03-11T17:40:36.866300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class SelfTrainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        cot_generator,\n",
    "        reflection_module,\n",
    "        retrieval_module,\n",
    "        refinement_module,\n",
    "        reward_function,\n",
    "        tokenizer=None\n",
    "    ):\n",
    "        self.cot_generator = cot_generator\n",
    "        self.reflection_module = reflection_module\n",
    "        self.retrieval_module = retrieval_module\n",
    "        self.refinement_module = refinement_module\n",
    "        self.reward_function = reward_function\n",
    "        try:\n",
    "            self.tokenizer = tokenizer or AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading tokenizer: {e}\")\n",
    "            self.tokenizer = None\n",
    "        \n",
    "    def generate_pseudo_labels(self, dataset, threshold=0.3, max_samples=100, training_progress=0.0):\n",
    "        \"\"\"\n",
    "        Generate high-quality pseudo-labeled examples with adaptive reward calculations\n",
    "        \n",
    "        Args:\n",
    "            dataset: Dataset to generate examples from\n",
    "            threshold: Minimum reward threshold for keeping examples\n",
    "            max_samples: Maximum number of samples to process\n",
    "            training_progress: Float between 0-1 indicating training progress\n",
    "            \n",
    "        Returns:\n",
    "            list: Pseudo-labeled examples that meet quality threshold\n",
    "        \"\"\"\n",
    "        pseudo_labeled = []\n",
    "        rewards = []\n",
    "        retry_count = 0\n",
    "        \n",
    "        # Sample more examples\n",
    "        sample_count = min(max_samples, len(dataset))\n",
    "        sample_indices = random.sample(range(len(dataset)), sample_count)\n",
    "        \n",
    "        for i in tqdm(sample_indices, desc=\"Generating pseudo-labels\"):\n",
    "            try:\n",
    "                item = dataset[i]\n",
    "                question = item[\"raw_question\"]\n",
    "                gold_answer = item[\"raw_answer\"]\n",
    "                \n",
    "                # Generate CoT with the current model using a stronger prompt\n",
    "                generated = self.cot_generator.generate(\n",
    "                    question,\n",
    "                    reflection_module=self.reflection_module,\n",
    "                    temperature=0.7  # Add some variation\n",
    "                )\n",
    "    \n",
    "                # Retrieve similar exemplars to guide generation\n",
    "                similar_exemplars = self.retrieval_module.retrieve_similar_exemplars(question, k=3)\n",
    "                exemplar_text = \"\"\n",
    "                if similar_exemplars:\n",
    "                    exemplar_text = \"Here are examples of good reasoning:\\n\"\n",
    "                    for i, exemplar in enumerate(similar_exemplars[:2]):  # Use top 2 exemplars\n",
    "                        steps_text = \"\\n\".join([f\"Step {j+1}: {step}\" for j, step in enumerate(exemplar)])\n",
    "                        exemplar_text += f\"Example {i+1}:\\n{steps_text}\\n\\n\"\n",
    "                \n",
    "                # Try to improve generation with exemplars if available\n",
    "                if exemplar_text:\n",
    "                    enhanced_prompt = f\"{exemplar_text}\\nNow solve this problem:\\n{question}\"\n",
    "                    enhanced_generation = self.cot_generator.generate(\n",
    "                        enhanced_prompt,\n",
    "                        reflection_module=self.reflection_module,\n",
    "                        temperature=0.65  # Slightly lower temperature for guided generation\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate rewards for both generations and use the better one\n",
    "                    base_reward = self.reward_function.combined_reward(\n",
    "                        generated[\"cot_steps\"], \n",
    "                        generated[\"final_answer\"], \n",
    "                        gold_answer, \n",
    "                        question,\n",
    "                        training_progress=training_progress\n",
    "                    )\n",
    "                    \n",
    "                    enhanced_reward = self.reward_function.combined_reward(\n",
    "                        enhanced_generation[\"cot_steps\"], \n",
    "                        enhanced_generation[\"final_answer\"], \n",
    "                        gold_answer, \n",
    "                        question,\n",
    "                        training_progress=training_progress\n",
    "                    )\n",
    "                    \n",
    "                    if enhanced_reward > base_reward:\n",
    "                        generated = enhanced_generation\n",
    "                        reward = enhanced_reward\n",
    "                    else:\n",
    "                        reward = base_reward\n",
    "                else:\n",
    "                    reward = self.reward_function.combined_reward(\n",
    "                        generated[\"cot_steps\"], \n",
    "                        generated[\"final_answer\"], \n",
    "                        gold_answer, \n",
    "                        question,\n",
    "                        training_progress=training_progress\n",
    "                    )\n",
    "                \n",
    "                # Apply refinement with different strategies based on reward quality\n",
    "                if reward < 0.8:  # Apply to all but the very best examples\n",
    "                    refinement_strategy = \"major\" if reward < 0.5 else \"minor\"\n",
    "                    refined_text = self.refinement_module.refine_text(\n",
    "                        generated[\"full_output\"], \n",
    "                        strategy=refinement_strategy\n",
    "                    )\n",
    "                    refined_steps = extract_cot_steps(refined_text)\n",
    "                    refined_answer = extract_final_answer(refined_text)\n",
    "                    \n",
    "                    refined_reward = self.reward_function.combined_reward(\n",
    "                        refined_steps, \n",
    "                        refined_answer, \n",
    "                        gold_answer, \n",
    "                        question,\n",
    "                        training_progress=training_progress\n",
    "                    )\n",
    "                    \n",
    "                    # Use the refined version if it's better\n",
    "                    if refined_reward > reward:\n",
    "                        generated[\"cot_steps\"] = refined_steps\n",
    "                        generated[\"final_answer\"] = refined_answer\n",
    "                        generated[\"full_output\"] = refined_text\n",
    "                        reward = refined_reward\n",
    "                \n",
    "                # Keep examples that exceed the quality threshold\n",
    "                if reward >= threshold:\n",
    "                    pseudo_labeled.append({\n",
    "                        \"question\": question,\n",
    "                        \"cot_steps\": generated[\"cot_steps\"],\n",
    "                        \"final_answer\": generated[\"final_answer\"],\n",
    "                        \"full_output\": generated[\"full_output\"],\n",
    "                        \"gold_answer\": gold_answer,\n",
    "                        \"reward\": reward,\n",
    "                        \"source_index\": i  # Track which example this came from\n",
    "                    })\n",
    "                    rewards.append(reward)\n",
    "                else:\n",
    "                    # For examples that almost meet the threshold, try one more generation with different parameters\n",
    "                    if reward >= threshold - 0.1 and retry_count < max_samples * 0.2:  # Limit retries to 20%\n",
    "                        retry_count += 1\n",
    "                        \n",
    "                        # Try again with different temperature\n",
    "                        retry_generated = self.cot_generator.generate(\n",
    "                            question,\n",
    "                            reflection_module=self.reflection_module,\n",
    "                            temperature=0.9  # Higher temperature for more exploration\n",
    "                        )\n",
    "                        \n",
    "                        retry_reward = self.reward_function.combined_reward(\n",
    "                            retry_generated[\"cot_steps\"], \n",
    "                            retry_generated[\"final_answer\"], \n",
    "                            gold_answer, \n",
    "                            question,\n",
    "                            training_progress=training_progress\n",
    "                        )\n",
    "                        \n",
    "                        if retry_reward >= threshold:\n",
    "                            pseudo_labeled.append({\n",
    "                                \"question\": question,\n",
    "                                \"cot_steps\": retry_generated[\"cot_steps\"],\n",
    "                                \"final_answer\": retry_generated[\"final_answer\"],\n",
    "                                \"full_output\": retry_generated[\"full_output\"],\n",
    "                                \"gold_answer\": gold_answer,\n",
    "                                \"reward\": retry_reward,\n",
    "                                \"source_index\": i,\n",
    "                                \"is_retry\": True\n",
    "                            })\n",
    "                            rewards.append(retry_reward)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing item {i}: {e}\")\n",
    "        \n",
    "        # Log statistics\n",
    "        if rewards:\n",
    "            avg_reward = sum(rewards) / len(rewards)\n",
    "            median_reward = sorted(rewards)[len(rewards) // 2]\n",
    "            print(f\"Generated {len(pseudo_labeled)} pseudo-labeled examples:\")\n",
    "            print(f\"  Average reward: {avg_reward:.3f}\")\n",
    "            print(f\"  Median reward: {median_reward:.3f}\")\n",
    "            print(f\"  Min/Max reward: {min(rewards):.3f}/{max(rewards):.3f}\")\n",
    "            print(f\"  Retries attempted: {retry_count}\")\n",
    "        else:\n",
    "            print(\"Failed to generate any valid pseudo-labeled examples\")\n",
    "            \n",
    "        return pseudo_labeled\n",
    "    \n",
    "    def train_with_pseudo_labels(self, pseudo_labeled, learning_rate=1e-5, epochs=2, batch_size=4):\n",
    "        \"\"\"Train the model with pseudo-labeled examples\"\"\"\n",
    "        if not pseudo_labeled:\n",
    "            print(\"No pseudo-labeled examples available for training\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Create a simple dataset from pseudo-labeled examples\n",
    "            class PseudoLabeledDataset(Dataset):\n",
    "                def __init__(self, examples, tokenizer, max_length=MAX_LENGTH):\n",
    "                    self.examples = examples\n",
    "                    self.tokenizer = tokenizer\n",
    "                    self.max_length = max_length\n",
    "                \n",
    "                def __len__(self):\n",
    "                    return len(self.examples)\n",
    "                \n",
    "                def __getitem__(self, idx):\n",
    "                    item = self.examples[idx]\n",
    "                    \n",
    "                    input_text = f\"Solve this math problem step-by-step: {item['question']}\"\n",
    "                    target_text = item[\"full_output\"]\n",
    "                    \n",
    "                    inputs = self.tokenizer(\n",
    "                        input_text,\n",
    "                        padding=\"max_length\",\n",
    "                        truncation=True,\n",
    "                        max_length=self.max_length // 3,\n",
    "                        return_tensors=\"pt\"\n",
    "                    )\n",
    "                    \n",
    "                    targets = self.tokenizer(\n",
    "                        target_text,\n",
    "                        padding=\"max_length\",\n",
    "                        truncation=True,\n",
    "                        max_length=self.max_length * 2 // 3,\n",
    "                        return_tensors=\"pt\"\n",
    "                    )\n",
    "                    \n",
    "                    return {\n",
    "                        \"input_ids\": inputs.input_ids.squeeze(),\n",
    "                        \"attention_mask\": inputs.attention_mask.squeeze(),\n",
    "                        \"labels\": targets.input_ids.squeeze(),\n",
    "                    }\n",
    "            \n",
    "            # Initialize dataset and dataloader\n",
    "            pseudo_dataset = PseudoLabeledDataset(pseudo_labeled, self.tokenizer)\n",
    "            pseudo_dataloader = DataLoader(\n",
    "                pseudo_dataset, \n",
    "                batch_size=batch_size,\n",
    "                shuffle=True\n",
    "            )\n",
    "            \n",
    "            # Set up optimizer\n",
    "            optimizer = AdamW(self.cot_generator.model.parameters(), lr=learning_rate)\n",
    "            \n",
    "            # Training loop\n",
    "            total_steps = len(pseudo_dataloader) * epochs\n",
    "            print(f\"Starting training on {len(pseudo_labeled)} examples for {epochs} epochs ({total_steps} steps)\")\n",
    "            \n",
    "            device = self.cot_generator.model.device\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                epoch_loss = 0.0\n",
    "                \n",
    "                for step, batch in enumerate(tqdm(pseudo_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n",
    "                    # Move batch to device\n",
    "                    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    self.cot_generator.model.train()\n",
    "                    outputs = self.cot_generator.model(\n",
    "                        input_ids=batch[\"input_ids\"],\n",
    "                        attention_mask=batch[\"attention_mask\"],\n",
    "                        labels=batch[\"labels\"]\n",
    "                    )\n",
    "                    \n",
    "                    loss = outputs.loss\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    epoch_loss += loss.item()\n",
    "                    \n",
    "                    # Log progress\n",
    "                    if step % 10 == 0:\n",
    "                        print(f\"Step {step}/{len(pseudo_dataloader)}, Loss: {loss.item():.4f}\")\n",
    "                \n",
    "                # End of epoch stats\n",
    "                avg_epoch_loss = epoch_loss / len(pseudo_dataloader)\n",
    "                print(f\"Epoch {epoch+1}/{epochs} completed. Average loss: {avg_epoch_loss:.4f}\")\n",
    "                \n",
    "                # Save checkpoint\n",
    "                self.cot_generator.save(f\"./models/recot_checkpoint_epoch_{epoch+1}\")\n",
    "                \n",
    "            print(\"Training completed successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in train_with_pseudo_labels: {e}\")\n",
    "    \n",
    "    def run_ppo_update(self, batch, ppo_epochs=4, mini_batch_size=2, clip_param=0.2, \n",
    "                  value_loss_coef=0.5, entropy_coef=0.01, max_grad_norm=0.5):\n",
    "        \"\"\"\n",
    "        Perform PPO update on the model using the given batch with improved implementation\n",
    "        \n",
    "        Args:\n",
    "            batch: List of examples with questions, gold answers, etc.\n",
    "            ppo_epochs: Number of epochs to run over the entire batch\n",
    "            mini_batch_size: Size of mini-batches for updates\n",
    "            clip_param: PPO clipping parameter\n",
    "            value_loss_coef: Value function loss coefficient\n",
    "            entropy_coef: Entropy bonus coefficient\n",
    "            max_grad_norm: Maximum gradient norm for clipping\n",
    "        \"\"\"\n",
    "        try:\n",
    "            device = self.cot_generator.model.device\n",
    "            \n",
    "            # Prepare data structures\n",
    "            trajectories = []\n",
    "            \n",
    "            # Collect trajectories\n",
    "            print(f\"Collecting trajectories from {len(batch)} examples...\")\n",
    "            for item in tqdm(batch, desc=\"Collecting trajectories\"):\n",
    "                question = item[\"question\"]\n",
    "                gold_answer = item[\"gold_answer\"]\n",
    "                \n",
    "                # Generate CoT with current policy and record log probs\n",
    "                with torch.no_grad():\n",
    "                    input_text = f\"Solve this math problem step-by-step: {question}\"\n",
    "                    inputs = self.tokenizer(\n",
    "                        input_text, \n",
    "                        return_tensors=\"pt\",\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        max_length=MAX_LENGTH // 3\n",
    "                    ).to(device)\n",
    "                    \n",
    "                    # Generate with model (record log probs)\n",
    "                    self.cot_generator.model.eval()\n",
    "                    outputs = self.cot_generator.model.generate(\n",
    "                        input_ids=inputs[\"input_ids\"],\n",
    "                        attention_mask=inputs[\"attention_mask\"],\n",
    "                        max_length=MAX_LENGTH,\n",
    "                        output_scores=True,\n",
    "                        return_dict_in_generate=True,\n",
    "                        do_sample=True,\n",
    "                        temperature=0.7\n",
    "                    )\n",
    "                    \n",
    "                    # Extract sequences and scores\n",
    "                    sequences = outputs.sequences\n",
    "                    log_probs = torch.stack(outputs.scores, dim=1)\n",
    "                    generated_text = self.tokenizer.decode(sequences[0], skip_special_tokens=True)\n",
    "                    \n",
    "                    # Extract CoT steps and final answer\n",
    "                    cot_steps = extract_cot_steps(generated_text)\n",
    "                    final_answer = extract_final_answer(generated_text)\n",
    "                    \n",
    "                    # Calculate reward\n",
    "                    reward = self.reward_function.combined_reward(\n",
    "                        cot_steps, \n",
    "                        final_answer, \n",
    "                        gold_answer, \n",
    "                        question,\n",
    "                        training_progress=0.5  # Provide estimated training progress\n",
    "                    )\n",
    "                    \n",
    "                    # Store trajectory\n",
    "                    trajectories.append({\n",
    "                        \"input_ids\": inputs[\"input_ids\"],\n",
    "                        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "                        \"generated_ids\": sequences,\n",
    "                        \"log_probs\": log_probs,\n",
    "                        \"reward\": torch.tensor([reward], device=device)\n",
    "                    })\n",
    "            \n",
    "            if not trajectories:\n",
    "                print(\"No valid trajectories collected. Skipping PPO update.\")\n",
    "                return\n",
    "                \n",
    "            # Create optimizer with smaller learning rate for PPO\n",
    "            optimizer = AdamW(self.cot_generator.model.parameters(), lr=5e-6)\n",
    "            \n",
    "            # Initialize PPO statistics\n",
    "            stats = {\n",
    "                \"policy_loss\": [],\n",
    "                \"value_loss\": [],\n",
    "                \"entropy\": [],\n",
    "                \"total_loss\": [],\n",
    "                \"approx_kl\": [],\n",
    "                \"clip_fraction\": [],\n",
    "                \"explained_variance\": []\n",
    "            }\n",
    "            \n",
    "            # PPO update loop\n",
    "            print(f\"Running PPO for {ppo_epochs} epochs with mini-batch size {mini_batch_size}...\")\n",
    "            for epoch in range(ppo_epochs):\n",
    "                # Shuffle trajectories for this epoch\n",
    "                random.shuffle(trajectories)\n",
    "                epoch_stats = []\n",
    "                \n",
    "                # Process mini-batches\n",
    "                for i in range(0, len(trajectories), mini_batch_size):\n",
    "                    mini_batch = trajectories[i:i+mini_batch_size]\n",
    "                    if not mini_batch:\n",
    "                        continue\n",
    "                        \n",
    "                    # Initialize batch tensors\n",
    "                    batch_input_ids = torch.cat([item[\"input_ids\"] for item in mini_batch], dim=0)\n",
    "                    batch_attention_mask = torch.cat([item[\"attention_mask\"] for item in mini_batch], dim=0)\n",
    "                    batch_generated_ids = torch.cat([item[\"generated_ids\"] for item in mini_batch], dim=0)\n",
    "                    batch_rewards = torch.cat([item[\"reward\"] for item in mini_batch], dim=0)\n",
    "                    \n",
    "                    # Normalize rewards for stability\n",
    "                    batch_rewards = (batch_rewards - batch_rewards.mean()) / (batch_rewards.std() + 1e-8)\n",
    "                    \n",
    "                    # Create labels for the model by right-shifting generated ids\n",
    "                    batch_labels = batch_generated_ids.clone()\n",
    "                    batch_labels[:, :-1] = batch_generated_ids[:, 1:]\n",
    "                    batch_labels[:, -1] = self.tokenizer.pad_token_id\n",
    "                    \n",
    "                    # Compute value predictions and action log probs with current policy\n",
    "                    self.cot_generator.model.train()\n",
    "                    \n",
    "                    # Forward pass with current policy\n",
    "                    outputs = self.cot_generator.model(\n",
    "                        input_ids=batch_input_ids,\n",
    "                        attention_mask=batch_attention_mask,\n",
    "                        labels=batch_labels\n",
    "                    )\n",
    "                    \n",
    "                    # Get log probs of current policy\n",
    "                    logits = outputs.logits\n",
    "                    current_log_probs = F.log_softmax(logits, dim=-1)\n",
    "                    \n",
    "                    # Get old log probs\n",
    "                    old_log_probs = torch.cat([item[\"log_probs\"] for item in mini_batch], dim=0)\n",
    "                    \n",
    "                    # Calculate ratio and clipped surrogate objective (simplification)\n",
    "                    # Note: In a full implementation, we'd need to match token by token\n",
    "                    ratio = torch.exp(current_log_probs.mean(dim=1) - old_log_probs.mean(dim=1))\n",
    "                    surr1 = ratio * batch_rewards\n",
    "                    surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * batch_rewards\n",
    "                    \n",
    "                    # Policy loss\n",
    "                    policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                    \n",
    "                    # Value loss (using model loss as a proxy for value)\n",
    "                    value_pred = -outputs.loss.detach()\n",
    "                    value_targets = batch_rewards\n",
    "                    value_loss = F.mse_loss(value_pred, value_targets)\n",
    "                    \n",
    "                    # Entropy bonus (encourage exploration)\n",
    "                    entropy = -(F.softmax(logits, dim=-1) * F.log_softmax(logits, dim=-1)).sum(dim=-1).mean()\n",
    "                    \n",
    "                    # Total loss\n",
    "                    loss = policy_loss + value_loss_coef * value_loss - entropy_coef * entropy\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # Gradient clipping\n",
    "                    grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                        self.cot_generator.model.parameters(), \n",
    "                        max_grad_norm\n",
    "                    )\n",
    "                    \n",
    "                    # Optimizer step\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    # Calculate statistics\n",
    "                    clip_fraction = ((ratio - 1.0).abs() > clip_param).float().mean().item()\n",
    "                    approx_kl = (ratio.log() * (ratio - 1)).mean().item()\n",
    "                    \n",
    "                    # Store batch statistics\n",
    "                    batch_stats = {\n",
    "                        \"policy_loss\": policy_loss.item(),\n",
    "                        \"value_loss\": value_loss.item(),\n",
    "                        \"entropy\": entropy.item(),\n",
    "                        \"total_loss\": loss.item(),\n",
    "                        \"approx_kl\": approx_kl,\n",
    "                        \"clip_fraction\": clip_fraction,\n",
    "                        \"grad_norm\": grad_norm.item()\n",
    "                    }\n",
    "                    epoch_stats.append(batch_stats)\n",
    "                \n",
    "                # Calculate epoch statistics\n",
    "                if epoch_stats:\n",
    "                    epoch_mean_stats = {k: sum(d[k] for d in epoch_stats) / len(epoch_stats) for k in epoch_stats[0]}\n",
    "                    for k, v in epoch_mean_stats.items():\n",
    "                        stats[k] = stats.get(k, []) + [v]\n",
    "                    \n",
    "                    # Print epoch statistics\n",
    "                    print(f\"PPO Epoch {epoch+1}/{ppo_epochs} stats:\")\n",
    "                    print(f\"  Policy loss: {epoch_mean_stats['policy_loss']:.4f}\")\n",
    "                    print(f\"  Value loss: {epoch_mean_stats['value_loss']:.4f}\")\n",
    "                    print(f\"  Entropy: {epoch_mean_stats['entropy']:.4f}\")\n",
    "                    print(f\"  Approx KL: {epoch_mean_stats['approx_kl']:.4f}\")\n",
    "                    print(f\"  Clip fraction: {epoch_mean_stats['clip_fraction']:.4f}\")\n",
    "                    print(f\"  Gradient norm: {epoch_mean_stats['grad_norm']:.4f}\")\n",
    "            \n",
    "            # Final stats summary\n",
    "            if all(len(v) > 0 for v in stats.values()):\n",
    "                print(\"\\nPPO update completed. Final statistics:\")\n",
    "                for k, v in stats.items():\n",
    "                    if v:\n",
    "                        print(f\"  {k}: {v[-1]:.4f} (started: {v[0]:.4f})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in PPO update: {e}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    def evaluate(self, test_dataset, num_samples=50):\n",
    "        \"\"\"Evaluate the current model on a test dataset\"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        rewards = []\n",
    "        \n",
    "        # Sample a subset for efficient evaluation\n",
    "        sample_indices = random.sample(range(len(test_dataset)), min(num_samples, len(test_dataset)))\n",
    "        \n",
    "        for idx in tqdm(sample_indices, desc=\"Evaluating\"):\n",
    "            try:\n",
    "                item = test_dataset[idx]\n",
    "                question = item[\"raw_question\"]\n",
    "                gold_answer = item[\"raw_answer\"]\n",
    "                \n",
    "                # Generate answer\n",
    "                generation = self.cot_generator.generate(question)\n",
    "                predicted_answer = generation[\"final_answer\"]\n",
    "                \n",
    "                # Check if correct\n",
    "                reward = self.reward_function.outcome_reward(predicted_answer, gold_answer)\n",
    "                rewards.append(reward)\n",
    "                \n",
    "                # Binary correctness (1.0 means exactly correct)\n",
    "                if reward >= 0.9:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating example {idx}: {e}\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        avg_reward = sum(rewards) / len(rewards) if rewards else 0\n",
    "        \n",
    "        print(f\"Evaluation results:\")\n",
    "        print(f\"  Accuracy: {accuracy:.2f} ({correct}/{total})\")\n",
    "        print(f\"  Average reward: {avg_reward:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"avg_reward\": avg_reward,\n",
    "            \"correct\": correct,\n",
    "            \"total\": total\n",
    "        }\n",
    "    \n",
    "    def run_training_loop(self, train_dataset, test_dataset, num_iterations=5, pseudo_samples=100, batch_size=4):\n",
    "        \"\"\"\n",
    "        Run the full training loop with pseudo-labeling and improved RL updates\n",
    "        \n",
    "        Args:\n",
    "            train_dataset: Dataset to generate pseudo-labels from\n",
    "            test_dataset: Dataset for evaluation\n",
    "            num_iterations: Number of training iterations\n",
    "            pseudo_samples: Maximum number of samples to generate pseudo-labels for\n",
    "            batch_size: Batch size for supervised training\n",
    "            \n",
    "        Returns:\n",
    "            dict: Final evaluation results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            results_history = []\n",
    "            \n",
    "            # Initial evaluation\n",
    "            print(\"Initial model evaluation:\")\n",
    "            results = self.evaluate(test_dataset)\n",
    "            results_history.append(results)\n",
    "            initial_accuracy = results[\"accuracy\"]\n",
    "            \n",
    "            for iteration in range(num_iterations):\n",
    "                print(f\"\\n===== Iteration {iteration+1}/{num_iterations} =====\")\n",
    "                \n",
    "                # Calculate training progress for adaptive reward weighting\n",
    "                training_progress = iteration / (num_iterations - 1) if num_iterations > 1 else 0.5\n",
    "                \n",
    "                # 1. Generate pseudo-labeled examples with adaptive threshold\n",
    "                # Lower threshold initially, increase as training progresses\n",
    "                threshold = 0.3 + (0.2 * training_progress)  # Starts at 0.3, increases to 0.5\n",
    "                \n",
    "                print(f\"Generating {pseudo_samples} pseudo-labeled examples (threshold={threshold:.2f})...\")\n",
    "                pseudo_labeled = self.generate_pseudo_labels(\n",
    "                    train_dataset, \n",
    "                    threshold=threshold,\n",
    "                    max_samples=pseudo_samples\n",
    "                )\n",
    "                \n",
    "                if not pseudo_labeled:\n",
    "                    print(\"No pseudo-labeled examples generated. Adjusting threshold and retrying...\")\n",
    "                    # Retry with lower threshold if no examples meet criteria\n",
    "                    retry_threshold = max(0.2, threshold - 0.1)\n",
    "                    pseudo_labeled = self.generate_pseudo_labels(\n",
    "                        train_dataset, \n",
    "                        threshold=retry_threshold,\n",
    "                        max_samples=pseudo_samples * 2  # Sample more to increase chances\n",
    "                    )\n",
    "                    \n",
    "                    if not pseudo_labeled:\n",
    "                        print(\"Still no pseudo-labeled examples. Skipping iteration.\")\n",
    "                        continue\n",
    "                \n",
    "                # 2. Analyze pseudo-labeled examples\n",
    "                rewards = [ex[\"reward\"] for ex in pseudo_labeled]\n",
    "                avg_reward = sum(rewards) / len(rewards) if rewards else 0\n",
    "                reward_std = (sum((r - avg_reward) ** 2 for r in rewards) / len(rewards)) ** 0.5 if rewards else 0\n",
    "                \n",
    "                print(f\"Generated {len(pseudo_labeled)} examples:\")\n",
    "                print(f\"  Average reward: {avg_reward:.3f} (std: {reward_std:.3f})\")\n",
    "                print(f\"  Reward range: {min(rewards):.3f} - {max(rewards):.3f}\")\n",
    "                \n",
    "                # 3. Train with pseudo-labeled examples - adaptive learning rate\n",
    "                # Decrease learning rate as training progresses for finer adjustments\n",
    "                base_lr = 2e-5\n",
    "                decay_factor = 1.0 - (0.5 * training_progress)  # Starts at 1.0, decreases to 0.5\n",
    "                adaptive_lr = base_lr * decay_factor\n",
    "                \n",
    "                print(f\"Training with {len(pseudo_labeled)} examples (lr={adaptive_lr:.2e})...\")\n",
    "                self.train_with_pseudo_labels(\n",
    "                    pseudo_labeled,\n",
    "                    learning_rate=adaptive_lr,\n",
    "                    epochs=1 + (1 if iteration > num_iterations // 2 else 0),  # Extra epoch later in training\n",
    "                    batch_size=batch_size\n",
    "                )\n",
    "                \n",
    "                # 4. Perform PPO update with improved implementation\n",
    "                # Detect if we have enough high-quality examples\n",
    "                high_quality = [ex for ex in pseudo_labeled if ex[\"reward\"] >= threshold + 0.1]\n",
    "                if high_quality:\n",
    "                    print(f\"Performing PPO update with {len(high_quality)} high-quality examples...\")\n",
    "                    # Adjust mini-batch size based on available examples\n",
    "                    optimal_mini_batch = max(2, len(high_quality) // 4)\n",
    "                    mini_batch_size = min(optimal_mini_batch, 8)  # Cap at 8\n",
    "                    \n",
    "                    # Dynamically set epochs based on batch size\n",
    "                    ppo_epochs = 6 - (mini_batch_size // 2)  # More epochs for smaller batches\n",
    "                    ppo_epochs = max(3, min(ppo_epochs, 5))  # Between 3-5\n",
    "                    \n",
    "                    self.run_ppo_update(\n",
    "                        high_quality,\n",
    "                        ppo_epochs=ppo_epochs,\n",
    "                        mini_batch_size=mini_batch_size,\n",
    "                        # Increase exploration early, focus more on exploitation later\n",
    "                        entropy_coef=0.02 * (1.0 - training_progress)\n",
    "                    )\n",
    "                else:\n",
    "                    print(\"No high-quality examples found for PPO. Skipping reinforcement learning step.\")\n",
    "                \n",
    "                # 5. Update retrieval module with new exemplars\n",
    "                print(\"Updating retrieval module...\")\n",
    "                exemplar_threshold = 0.6 + (0.1 * training_progress)  # Increase quality threshold over time\n",
    "                quality_exemplars = [ex for ex in pseudo_labeled if ex[\"reward\"] >= exemplar_threshold]\n",
    "                \n",
    "                if quality_exemplars:\n",
    "                    print(f\"Adding {len(quality_exemplars)} new exemplars to retrieval module...\")\n",
    "                    for example in quality_exemplars:\n",
    "                        self.retrieval_module.add_exemplar(\n",
    "                            example[\"question\"],\n",
    "                            example[\"cot_steps\"]\n",
    "                        )\n",
    "                \n",
    "                # 6. Save checkpoint with iteration info\n",
    "                checkpoint_path = f\"./models/recot_checkpoint_iteration_{iteration+1}\"\n",
    "                print(f\"Saving checkpoint to {checkpoint_path}...\")\n",
    "                try:\n",
    "                    # Save with metadata\n",
    "                    metadata = {\n",
    "                        \"iteration\": iteration + 1,\n",
    "                        \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "                        \"train_examples\": len(pseudo_labeled),\n",
    "                        \"avg_reward\": avg_reward,\n",
    "                        \"exemplars_added\": len(quality_exemplars) if quality_exemplars else 0\n",
    "                    }\n",
    "                    self.cot_generator.save(checkpoint_path, metadata=metadata)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving checkpoint: {e}\")\n",
    "                    # Fallback to simple save\n",
    "                    self.cot_generator.save(checkpoint_path)\n",
    "                \n",
    "                # 7. Evaluate progress\n",
    "                print(\"Evaluating current model:\")\n",
    "                results = self.evaluate(test_dataset)\n",
    "                results_history.append(results)\n",
    "                \n",
    "                # Print improvement\n",
    "                if len(results_history) > 1:\n",
    "                    prev = results_history[-2][\"accuracy\"]\n",
    "                    curr = results_history[-1][\"accuracy\"]\n",
    "                    diff = curr - prev\n",
    "                    rel_improvement = f\"{(diff / prev * 100):.1f}%\" if prev > 0 else \"N/A\"\n",
    "                    print(f\"Accuracy change: {diff:+.3f} ({prev:.3f}  {curr:.3f}, {rel_improvement} relative)\")\n",
    "                    \n",
    "                    # Early stopping check - if performance degraded significantly\n",
    "                    if diff < -0.05 and iteration > 0:\n",
    "                        print(\"Warning: Performance degraded significantly. Consider restoring previous checkpoint.\")\n",
    "            \n",
    "            # Final comprehensive evaluation\n",
    "            print(\"\\n===== Final Evaluation =====\")\n",
    "            final_results = self.evaluate(test_dataset, num_samples=min(200, len(test_dataset)))\n",
    "            \n",
    "            # Print training summary\n",
    "            print(\"\\n===== Training Summary =====\")\n",
    "            print(f\"Initial accuracy: {initial_accuracy:.3f}\")\n",
    "            print(f\"Final accuracy: {final_results['accuracy']:.3f}\")\n",
    "            absolute_improvement = final_results['accuracy'] - initial_accuracy\n",
    "            relative_improvement = (absolute_improvement / initial_accuracy * 100) if initial_accuracy > 0 else float('inf')\n",
    "            print(f\"Absolute improvement: {absolute_improvement:+.3f}\")\n",
    "            print(f\"Relative improvement: {relative_improvement:+.1f}%\")\n",
    "            \n",
    "            # Plot training curve if matplotlib is available\n",
    "            try:\n",
    "                import matplotlib.pyplot as plt\n",
    "                \n",
    "                iterations = list(range(num_iterations + 1))\n",
    "                accuracies = [r[\"accuracy\"] for r in results_history] + [final_results[\"accuracy\"]]\n",
    "                rewards = [r[\"avg_reward\"] for r in results_history] + [final_results[\"avg_reward\"]]\n",
    "                \n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.plot(iterations, accuracies, 'b-o', label='Accuracy')\n",
    "                plt.plot(iterations, rewards, 'r-o', label='Avg Reward')\n",
    "                plt.xlabel('Iteration')\n",
    "                plt.ylabel('Score')\n",
    "                plt.title('Training Progress')\n",
    "                plt.legend()\n",
    "                plt.grid(True)\n",
    "                \n",
    "                # Save plot\n",
    "                plt.savefig('./training_progress.png')\n",
    "                print(\"Training progress plot saved to ./training_progress.png\")\n",
    "            except ImportError:\n",
    "                print(\"Matplotlib not available. Skipping training curve plot.\")\n",
    "            \n",
    "            return final_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in training loop: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "        \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count the number of trainable parameters in the model\"\"\"\n",
    "        model = self.cot_generator.model\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T17:40:36.917563Z",
     "iopub.status.busy": "2025-03-11T17:40:36.917284Z",
     "iopub.status.idle": "2025-03-11T17:40:44.911692Z",
     "shell.execute_reply": "2025-03-11T17:40:44.910606Z",
     "shell.execute_reply.started": "2025-03-11T17:40:36.917536Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "PyTorch version: 2.6.0\n",
      "CUDA available: False\n",
      "\n",
      "===== Initializing Components =====\n",
      "Initializing tokenizer...\n",
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing data: 100%|| 400/400 [00:00<00:00, 12794.34it/s]\n",
      "Preprocessing data: 100%|| 100/100 [00:00<00:00, 11158.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400 training examples and 100 test examples\n",
      "Initializing CoT Generator...\n",
      "Loading model t5-base...\n",
      "Model not found locally. Downloading t5-base...\n",
      "Tokenizer downloaded and saved to ./models/t5_base_cache\n",
      "Model downloaded and moved to mps\n",
      "Initializing Reflection Module...\n",
      "Initializing Retrieval Module...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building exemplar bank:   1%|         | 5/400 [00:00<00:17, 22.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building exemplar bank:   3%|         | 12/400 [00:00<00:14, 27.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building exemplar bank:   5%|         | 20/400 [00:00<00:12, 30.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building exemplar bank:   6%|         | 24/400 [00:00<00:12, 30.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building exemplar bank:   8%|         | 32/400 [00:01<00:12, 30.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building exemplar bank:   9%|         | 36/400 [00:01<00:13, 27.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building exemplar bank:  11%|         | 43/400 [00:01<00:12, 28.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building exemplar bank:  13%|        | 51/400 [00:01<00:11, 31.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building exemplar bank:  15%|        | 59/400 [00:02<00:10, 31.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building exemplar bank:  16%|        | 63/400 [00:02<00:18, 18.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building exemplar bank:  16%|        | 66/400 [00:02<00:19, 16.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building exemplar bank:  18%|        | 71/400 [00:03<00:22, 14.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building exemplar bank:  18%|        | 73/400 [00:03<00:24, 13.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building exemplar bank:  19%|        | 75/400 [00:03<00:23, 14.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building exemplar bank:  20%|        | 79/400 [00:04<00:37,  8.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building exemplar bank:  20%|        | 81/400 [00:04<00:34,  9.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building exemplar bank:  21%|       | 85/400 [00:04<00:31, 10.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building exemplar bank:  22%|       | 88/400 [00:04<00:27, 11.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building exemplar bank:  24%|       | 96/400 [00:05<00:15, 19.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building exemplar bank:  25%|       | 99/400 [00:05<00:16, 18.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building exemplar bank:  25%|       | 100/400 [00:05<00:16, 18.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error computing embedding: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "Added 100 exemplars from dataset to retrieval module\n",
      "Initializing Refinement Module...\n",
      "Error initializing refinement module: 'TextRefinementTransformer' object has no attribute 'device'\n",
      "Initializing Reward Function...\n",
      "Initializing Self-Trainer...\n",
      "Model has 222,903,552 trainable parameters\n",
      "\n",
      "===== Running Quick Test =====\n",
      "Test question: John has 5 apples. Mary gives him 3 more apples. How many apples does John have now?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m test_question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJohn has 5 apples. Mary gives him 3 more apples. How many apples does John have now?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest question: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_question\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 97\u001b[0m generation \u001b[38;5;241m=\u001b[39m \u001b[43mcot_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_question\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreflection_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreflection_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated CoT:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(generation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcot_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n",
      "Cell \u001b[0;32mIn[31], line 222\u001b[0m, in \u001b[0;36mCoTGenerator.generate\u001b[0;34m(self, question, max_length, cot_prompt, reflection_module, max_steps)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# Generate steps iteratively\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# Generate the next reasoning step\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     step_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcot_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m     current_step \u001b[38;5;241m=\u001b[39m step_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# Evaluate the step quality\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[31], line 109\u001b[0m, in \u001b[0;36mCoTGenerator.generate_step\u001b[0;34m(self, question, previous_steps, max_length)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Generate with clear stopping criteria - REVISED\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 109\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Decode and clean up the output - REVISED\u001b[39;00m\n\u001b[1;32m    123\u001b[0m decoded_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2254\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2246\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2247\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2248\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2249\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2250\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2251\u001b[0m     )\n\u001b[1;32m   2253\u001b[0m     \u001b[38;5;66;03m# 13. run beam sample\u001b[39;00m\n\u001b[0;32m-> 2254\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2255\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2264\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2266\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2267\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2268\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2275\u001b[0m     )\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:3463\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3460\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m stack_model_outputs(outputs_per_sub_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_text_config())\n\u001b[1;32m   3462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Unchanged original behavior\u001b[39;00m\n\u001b[0;32m-> 3463\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3465\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3466\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3467\u001b[0m     outputs,\n\u001b[1;32m   3468\u001b[0m     model_kwargs,\n\u001b[1;32m   3469\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3470\u001b[0m )\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1893\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1890\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1892\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1893\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1894\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1896\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1900\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1901\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1902\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1903\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1904\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1906\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1907\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1909\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1911\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1124\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1108\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1109\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         cache_position,\n\u001b[1;32m   1122\u001b[0m     )\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1124\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:675\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    661\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    673\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    674\u001b[0m ):\n\u001b[0;32m--> 675\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    685\u001b[0m     hidden_states, past_key_value \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    686\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:593\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    583\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    590\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    591\u001b[0m ):\n\u001b[1;32m    592\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 593\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    604\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (hidden_states,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:520\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    517\u001b[0m             past_key_value\u001b[38;5;241m.\u001b[39mis_updated[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position_bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    523\u001b[0m     key_length \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random.seed(11)\n",
    "np.random.seed(11)\n",
    "torch.manual_seed(11)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(11)\n",
    "\n",
    "# Print system information\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                        \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "\n",
    "print(\"\\n===== Initializing Components =====\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "print(\"Initializing tokenizer...\")\n",
    "try:\n",
    "    tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing tokenizer: {e}\")\n",
    "    tokenizer = None\n",
    "\n",
    "# Initialize data\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    train_dataset = GSM8KDataset(\n",
    "        split=\"train\", \n",
    "        tokenizer=tokenizer, \n",
    "        max_length=MAX_LENGTH, \n",
    "        max_samples=MAX_SAMPLES\n",
    "    )\n",
    "    test_dataset = GSM8KDataset(\n",
    "        split=\"test\", \n",
    "        tokenizer=tokenizer, \n",
    "        max_length=MAX_LENGTH, \n",
    "        max_samples=MAX_SAMPLES // 4\n",
    "    )\n",
    "    print(f\"Loaded {len(train_dataset)} training examples and {len(test_dataset)} test examples\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading datasets: {e}\")\n",
    "    raise e\n",
    "\n",
    "# Initialize CoT Generator (main model)\n",
    "print(\"Initializing CoT Generator...\")\n",
    "cot_generator = CoTGenerator(MODEL_NAME)\n",
    "\n",
    "# Initialize Reflection Module\n",
    "print(\"Initializing Reflection Module...\")\n",
    "reflection_module = ReflectionModule()\n",
    "reflection_module = to_device(reflection_module)\n",
    "\n",
    "# Initialize Retrieval Module\n",
    "print(\"Initializing Retrieval Module...\")\n",
    "retrieval_module = RetrievalModule()\n",
    "# Populate retrieval module with examples from the dataset\n",
    "retrieval_module.initialize_from_dataset(train_dataset, max_exemplars=100)\n",
    "\n",
    "# Initialize Refinement Module (transformer for text refinement)\n",
    "print(\"Initializing Refinement Module...\")\n",
    "refinement_module = TextRefinementTransformer()\n",
    "\n",
    "# Initialize Reward Function\n",
    "print(\"Initializing Reward Function...\")\n",
    "reward_function = RewardFunction(reflection_module)\n",
    "\n",
    "# Initialize Self-Trainer\n",
    "print(\"Initializing Self-Trainer...\")\n",
    "self_trainer = SelfTrainer(\n",
    "    cot_generator=cot_generator,\n",
    "    reflection_module=reflection_module,\n",
    "    retrieval_module=retrieval_module,\n",
    "    refinement_module=refinement_module,\n",
    "    reward_function=reward_function,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "num_params = self_trainer.count_parameters()\n",
    "print(f\"Model has {num_params:,} trainable parameters\")\n",
    "\n",
    "\n",
    "# Run a quick test to ensure everything is working\n",
    "print(\"\\n===== Running Quick Test =====\")\n",
    "try:\n",
    "    # Test CoT generation\n",
    "    test_question = \"John has 5 apples. Mary gives him 3 more apples. How many apples does John have now?\"\n",
    "    print(f\"Test question: {test_question}\")\n",
    "    \n",
    "    generation = cot_generator.generate(test_question, reflection_module=reflection_module)\n",
    "    print(f\"Generated CoT:\")\n",
    "    for i, step in enumerate(generation[\"cot_steps\"]):\n",
    "        print(f\"  Step {i+1}: {step}\")\n",
    "    print(f\"Final answer: {generation['final_answer']}\")\n",
    "    \n",
    "    # Test evaluation\n",
    "    print(\"Testing evaluation...\")\n",
    "    test_results = self_trainer.evaluate(test_dataset, num_samples=5)\n",
    "    print(f\"Test evaluation completed with accuracy: {test_results['accuracy']:.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in quick test: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Run the main training loop\n",
    "print(\"\\n===== Starting Main Training Loop =====\")\n",
    "try:\n",
    "    training_results = self_trainer.run_training_loop(\n",
    "        train_dataset=train_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        num_iterations=3,  # Reduced number of iterations for initial testing\n",
    "        pseudo_samples=50,  # Start with fewer samples\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    # Save final model\n",
    "    print(\"Saving final model...\")\n",
    "    cot_generator.save(\"./models/recot_final_model\")\n",
    "    \n",
    "    print(\"\\n===== Training Complete =====\")\n",
    "    if training_results:\n",
    "        print(f\"Final accuracy: {training_results['accuracy']:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in training loop: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Example of using the trained model\n",
    "print(\"\\n===== Example Usage =====\")\n",
    "try:\n",
    "    example_questions = [\n",
    "        \"Janet's ducks lay 16 eggs per day. She eats 3 for breakfast every morning and bakes muffins for her friends every day with 4 eggs per batch. She bakes 2 batches of muffins per day. How many eggs does Janet have left each day?\",\n",
    "        \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\",\n",
    "        \"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\"\n",
    "    ]\n",
    "    \n",
    "    for i, question in enumerate(example_questions):\n",
    "        print(f\"\\nExample {i+1}: {question}\")\n",
    "        result = cot_generator.generate(question, reflection_module=reflection_module)\n",
    "        print(\"Generated reasoning:\")\n",
    "        for j, step in enumerate(result[\"cot_steps\"]):\n",
    "            print(f\"  Step {j+1}: {step}\")\n",
    "        print(f\"Final answer: {result['final_answer']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in example usage: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
