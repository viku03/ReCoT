{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Viku/GitHub/ReCoT/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer\n",
    ")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from torch.optim import Adam\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple M1 MPS device\n"
     ]
    }
   ],
   "source": [
    "# Device configuration - M1 Mac specific\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple M1 MPS device\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 4  # Reduced batch size for M1\n",
    "LEARNING_RATE = 5e-5\n",
    "EPOCHS = 2  # Reduced epochs for faster testing on M1\n",
    "COT_PROMPT = \"Let's think step by step!\"\n",
    "MODEL_NAME = \"t5-small\"  # Using smaller model for M1 compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for extracting final answers and CoT steps\n",
    "def extract_final_answer(answer_text):\n",
    "    # GSM8K typically has the answer after \"The answer is\"\n",
    "    matches = re.search(r\"The answer is\\s*[-]?\\s*\\$?\\s*([\\d,\\.]+)\", answer_text, re.DOTALL | re.IGNORECASE)\n",
    "    if matches:\n",
    "        return matches.group(1).strip()\n",
    "    \n",
    "    # Look for \"Therefore\"\n",
    "    matches = re.search(r\"Therefore,?\\s.*?[-]?\\s*\\$?\\s*([\\d,\\.]+)\", answer_text, re.DOTALL | re.IGNORECASE)\n",
    "    if matches:\n",
    "        return matches.group(1).strip()\n",
    "    \n",
    "    # Look for \"So\"\n",
    "    matches = re.search(r\"So,?\\s.*?[-]?\\s*\\$?\\s*([\\d,\\.]+)\", answer_text, re.DOTALL | re.IGNORECASE)\n",
    "    if matches:\n",
    "        return matches.group(1).strip()\n",
    "    \n",
    "    # Fallback: last numeric value in the text\n",
    "    numbers = re.findall(r\"\\d+(?:,\\d+)*(?:\\.\\d+)?\", answer_text)\n",
    "    if numbers:\n",
    "        return numbers[-1].strip()\n",
    "    \n",
    "    return answer_text.strip().split(\"\\n\")[-1]  # Last line as fallback\n",
    "\n",
    "\n",
    "def extract_cot_steps(answer_text):\n",
    "    # Remove the final answer part\n",
    "    final_answer_match = re.search(r\"The answer is(.*?)$\", answer_text, re.DOTALL)\n",
    "    if final_answer_match:\n",
    "        cot_text = answer_text[:final_answer_match.start()].strip()\n",
    "    else:\n",
    "        # If no \"The answer is\" pattern, assume the last line is the answer\n",
    "        lines = answer_text.strip().split(\"\\n\")\n",
    "        cot_text = \"\\n\".join(lines[:-1]) if len(lines) > 1 else \"\"\n",
    "    \n",
    "    # Split into steps\n",
    "    steps = [step.strip() for step in cot_text.split(\"\\n\") if step.strip()]\n",
    "    return steps\n",
    "\n",
    "# Safe device transfer function for M1 MPS\n",
    "def to_device(tensor_or_module):\n",
    "    \"\"\"Safely move tensors or modules to the selected device\"\"\"\n",
    "    if tensor_or_module is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        return tensor_or_module.to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not move to {device}: {e}\")\n",
    "        return tensor_or_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare GSM8K dataset\n",
    "class GSM8KDataset(Dataset):\n",
    "    def __init__(self, split=\"train\", tokenizer=None, max_length=512, max_samples=None):\n",
    "        self.data = load_dataset(\"gsm8k\", \"main\")[split]\n",
    "        if max_samples:\n",
    "            # Limit the number of samples for M1 Mac to reduce memory usage\n",
    "            self.data = self.data.select(range(min(max_samples, len(self.data))))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.processed_data = self.preprocess_data()\n",
    "        \n",
    "    def preprocess_data(self):\n",
    "        processed = []\n",
    "        for item in tqdm(self.data, desc=\"Preprocessing data\"):\n",
    "            question = item[\"question\"]\n",
    "            answer_with_cot = item[\"answer\"]\n",
    "            \n",
    "            # Extract the CoT steps and the final answer\n",
    "            final_answer = extract_final_answer(answer_with_cot)\n",
    "            cot_steps = extract_cot_steps(answer_with_cot)\n",
    "            \n",
    "            processed.append({\n",
    "                \"question\": question,\n",
    "                \"cot_steps\": cot_steps,\n",
    "                \"final_answer\": final_answer,\n",
    "                \"full_answer\": answer_with_cot\n",
    "            })\n",
    "        return processed\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.processed_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.processed_data[idx]\n",
    "        \n",
    "        if self.tokenizer:\n",
    "            # Prepare input (question + CoT prompt)\n",
    "            input_text = item[\"question\"] + \" \" + COT_PROMPT\n",
    "            target_text = item[\"full_answer\"]\n",
    "            \n",
    "            # Handle tokenization with mps device\n",
    "            try:\n",
    "                inputs = self.tokenizer(\n",
    "                    input_text,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=self.max_length,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                targets = self.tokenizer(\n",
    "                    target_text,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=self.max_length,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                return {\n",
    "                    \"input_ids\": inputs.input_ids.squeeze(),\n",
    "                    \"attention_mask\": inputs.attention_mask.squeeze(),\n",
    "                    \"labels\": targets.input_ids.squeeze(),\n",
    "                    \"raw_question\": item[\"question\"],\n",
    "                    \"raw_cot\": item[\"cot_steps\"],\n",
    "                    \"raw_answer\": item[\"final_answer\"]\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"Error tokenizing item {idx}: {e}\")\n",
    "                # Return a simple fallback\n",
    "                dummy_tensor = torch.zeros(self.max_length, dtype=torch.long)\n",
    "                return {\n",
    "                    \"input_ids\": dummy_tensor,\n",
    "                    \"attention_mask\": dummy_tensor,\n",
    "                    \"labels\": dummy_tensor,\n",
    "                    \"raw_question\": item[\"question\"],\n",
    "                    \"raw_cot\": item[\"cot_steps\"],\n",
    "                    \"raw_answer\": item[\"final_answer\"]\n",
    "                }\n",
    "        else:\n",
    "            return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoTGenerator:\n",
    "    def __init__(self, model_name=\"t5-base\", local_dir=\"./models/t5_base_cache\"):\n",
    "        self.model_name = model_name\n",
    "        self.local_dir = local_dir\n",
    "        self.model_path = os.path.join(local_dir, model_name.split('/')[-1])\n",
    "        \n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(self.local_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"Loading model {model_name}...\")\n",
    "        \n",
    "        # Check if model is already saved locally - look for specific files\n",
    "        if os.path.exists(os.path.join(self.local_dir, \"pytorch_model.bin\")) and \\\n",
    "           os.path.exists(os.path.join(self.local_dir, \"tokenizer_config.json\")):\n",
    "            print(f\"Found existing model at {self.local_dir}. Loading locally...\")\n",
    "            self._load_local_model()\n",
    "        else:\n",
    "            print(f\"Model not found locally. Downloading {model_name}...\")\n",
    "            self._download_model()\n",
    "        \n",
    "    def _download_model(self):\n",
    "        try:\n",
    "            # Use the download function from your script\n",
    "            print(f\"Downloading model {self.model_name} to {self.local_dir}...\")\n",
    "            \n",
    "            # Download tokenizer and save it immediately\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(\n",
    "                self.model_name,\n",
    "                cache_dir=self.local_dir,\n",
    "                use_fast=True\n",
    "            )\n",
    "            print(f\"Tokenizer downloaded and saved to {self.local_dir}\")\n",
    "            \n",
    "            # Download model and save it immediately\n",
    "            try:\n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                                     \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "                self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    cache_dir=self.local_dir,\n",
    "                    low_cpu_mem_usage=True,  # Helpful for M1 Macs\n",
    "                    torch_dtype=torch.float16  # Use half precision\n",
    "                )\n",
    "                self.model = self.model.to(device)\n",
    "                print(f\"Model downloaded and moved to {device}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model to device: {e}\")\n",
    "                print(\"Falling back to CPU\")\n",
    "                self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    cache_dir=self.local_dir\n",
    "                )\n",
    "                print(f\"Model downloaded (CPU version)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading model: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def _load_local_model(self):\n",
    "        try:\n",
    "            # Load locally saved tokenizer\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(self.local_dir)\n",
    "            \n",
    "            # Load locally saved model\n",
    "            try:\n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                                     \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "                self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "                    self.local_dir,\n",
    "                    torch_dtype=torch.float16\n",
    "                )\n",
    "                self.model = self.model.to(device)\n",
    "                print(f\"Model loaded from {self.local_dir} and moved to {device}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model to device: {e}\")\n",
    "                print(\"Falling back to CPU\")\n",
    "                self.model = T5ForConditionalGeneration.from_pretrained(self.local_dir)\n",
    "                print(f\"Model loaded from {self.local_dir} (CPU version)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading local model: {e}\")\n",
    "            print(\"Will attempt to download from source...\")\n",
    "            self._download_model()\n",
    "        \n",
    "    def generate(self, question, max_length=512, cot_prompt=\"Let's think step by step.\"):\n",
    "        input_text = f\"{question} {cot_prompt}\"\n",
    "        \n",
    "        try:\n",
    "            inputs = self.tokenizer(\n",
    "                input_text, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True,\n",
    "                max_length=max_length\n",
    "            )\n",
    "            device = self.model.device\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # For MPS compatibility, use simpler generation parameters\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    max_length=max_length,\n",
    "                    num_return_sequences=1,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "            \n",
    "            # Decode the generated text\n",
    "            decoded_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Extract the CoT steps and final answer\n",
    "            try:\n",
    "                final_answer = self._extract_final_answer(decoded_output)\n",
    "                cot_steps = self._extract_cot_steps(decoded_output)\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting steps: {e}\")\n",
    "                # Fallback if extraction fails\n",
    "                cot_steps = [decoded_output.strip()]\n",
    "                final_answer = \"Unable to extract final answer\"\n",
    "            \n",
    "            return {\n",
    "                \"cot_steps\": cot_steps,\n",
    "                \"final_answer\": final_answer,\n",
    "                \"full_output\": decoded_output\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in generation: {e}\")\n",
    "            return {\n",
    "                \"cot_steps\": [\"Error generating steps\"],\n",
    "                \"final_answer\": \"Error\",\n",
    "                \"full_output\": f\"Error: {str(e)}\"\n",
    "            }\n",
    "    \n",
    "    def _extract_final_answer(self, text):\n",
    "        # Simple extraction: look for \"The answer is\" or similar patterns\n",
    "        # You may need to customize this based on your model's output format\n",
    "        if \"The answer is\" in text:\n",
    "            return text.split(\"The answer is\")[-1].strip()\n",
    "        elif \"Therefore,\" in text:\n",
    "            return text.split(\"Therefore,\")[-1].strip()\n",
    "        else:\n",
    "            # Return the last sentence as a fallback\n",
    "            sentences = text.split('.')\n",
    "            if sentences:\n",
    "                return sentences[-1].strip()\n",
    "            return text.strip()\n",
    "    \n",
    "    def _extract_cot_steps(self, text):\n",
    "        # Simple extraction: split by numbering or line breaks\n",
    "        # You may need to customize this based on your model's output format\n",
    "        if any(f\"{i}.\" in text for i in range(1, 10)):\n",
    "            # Try to split by numbered steps\n",
    "            steps = []\n",
    "            for i in range(1, 10):\n",
    "                pattern = f\"{i}.\"\n",
    "                next_pattern = f\"{i+1}.\"\n",
    "                if pattern in text:\n",
    "                    start = text.find(pattern)\n",
    "                    if next_pattern in text:\n",
    "                        end = text.find(next_pattern)\n",
    "                        steps.append(text[start:end].strip())\n",
    "                    else:\n",
    "                        # Last step or only one step\n",
    "                        steps.append(text[start:].strip())\n",
    "            return steps if steps else text.split(\"\\n\")\n",
    "        else:\n",
    "            # Fallback to splitting by newlines\n",
    "            return [s.strip() for s in text.split(\"\\n\") if s.strip()]\n",
    "    \n",
    "    def save(self, path=None):\n",
    "        save_path = path if path else self.local_dir\n",
    "        try:\n",
    "            # Move to CPU before saving to avoid device-specific tensors in saved model\n",
    "            cpu_model = self.model.to(\"cpu\")\n",
    "            cpu_model.save_pretrained(save_path)\n",
    "            self.tokenizer.save_pretrained(save_path)\n",
    "            print(f\"Model saved to {save_path}\")\n",
    "            # Move back to device\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                               \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "            self.model = self.model.to(device)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {e}\")\n",
    "    \n",
    "    def load(self, path=None):\n",
    "        load_path = path if path else self.local_dir\n",
    "        try:\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(load_path)\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                               \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(load_path)\n",
    "            self.model = self.model.to(device)\n",
    "            print(f\"Model loaded from {load_path} and moved to {device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Reflection Module\n",
    "class ReflectionModule(nn.Module):\n",
    "    def __init__(self, embedding_dim=512):  # Smaller embedding for M1\n",
    "        super(ReflectionModule, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4, batch_first=True),\n",
    "            num_layers=1  # Reduced layers for M1\n",
    "        )\n",
    "        self.fc = nn.Linear(embedding_dim, 1)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        \n",
    "    def forward(self, step_embeddings):\n",
    "        # step_embeddings shape: [batch_size, seq_len, embedding_dim]\n",
    "        encoded = self.encoder(step_embeddings)\n",
    "        # Take the mean over the sequence dimension\n",
    "        pooled = encoded.mean(dim=1)\n",
    "        # Output a scalar reward score for each step\n",
    "        score = torch.sigmoid(self.fc(pooled))\n",
    "        return score\n",
    "    \n",
    "    def evaluate_step(self, step_text, question_context):\n",
    "        # Tokenize and get embeddings for the reasoning step\n",
    "        combined_text = f\"{question_context} Step: {step_text}\"\n",
    "        try:\n",
    "            tokens = self.tokenizer(\n",
    "                combined_text, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True,\n",
    "                max_length=MAX_LENGTH\n",
    "            )\n",
    "            tokens = {k: to_device(v) for k, v in tokens.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                embeddings = self.get_embeddings(tokens)\n",
    "                score = self.forward(embeddings)\n",
    "            \n",
    "            return score.item()\n",
    "        except Exception as e:\n",
    "            print(f\"Error in evaluate_step: {e}\")\n",
    "            return 0.5  # Default neutral score\n",
    "    \n",
    "    def get_embeddings(self, tokens):\n",
    "        # This is a placeholder - in a real implementation, you'd use the \n",
    "        # encoder model to get token embeddings\n",
    "        # For simplicity, we're using random embeddings of the right shape\n",
    "        batch_size = tokens[\"input_ids\"].shape[0]\n",
    "        seq_len = tokens[\"input_ids\"].shape[1]\n",
    "        return to_device(torch.randn(batch_size, seq_len, 512))  # Changed to 512 dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Retrieval Module\n",
    "class RetrievalModule:\n",
    "    def __init__(self, embedding_model_name=MODEL_NAME):\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading tokenizer: {e}\")\n",
    "            # Fallback for M1\n",
    "            self.tokenizer = None\n",
    "        self.exemplar_bank = []  # Will contain (question, embedding, cot_sequence) tuples\n",
    "        \n",
    "    def add_exemplar(self, question, cot_sequence):\n",
    "        embedding = self.compute_embedding(question)\n",
    "        self.exemplar_bank.append((question, embedding, cot_sequence))\n",
    "    \n",
    "    def initialize_from_dataset(self, dataset, max_exemplars=100):  # Reduced for M1\n",
    "        \"\"\"Initialize the retrieval module with examples from a dataset\"\"\"\n",
    "        for i, item in enumerate(tqdm(dataset, desc=\"Building exemplar bank\")):\n",
    "            if i >= max_exemplars:\n",
    "                break\n",
    "            try:\n",
    "                self.add_exemplar(item[\"raw_question\"], item[\"raw_cot\"])\n",
    "            except Exception as e:\n",
    "                print(f\"Error adding exemplar {i}: {e}\")\n",
    "    \n",
    "    def compute_embedding(self, text):\n",
    "        # For M1 compatibility, use a simplified embedding approach\n",
    "        try:\n",
    "            if self.tokenizer:\n",
    "                # Tokenize and encode the text\n",
    "                tokens = self.tokenizer(\n",
    "                    text, \n",
    "                    padding=True, \n",
    "                    truncation=True, \n",
    "                    max_length=MAX_LENGTH, \n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                # Generate a deterministic but simplified embedding based on token IDs\n",
    "                token_ids = tokens[\"input_ids\"].numpy().flatten()\n",
    "                # Use a simple hash-based approach for embeddings\n",
    "                embedding = np.zeros(512)  # Reduced dimension for M1\n",
    "                for i, token_id in enumerate(token_ids):\n",
    "                    if i < 512:\n",
    "                        embedding[i % 512] += token_id\n",
    "                \n",
    "                # Normalize\n",
    "                norm = np.linalg.norm(embedding)\n",
    "                if norm > 0:\n",
    "                    embedding = embedding / norm\n",
    "                \n",
    "                return embedding\n",
    "            else:\n",
    "                return np.random.randn(512)  # Fallback\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing embedding: {e}\")\n",
    "            return np.random.randn(512)\n",
    "    \n",
    "    def retrieve_similar_exemplars(self, question, k=2):  # Reduced k for M1\n",
    "        query_embedding = self.compute_embedding(question)\n",
    "        \n",
    "        # Compute similarities\n",
    "        similarities = []\n",
    "        for _, exemplar_embedding, _ in self.exemplar_bank:\n",
    "            try:\n",
    "                sim = cosine_similarity([query_embedding], [exemplar_embedding])[0][0]\n",
    "                similarities.append(sim)\n",
    "            except Exception as e:\n",
    "                print(f\"Error computing similarity: {e}\")\n",
    "                similarities.append(0.0)\n",
    "        \n",
    "        # Get top-k indices\n",
    "        if not similarities:\n",
    "            return []\n",
    "        \n",
    "        top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "        \n",
    "        # Return the corresponding exemplars\n",
    "        return [self.exemplar_bank[i][2] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Transformer-based Refinement Module (replacing the GAN module)\n",
    "class TextRefinementTransformer(nn.Module):\n",
    "    def __init__(self, model_name=MODEL_NAME):\n",
    "        super(TextRefinementTransformer, self).__init__()\n",
    "        try:\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "            self.model = to_device(self.model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing refinement module: {e}\")\n",
    "            # Fallback - dummy model\n",
    "            self.tokenizer = None\n",
    "            self.model = None\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        if self.model is None:\n",
    "            return None\n",
    "        try:\n",
    "            return self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error in forward pass: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def refine_text(self, input_text, max_length=MAX_LENGTH):\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            return input_text  # Fallback: return original text\n",
    "            \n",
    "        try:\n",
    "            inputs = self.tokenizer(\n",
    "                input_text, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True,\n",
    "                max_length=max_length\n",
    "            )\n",
    "            inputs = {k: to_device(v) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():  # No grad for inference\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    max_length=max_length,\n",
    "                    num_return_sequences=1,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "            \n",
    "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error refining text: {e}\")\n",
    "            return input_text  # Fallback\n",
    "    \n",
    "    def train_step(self, batch, optimizer):\n",
    "        if self.model is None:\n",
    "            return 0.0  # Fallback\n",
    "            \n",
    "        try:\n",
    "            self.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Move batch to device\n",
    "            device_batch = {k: to_device(v) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "            \n",
    "            outputs = self.forward(\n",
    "                input_ids=device_batch[\"input_ids\"],\n",
    "                attention_mask=device_batch[\"attention_mask\"],\n",
    "                labels=device_batch[\"labels\"]\n",
    "            )\n",
    "            \n",
    "            if outputs is None:\n",
    "                return 0.0\n",
    "                \n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            return loss.item()\n",
    "        except Exception as e:\n",
    "            print(f\"Error in train_step: {e}\")\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardFunction:\n",
    "    def __init__(self, reflection_module):\n",
    "        self.reflection_module = reflection_module\n",
    "        \n",
    "    def outcome_reward(self, predicted_answer, true_answer):\n",
    "        \"\"\"Improved reward based on partial matching and numeric comparison\"\"\"\n",
    "        try:\n",
    "            # Clean up answers - remove $, commas, and whitespace\n",
    "            pred_clean = re.sub(r'[$,\\s]', '', predicted_answer).strip()\n",
    "            true_clean = re.sub(r'[$,\\s]', '', true_answer).strip()\n",
    "            \n",
    "            # Exact match\n",
    "            if pred_clean == true_clean:\n",
    "                return 1.0\n",
    "                \n",
    "            # Both are numbers - check if they're close\n",
    "            if pred_clean.replace('.', '', 1).isdigit() and true_clean.replace('.', '', 1).isdigit():\n",
    "                try:\n",
    "                    pred_num = float(pred_clean)\n",
    "                    true_num = float(true_clean)\n",
    "                    \n",
    "                    # If they're within 1% of each other\n",
    "                    if abs(pred_num - true_num) / max(abs(true_num), 1) < 0.01:\n",
    "                        return 0.9\n",
    "                        \n",
    "                    # If they're within 10% of each other\n",
    "                    if abs(pred_num - true_num) / max(abs(true_num), 1) < 0.1:\n",
    "                        return 0.7\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Partial string match - useful for non-numeric answers\n",
    "            if pred_clean in true_clean or true_clean in pred_clean:\n",
    "                return 0.8\n",
    "                \n",
    "            # No match\n",
    "            return 0.0\n",
    "        except Exception as e:\n",
    "            print(f\"Error in outcome_reward: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def process_reward(self, cot_steps, question):\n",
    "        \"\"\"Improved reward based on the quality of the reasoning process\"\"\"\n",
    "        if not cot_steps:\n",
    "            return 0.2  # Small baseline reward even with no steps\n",
    "        \n",
    "        try:\n",
    "            # Evaluate each step and take the mean\n",
    "            step_scores = []\n",
    "            for step in cot_steps:\n",
    "                score = self.reflection_module.evaluate_step(step, question)\n",
    "                step_scores.append(score)\n",
    "            \n",
    "            # More generous length bonus\n",
    "            length_bonus = min(len(step_scores) / 3, 1.0)  # Bonus for 3+ steps, capped at 1.0\n",
    "            \n",
    "            # Check for numerical values in steps (good sign for math problems)\n",
    "            has_numbers = any(bool(re.search(r'\\d+', step)) for step in cot_steps)\n",
    "            number_bonus = 0.2 if has_numbers else 0.0\n",
    "            \n",
    "            # Return the mean score with bonuses\n",
    "            base_score = sum(step_scores) / len(step_scores) if step_scores else 0.3\n",
    "            return base_score * (1.0 + 0.5 * length_bonus + number_bonus)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in process_reward: {e}\")\n",
    "            return 0.3  # More generous fallback\n",
    "    \n",
    "    def combined_reward(self, cot_steps, predicted_answer, true_answer, question, alpha=0.6):\n",
    "        \"\"\"Combine process and outcome rewards with detailed logging\"\"\"\n",
    "        try:\n",
    "            outcome = self.outcome_reward(predicted_answer, true_answer)\n",
    "            process = self.process_reward(cot_steps, question)\n",
    "            \n",
    "            # More weight on the process for early training\n",
    "            combined = alpha * outcome + (1 - alpha) * process\n",
    "            \n",
    "            # Debug info\n",
    "            if random.random() < 0.1:  # Only print for 10% of examples to avoid log flooding\n",
    "                print(f\"\\nQuestion: {question[:50]}...\")\n",
    "                print(f\"Predicted: {predicted_answer}\")\n",
    "                print(f\"True: {true_answer}\")\n",
    "                print(f\"Outcome reward: {outcome:.2f}, Process reward: {process:.2f}, Combined: {combined:.2f}\")\n",
    "                if cot_steps:\n",
    "                    print(f\"First reasoning step: {cot_steps[0][:50]}...\")\n",
    "            \n",
    "            return combined\n",
    "        except Exception as e:\n",
    "            print(f\"Error in combined_reward: {e}\")\n",
    "            return 0.3  # More generous fallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Self-Training and Distillation Loop\n",
    "class SelfTrainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        cot_generator,\n",
    "        reflection_module,\n",
    "        retrieval_module,\n",
    "        refinement_module,\n",
    "        reward_function,\n",
    "        tokenizer=None\n",
    "    ):\n",
    "        self.cot_generator = cot_generator\n",
    "        self.reflection_module = reflection_module\n",
    "        self.retrieval_module = retrieval_module\n",
    "        self.refinement_module = refinement_module\n",
    "        self.reward_function = reward_function\n",
    "        try:\n",
    "            self.tokenizer = tokenizer or AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading tokenizer: {e}\")\n",
    "            self.tokenizer = None\n",
    "        \n",
    "    def generate_pseudo_labels(self, dataset, threshold=0.4, max_samples=50):  # Lower threshold!\n",
    "        \"\"\"Generate high-quality pseudo-labeled examples with better logging\"\"\"\n",
    "        pseudo_labeled = []\n",
    "        rewards = []\n",
    "        \n",
    "        # Limit the number of samples for M1\n",
    "        sample_count = min(max_samples, len(dataset))\n",
    "        sample_indices = random.sample(range(len(dataset)), sample_count)\n",
    "        \n",
    "        for i in tqdm(sample_indices, desc=\"Generating pseudo-labels\"):\n",
    "            try:\n",
    "                item = dataset[i]\n",
    "                question = item[\"raw_question\"]\n",
    "                gold_answer = item[\"raw_answer\"]\n",
    "                \n",
    "                # Generate CoT with the current model\n",
    "                generated = self.cot_generator.generate(question)\n",
    "                cot_steps = generated[\"cot_steps\"]\n",
    "                predicted_answer = generated[\"final_answer\"]\n",
    "                \n",
    "                # Calculate reward\n",
    "                reward = self.reward_function.combined_reward(\n",
    "                    cot_steps, predicted_answer, gold_answer, question\n",
    "                )\n",
    "                rewards.append(reward)\n",
    "                \n",
    "                # If the reward is above threshold, add to pseudo-labeled data\n",
    "                if reward >= threshold:\n",
    "                    pseudo_labeled.append({\n",
    "                        \"question\": question,\n",
    "                        \"cot_steps\": cot_steps,\n",
    "                        \"final_answer\": predicted_answer,\n",
    "                        \"reward\": reward\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating pseudo-label for item {i}: {e}\")\n",
    "        \n",
    "        # Print reward statistics\n",
    "        if rewards:\n",
    "            print(f\"Reward stats - Min: {min(rewards):.2f}, Max: {max(rewards):.2f}, Avg: {sum(rewards)/len(rewards):.2f}\")\n",
    "            print(f\"Rewards histogram: {[(r, sum(1 for x in rewards if r-0.1 <= x < r+0.1)) for r in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]]}\")\n",
    "        \n",
    "        # If we still don't have any examples, take the top 20% regardless of threshold\n",
    "        if not pseudo_labeled and rewards:\n",
    "            top_k = max(1, int(0.2 * len(rewards)))  # At least 1, up to 20% of examples\n",
    "            top_indices = sorted(range(len(rewards)), key=lambda i: rewards[i], reverse=True)[:top_k]\n",
    "            \n",
    "            print(f\"Forcing inclusion of top {top_k} examples regardless of threshold\")\n",
    "            for i in top_indices:\n",
    "                if i < len(sample_indices):  # Make sure index is valid\n",
    "                    idx = sample_indices[i]\n",
    "                    if idx < len(dataset):  # Double check index\n",
    "                        try:\n",
    "                            item = dataset[idx]\n",
    "                            question = item[\"raw_question\"]\n",
    "                            generated = self.cot_generator.generate(question)\n",
    "                            \n",
    "                            pseudo_labeled.append({\n",
    "                                \"question\": question,\n",
    "                                \"cot_steps\": generated[\"cot_steps\"],\n",
    "                                \"final_answer\": generated[\"final_answer\"],\n",
    "                                \"reward\": rewards[i]\n",
    "                            })\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error adding forced example {i}: {e}\")\n",
    "        \n",
    "        return pseudo_labeled\n",
    "    \n",
    "    def train_refinement_module(self, dataset, epochs=2):  # Reduced epochs for M1\n",
    "        \"\"\"Train the refinement module to improve CoT quality\"\"\"\n",
    "        if self.refinement_module.model is None:\n",
    "            print(\"Refinement module not initialized, skipping training\")\n",
    "            return\n",
    "            \n",
    "        # Create a smaller dataset for M1\n",
    "        subset_size = min(100, len(dataset))  # Limit size for M1\n",
    "        subset_indices = random.sample(range(len(dataset)), subset_size)\n",
    "        subset = [dataset[i] for i in subset_indices]\n",
    "        \n",
    "        dataloader = DataLoader(subset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        \n",
    "        try:\n",
    "            optimizer = AdamW(self.refinement_module.parameters(), lr=LEARNING_RATE)\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                total_loss = 0\n",
    "                batch_count = 0\n",
    "                \n",
    "                for batch in tqdm(dataloader, desc=f\"Training refinement module - Epoch {epoch+1}/{epochs}\"):\n",
    "                    try:\n",
    "                        loss = self.refinement_module.train_step(batch, optimizer)\n",
    "                        total_loss += loss\n",
    "                        batch_count += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing batch: {e}\")\n",
    "                        continue\n",
    "                    \n",
    "                if batch_count > 0:\n",
    "                    avg_loss = total_loss / batch_count\n",
    "                    print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch+1}/{epochs}, No valid batches processed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in train_refinement_module: {e}\")\n",
    "    \n",
    "    def train_loop(self, train_dataset, val_dataset, epochs=2):\n",
    "        \"\"\"Improved main training loop with better logging and fallbacks\"\"\"\n",
    "        try:\n",
    "            # Initialize retrieval module with a small subset of examples\n",
    "            self.retrieval_module.initialize_from_dataset(train_dataset, max_exemplars=50)\n",
    "            \n",
    "            # Track best validation score for model saving\n",
    "            best_val_reward = 0.0\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                print(f\"\\n=== Starting epoch {epoch+1}/{epochs} ===\")\n",
    "                \n",
    "                # 1. Generate pseudo-labeled data (with lower threshold for M1)\n",
    "                pseudo_labeled = self.generate_pseudo_labels(train_dataset, threshold=0.4, max_samples=50)\n",
    "                print(f\"Generated {len(pseudo_labeled)} pseudo-labeled examples\")\n",
    "                \n",
    "                # Display top examples if available\n",
    "                if pseudo_labeled:\n",
    "                    top_example = max(pseudo_labeled, key=lambda x: x[\"reward\"])\n",
    "                    print(f\"\\nTop example (reward: {top_example['reward']:.2f}):\")\n",
    "                    print(f\"Question: {top_example['question'][:100]}...\")\n",
    "                    print(f\"Answer: {top_example['final_answer']}\")\n",
    "                    print(f\"First step: {top_example['cot_steps'][0] if top_example['cot_steps'] else 'No steps'}\")\n",
    "                \n",
    "                # 3. Validate on a small subset of validation set\n",
    "                val_subset_size = min(20, len(val_dataset))  # Limit for M1\n",
    "                val_subset_indices = random.sample(range(len(val_dataset)), val_subset_size)\n",
    "                \n",
    "                val_rewards = []\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                \n",
    "                for i in tqdm(val_subset_indices, desc=\"Validating\"):\n",
    "                    try:\n",
    "                        item = val_dataset[i]\n",
    "                        question = item[\"raw_question\"]\n",
    "                        gold_answer = item[\"raw_answer\"]\n",
    "                        \n",
    "                        # Generate CoT\n",
    "                        generated = self.cot_generator.generate(question)\n",
    "                        \n",
    "                        # Calculate reward\n",
    "                        reward = self.reward_function.combined_reward(\n",
    "                            generated[\"cot_steps\"], generated[\"final_answer\"], gold_answer, question\n",
    "                        )\n",
    "                        val_rewards.append(reward)\n",
    "                        \n",
    "                        # Check if answer is correct (exact match)\n",
    "                        pred_clean = re.sub(r'[$,\\s]', '', generated[\"final_answer\"]).strip()\n",
    "                        true_clean = re.sub(r'[$,\\s]', '', gold_answer).strip()\n",
    "                        if pred_clean == true_clean:\n",
    "                            correct += 1\n",
    "                        total += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error validating item {i}: {e}\")\n",
    "                \n",
    "                if val_rewards:\n",
    "                    avg_reward = sum(val_rewards) / len(val_rewards)\n",
    "                    accuracy = correct / total if total > 0 else 0\n",
    "                    print(f\"Epoch {epoch+1}/{epochs}, Validation Reward: {avg_reward:.4f}, Accuracy: {accuracy:.4f} ({correct}/{total})\")\n",
    "                    \n",
    "                    # Save best model\n",
    "                    if avg_reward > best_val_reward:\n",
    "                        best_val_reward = avg_reward\n",
    "                        print(f\"New best validation reward: {best_val_reward:.4f}, saving model...\")\n",
    "                        self.cot_generator.save(\"./model_output/best_cot_generator\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch+1}/{epochs}, No valid validation rewards calculated\")\n",
    "                \n",
    "            print(f\"\\nTraining complete. Best validation reward: {best_val_reward:.4f}\")\n",
    "            return best_val_reward\n",
    "        except Exception as e:\n",
    "            print(f\"Error in train_loop: {e}\")\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluation Functions\n",
    "def train_model(train_data, val_data):\n",
    "    try:\n",
    "        # Initialize all components with error handling\n",
    "        print(\"Initializing model components...\")\n",
    "        \n",
    "        cot_generator = CoTGenerator()\n",
    "        print(\"CoT Generator initialized\")\n",
    "        \n",
    "        reflection_module = ReflectionModule()\n",
    "        reflection_module = to_device(reflection_module)\n",
    "        print(\"Reflection Module initialized\")\n",
    "        \n",
    "        retrieval_module = RetrievalModule()\n",
    "        print(\"Retrieval Module initialized\")\n",
    "        \n",
    "        refinement_module = TextRefinementTransformer()\n",
    "        refinement_module = to_device(refinement_module)\n",
    "        print(\"Refinement Module initialized\")\n",
    "        \n",
    "        reward_function = RewardFunction(reflection_module)\n",
    "        print(\"Reward Function initialized\")\n",
    "        \n",
    "        # Create the self-trainer\n",
    "        trainer = SelfTrainer(\n",
    "            cot_generator=cot_generator,\n",
    "            reflection_module=reflection_module,\n",
    "            retrieval_module=retrieval_module,\n",
    "            refinement_module=refinement_module,\n",
    "            reward_function=reward_function\n",
    "        )\n",
    "        print(\"Self-Trainer initialized\")\n",
    "        \n",
    "        # Train the model\n",
    "        trainer.train_loop(train_data, val_data, epochs=EPOCHS)\n",
    "        \n",
    "        # Save the trained generator model\n",
    "        print(\"Saving model...\")\n",
    "        os.makedirs(\"./model_output\", exist_ok=True)\n",
    "        cot_generator.save(\"./model_output/cot_generator_model\")\n",
    "        print(\"Model saved to ./model_output/cot_generator_model\")\n",
    "        \n",
    "        return cot_generator\n",
    "    except Exception as e:\n",
    "        print(f\"Error in train_model: {e}\")\n",
    "        # Return a default model anyway so the script can continue\n",
    "        return CoTGenerator()\n",
    "\n",
    "def evaluate_model(model, test_data, max_samples=20):  # Limited for M1\n",
    "    try:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Limit test samples for M1\n",
    "        sample_indices = random.sample(range(len(test_data)), min(max_samples, len(test_data)))\n",
    "        \n",
    "        for i in tqdm(sample_indices, desc=\"Evaluating\"):\n",
    "            try:\n",
    "                item = test_data[i]\n",
    "                question = item[\"raw_question\"]\n",
    "                gold_answer = item[\"raw_answer\"]\n",
    "                \n",
    "                generated = model.generate(question)\n",
    "                predicted_answer = generated[\"final_answer\"]\n",
    "                \n",
    "                # Simple exact match evaluation\n",
    "                if predicted_answer.strip() == gold_answer.strip():\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating item {i}: {e}\")\n",
    "        \n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        print(f\"Evaluation Accuracy: {accuracy:.4f} ({correct}/{total})\")\n",
    "        return accuracy\n",
    "    except Exception as e:\n",
    "        print(f\"Error in evaluate_model: {e}\")\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Starting CoT reasoning framework for M1 Mac...\")\n",
    "# print(f\"PyTorch version: {torch.__version__}\")\n",
    "# print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# print(\"Loading GSM8K dataset...\")\n",
    "\n",
    "# # Try loading tokenizer with error handling\n",
    "# try:\n",
    "#     tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "#     print(\"Tokenizer loaded successfully\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading tokenizer: {e}\")\n",
    "#     print(\"Proceeding without tokenizer\")\n",
    "#     tokenizer = None\n",
    "\n",
    "# # Load datasets with limited samples for M1\n",
    "# try:\n",
    "#     train_dataset = GSM8KDataset(\"train\", tokenizer, max_samples=200)  # Limited for M1\n",
    "#     val_dataset = GSM8KDataset(\"test\", tokenizer, max_samples=50)  # Limited for M1\n",
    "    \n",
    "#     print(f\"Loaded {len(train_dataset)} training examples and {len(val_dataset)} validation examples\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading datasets: {e}\")\n",
    "#     print(\"Using dummy datasets for testing\")\n",
    "#     # Create dummy datasets for testing\n",
    "#     from torch.utils.data import TensorDataset\n",
    "#     dummy_data = torch.zeros(10, MAX_LENGTH, dtype=torch.long)\n",
    "#     train_dataset = TensorDataset(dummy_data, dummy_data, dummy_data)\n",
    "#     val_dataset = TensorDataset(dummy_data, dummy_data, dummy_data)\n",
    "\n",
    "# # Train the model\n",
    "# print(\"Training model...\")\n",
    "# trained_model = train_model(train_dataset, val_dataset)\n",
    "\n",
    "# # Evaluate the model\n",
    "# print(\"Evaluating model...\")\n",
    "# evaluate_model(trained_model, val_dataset)\n",
    "\n",
    "# # Example inference\n",
    "# example_question = \"Janet's ducks lay 16 eggs per day. She eats 3 for breakfast every morning and sells the rest at the farmers' market for $2 per egg. How much money does she make in a week?\"\n",
    "\n",
    "# print(\"\\nTesting with example question:\")\n",
    "# print(f\"Question: {example_question}\")\n",
    "\n",
    "# try:\n",
    "#     # Generate answer with trained model\n",
    "#     result = trained_model.generate(example_question)\n",
    "    \n",
    "#     print(\"\\nGenerated Chain-of-Thought:\")\n",
    "#     for i, step in enumerate(result[\"cot_steps\"]):\n",
    "#         print(f\"Step {i+1}: {step}\")\n",
    "    \n",
    "#     print(f\"\\nFinal Answer: {result['final_answer']}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error in example inference: {e}\")\n",
    "\n",
    "# print(\"\\nCoT reasoning framework execution completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the training and evaluation process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing data: 100%|██████████| 200/200 [00:00<00:00, 20597.67it/s]\n",
      "Preprocessing data: 100%|██████████| 50/50 [00:00<00:00, 11382.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200 training examples and 50 validation examples\n",
      "Initializing model components...\n",
      "Loading model t5-base...\n",
      "Model not found locally. Downloading t5-base...\n",
      "Downloading model t5-base to ./models/t5_base_cache...\n",
      "Tokenizer downloaded and saved to ./models/t5_base_cache\n",
      "Model downloaded and moved to mps\n",
      "CoT Generator initialized\n",
      "Reflection Module initialized\n",
      "Retrieval Module initialized\n",
      "Refinement Module initialized\n",
      "Reward Function initialized\n",
      "Self-Trainer initialized\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building exemplar bank:  25%|██▌       | 50/200 [00:00<00:00, 301.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting epoch 1/2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pseudo-labels:   6%|▌         | 3/50 [00:14<04:01,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Albert is wondering how much pizza he can eat in o...\n",
      "Predicted: What are the pieces he can eat in one day?\n",
      "True: 48\n",
      "Outcome reward: 0.00, Process reward: 0.66, Combined: 0.26\n",
      "First reasoning step: Albert is a pizza person. Albert wants to eat 16 s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pseudo-labels:  18%|█▊        | 9/50 [00:25<01:26,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Yesterday, David and William were invited to a par...\n",
      "Predicted: \n",
      "True: 8\n",
      "Outcome reward: 0.80, Process reward: 0.65, Combined: 0.74\n",
      "First reasoning step: David broke 2 glasses, while his friend William br...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pseudo-labels:  46%|████▌     | 23/50 [01:04<01:22,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: There are 16 people at a dinner party. There are 4...\n",
      "Predicted: \n",
      "True: 24\n",
      "Outcome reward: 0.80, Process reward: 0.64, Combined: 0.74\n",
      "First reasoning step: 40 dinner rolls are available for them. each. The ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pseudo-labels:  68%|██████▊   | 34/50 [01:30<00:33,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: A concert ticket costs $40. Mr. Benson bought 12 t...\n",
      "Predicted: How about you?\n",
      "True: 40\n",
      "Outcome reward: 0.00, Process reward: 0.65, Combined: 0.26\n",
      "First reasoning step: 5. How about you?...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pseudo-labels:  76%|███████▌  | 38/50 [01:39<00:25,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Anna goes trick-or-treating in a subdivision where...\n",
      "Predicted: How many more pieces of candy does Anna get?\n",
      "True: 15\n",
      "Outcome reward: 0.00, Process reward: 0.70, Combined: 0.28\n",
      "First reasoning step: Her brother Billy goes trick-or-treating in a neig...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pseudo-labels: 100%|██████████| 50/50 [02:13<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward stats - Min: 0.21, Max: 0.87, Avg: 0.59\n",
      "Rewards histogram: [(0.1, 0), (0.2, 16), (0.3, 16), (0.4, 0), (0.5, 0), (0.6, 2), (0.7, 32), (0.8, 32), (0.9, 2)]\n",
      "Generated 34 pseudo-labeled examples\n",
      "\n",
      "Top example (reward: 0.87):\n",
      "Question: There is very little car traffic on Happy Street. During the week, most cars pass it on Tuesday - 25...\n",
      "Answer: \n",
      "First step: On Wednesday, 2 more cars than on Monday. On Thursday, 9 cars. On Friday, 10 cars. On the weekend, 5 cars.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  10%|█         | 2/20 [00:06<00:57,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: John plans to sell all his toys and use the money ...\n",
      "Predicted: \n",
      "True: 2\n",
      "Outcome reward: 0.80, Process reward: 0.66, Combined: 0.74\n",
      "First reasoning step: John sells his toys. he will sell everything he ha...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 20/20 [00:47<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Validation Reward: 0.6423, Accuracy: 0.0500 (1/20)\n",
      "New best validation reward: 0.6423, saving model...\n",
      "Model saved to ./model_output/best_cot_generator\n",
      "\n",
      "=== Starting epoch 2/2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pseudo-labels:   6%|▌         | 3/50 [00:07<01:55,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Mary bought 5 boxes of drinks at $6 each box and 1...\n",
      "Predicted: \n",
      "True: 200\n",
      "Outcome reward: 0.80, Process reward: 0.64, Combined: 0.74\n",
      "First reasoning step: Mary paid $200 for everything she buys. Mary bough...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pseudo-labels:  16%|█▌        | 8/50 [00:19<01:40,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Tim's cat bit him.  He decided to get himself and ...\n",
      "Predicted: \n",
      "True: 300\n",
      "Outcome reward: 0.80, Process reward: 0.62, Combined: 0.73\n",
      "First reasoning step: 5. his cat bit him. his cat insurance covered $60....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pseudo-labels:  38%|███▊      | 19/50 [00:49<01:51,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Bella bought stamps at the post office. Some of th...\n",
      "Predicted: \n",
      "True: 38\n",
      "Outcome reward: 0.80, Process reward: 0.63, Combined: 0.73\n",
      "First reasoning step: Bella walked down the hall with her stamps. She bo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pseudo-labels:  52%|█████▏    | 26/50 [01:14<01:13,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Silvia’s bakery is offering 10% on advanced orders...\n",
      "Predicted: \n",
      "True: 2\n",
      "Outcome reward: 0.80, Process reward: 0.61, Combined: 0.72\n",
      "First reasoning step: Silvia’s bakery is offering 10% on advanced orders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pseudo-labels:  80%|████████  | 40/50 [02:03<00:35,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Lilah's family gallery has 400 photos. On a two-da...\n",
      "Predicted: \n",
      "True: 920\n",
      "Outcome reward: 0.80, Process reward: 0.65, Combined: 0.74\n",
      "First reasoning step: Lilah's family gallery has 400 photos. the family ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pseudo-labels: 100%|██████████| 50/50 [02:26<00:00,  2.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Larry spends half an hour twice a day walking and ...\n",
      "Predicted: \n",
      "True: 72\n",
      "Outcome reward: 0.80, Process reward: 0.56, Combined: 0.70\n",
      "First reasoning step: Larry is a dedicated dog trainer.. Larry spends an...\n",
      "Reward stats - Min: 0.21, Max: 0.82, Avg: 0.63\n",
      "Rewards histogram: [(0.1, 0), (0.2, 11), (0.3, 11), (0.4, 0), (0.5, 0), (0.6, 3), (0.7, 38), (0.8, 36), (0.9, 1)]\n",
      "Generated 39 pseudo-labeled examples\n",
      "\n",
      "Top example (reward: 0.82):\n",
      "Question: Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are y...\n",
      "Answer: \n",
      "First step: He has a garden with flowers. Mark's garden is filled with flowers. He planted plants of three different colors in it. He planted about ten yellow flowers. The yellow flowers are mostly green. There are also green flowers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  20%|██        | 4/20 [00:10<00:40,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Two trains leave San Rafael at the same time. They...\n",
      "Predicted: What's the distance covered?\n",
      "True: 150\n",
      "Outcome reward: 0.00, Process reward: 0.62, Combined: 0.25\n",
      "First reasoning step: the next day, they travel northward. The next day,...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  35%|███▌      | 7/20 [00:18<00:37,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Cynthia eats one serving of ice cream every night....\n",
      "Predicted: \n",
      "True: 60\n",
      "Outcome reward: 0.80, Process reward: 0.72, Combined: 0.77\n",
      "First reasoning step: 1.00 per serving of ice cream. Cynthia's budget is...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  40%|████      | 8/20 [00:20<00:30,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Mike plays ping pong for 40 minutes.  In the first...\n",
      "Predicted: \n",
      "True: 4\n",
      "Outcome reward: 0.80, Process reward: 0.66, Combined: 0.74\n",
      "First reasoning step: In the second 20 minutes, he scores 4 points. 6 po...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  70%|███████   | 14/20 [00:33<00:12,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: James decides to run 3 sprints 3 times a week.  He...\n",
      "Predicted: \n",
      "True: 9\n",
      "Outcome reward: 0.80, Process reward: 0.65, Combined: 0.74\n",
      "First reasoning step: . He runs 60 meters each sprint..? 668 meters each...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  75%|███████▌  | 15/20 [00:35<00:10,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Siobhan has 2 fewer jewels than Aaron. Aaron has 5...\n",
      "Predicted: \n",
      "True: 23\n",
      "Outcome reward: 0.80, Process reward: 0.64, Combined: 0.73\n",
      "First reasoning step: Aaron has 5 more jewels than Aaron's.. Aaron has 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 20/20 [00:48<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Validation Reward: 0.6159, Accuracy: 0.0000 (0/20)\n",
      "\n",
      "Training complete. Best validation reward: 0.6423\n",
      "Saving final model...\n",
      "Model saved to ./model_output/final_cot_generator\n",
      "Model saved to ./model_output/final_cot_generator\n",
      "\n",
      "Final evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 20/20 [00:47<00:00,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 0.0000 (0/20)\n",
      "\n",
      "Training complete!\n",
      "Best validation reward: 0.6423\n",
      "Final evaluation accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Starting the training and evaluation process...\")\n",
    "\n",
    "# Set up tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Create datasets with limited size\n",
    "print(\"Creating datasets...\")\n",
    "train_dataset = GSM8KDataset(\"train\", tokenizer, MAX_LENGTH, max_samples=200)\n",
    "val_dataset = GSM8KDataset(\"test\", tokenizer, MAX_LENGTH, max_samples=50)\n",
    "print(f\"Loaded {len(train_dataset)} training examples and {len(val_dataset)} validation examples\")\n",
    "\n",
    "# Initialize all components\n",
    "print(\"Initializing model components...\")\n",
    "cot_generator = CoTGenerator()\n",
    "print(\"CoT Generator initialized\")\n",
    "\n",
    "reflection_module = ReflectionModule()\n",
    "reflection_module = to_device(reflection_module)\n",
    "print(\"Reflection Module initialized\")\n",
    "\n",
    "retrieval_module = RetrievalModule()\n",
    "print(\"Retrieval Module initialized\")\n",
    "\n",
    "refinement_module = TextRefinementTransformer()\n",
    "refinement_module = to_device(refinement_module)\n",
    "print(\"Refinement Module initialized\")\n",
    "\n",
    "# Use improved reward function\n",
    "reward_function = RewardFunction(reflection_module)\n",
    "print(\"Reward Function initialized\")\n",
    "\n",
    "# Create the self-trainer\n",
    "trainer = SelfTrainer(\n",
    "    cot_generator=cot_generator,\n",
    "    reflection_module=reflection_module,\n",
    "    retrieval_module=retrieval_module,\n",
    "    refinement_module=refinement_module,\n",
    "    reward_function=reward_function\n",
    ")\n",
    "print(\"Self-Trainer initialized\")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training model...\")\n",
    "best_reward = trainer.train_loop(train_dataset, val_dataset, epochs=EPOCHS)\n",
    "\n",
    "# Save the trained generator model\n",
    "print(\"Saving final model...\")\n",
    "os.makedirs(\"./model_output\", exist_ok=True)\n",
    "cot_generator.save(\"./model_output/final_cot_generator\")\n",
    "print(\"Model saved to ./model_output/final_cot_generator\")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nFinal evaluation...\")\n",
    "accuracy = evaluate_model(cot_generator, val_dataset, max_samples=20)\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Best validation reward: {best_reward:.4f}\")\n",
    "print(f\"Final evaluation accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
