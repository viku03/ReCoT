{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T20:27:38.220487Z",
     "iopub.status.busy": "2025-03-11T20:27:38.220294Z",
     "iopub.status.idle": "2025-03-11T20:27:38.238414Z",
     "shell.execute_reply": "2025-03-11T20:27:38.237637Z",
     "shell.execute_reply.started": "2025-03-11T20:27:38.220470Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer\n",
    ")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from torch.optim import Adam\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T20:27:43.772179Z",
     "iopub.status.busy": "2025-03-11T20:27:43.771887Z",
     "iopub.status.idle": "2025-03-11T20:27:43.778307Z",
     "shell.execute_reply": "2025-03-11T20:27:43.777406Z",
     "shell.execute_reply.started": "2025-03-11T20:27:43.772158Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=11):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T20:27:44.116780Z",
     "iopub.status.busy": "2025-03-11T20:27:44.116459Z",
     "iopub.status.idle": "2025-03-11T20:27:44.121826Z",
     "shell.execute_reply": "2025-03-11T20:27:44.121061Z",
     "shell.execute_reply.started": "2025-03-11T20:27:44.116757Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device\n"
     ]
    }
   ],
   "source": [
    "# Device configuration - M1 Mac specific\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple M1 MPS device\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T20:27:44.312503Z",
     "iopub.status.busy": "2025-03-11T20:27:44.312230Z",
     "iopub.status.idle": "2025-03-11T20:27:44.316473Z",
     "shell.execute_reply": "2025-03-11T20:27:44.315629Z",
     "shell.execute_reply.started": "2025-03-11T20:27:44.312482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 768\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 3\n",
    "COT_PROMPT = \"Let's solve this step-by-step. To find the answer, I'll break down the problem into smaller parts.\"\n",
    "MODEL_NAME = \"t5-base\"\n",
    "MAX_SAMPLES = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T20:27:44.477156Z",
     "iopub.status.busy": "2025-03-11T20:27:44.476884Z",
     "iopub.status.idle": "2025-03-11T20:27:44.484361Z",
     "shell.execute_reply": "2025-03-11T20:27:44.483494Z",
     "shell.execute_reply.started": "2025-03-11T20:27:44.477136Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Utility functions for extracting final answers and CoT steps\n",
    "def extract_final_answer(answer_text):\n",
    "    # Look for patterns like \"The answer is X\" or \"Therefore, the answer is X\"\n",
    "    patterns = [\n",
    "        r\"The answer is\\s*[-]?\\s*\\$?\\s*([\\d,\\.]+)\",\n",
    "        r\"Therefore,?\\s.*?[-]?\\s*\\$?\\s*([\\d,\\.]+)\",\n",
    "        r\"So,?\\s.*?[-]?\\s*\\$?\\s*([\\d,\\.]+)\",\n",
    "        r\"Thus,?\\s.*?[-]?\\s*\\$?\\s*([\\d,\\.]+)\",\n",
    "        r\"The final answer is\\s*[-]?\\s*\\$?\\s*([\\d,\\.]+)\",\n",
    "        # Add a pattern to catch just the last number in the text\n",
    "        r\".*?([\\d,\\.]+)$\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.search(pattern, answer_text, re.DOTALL | re.IGNORECASE)\n",
    "        if matches:\n",
    "            return matches.group(1).strip()\n",
    "    \n",
    "    # If no patterns match, extract the last number in the text\n",
    "    numbers = re.findall(r\"\\d+(?:,\\d+)*(?:\\.\\d+)?\", answer_text)\n",
    "    if numbers:\n",
    "        return numbers[-1].strip()\n",
    "    \n",
    "    # Last resort fallback\n",
    "    return answer_text.strip().split(\"\\n\")[-1]\n",
    "\n",
    "def extract_cot_steps(answer_text):\n",
    "    # Remove the final answer part\n",
    "    final_answer_match = re.search(r\"The answer is(.*?)$\", answer_text, re.DOTALL)\n",
    "    if final_answer_match:\n",
    "        cot_text = answer_text[:final_answer_match.start()].strip()\n",
    "    else:\n",
    "        # If no \"The answer is\" pattern, assume the last line is the answer\n",
    "        lines = answer_text.strip().split(\"\\n\")\n",
    "        cot_text = \"\\n\".join(lines[:-1]) if len(lines) > 1 else \"\"\n",
    "    \n",
    "    # Split into steps\n",
    "    steps = [step.strip() for step in cot_text.split(\"\\n\") if step.strip()]\n",
    "    return steps\n",
    "\n",
    "# Safe device transfer function for M1 MPS\n",
    "def to_device(tensor_or_module):\n",
    "    \"\"\"Safely move tensors or modules to the selected device\"\"\"\n",
    "    if tensor_or_module is None:\n",
    "        return None\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                         \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    \n",
    "    try:\n",
    "        return tensor_or_module.to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not move to {device}: {e}\")\n",
    "        return tensor_or_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T20:27:44.646677Z",
     "iopub.status.busy": "2025-03-11T20:27:44.646341Z",
     "iopub.status.idle": "2025-03-11T20:27:44.654875Z",
     "shell.execute_reply": "2025-03-11T20:27:44.654057Z",
     "shell.execute_reply.started": "2025-03-11T20:27:44.646650Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GSM8KDataset(Dataset):\n",
    "    def __init__(self, split=\"train\", tokenizer=None, max_length=768, max_samples=None):\n",
    "        self.data = load_dataset(\"gsm8k\", \"main\")[split]\n",
    "        if max_samples:\n",
    "            self.data = self.data.select(range(min(max_samples, len(self.data))))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.processed_data = self.preprocess_data()\n",
    "        \n",
    "    def preprocess_data(self):\n",
    "        processed = []\n",
    "        for item in tqdm(self.data, desc=\"Preprocessing data\"):\n",
    "            question = item[\"question\"]\n",
    "            answer_with_cot = item[\"answer\"]\n",
    "            \n",
    "            # Extract the CoT steps and the final answer\n",
    "            final_answer = extract_final_answer(answer_with_cot)\n",
    "            cot_steps = extract_cot_steps(answer_with_cot)\n",
    "            \n",
    "            # Format for T5 training - improved prompt to guide the model\n",
    "            formatted_question = f\"Solve this math problem step-by-step: {question} {COT_PROMPT}\"\n",
    "            \n",
    "            processed.append({\n",
    "                \"question\": question,\n",
    "                \"formatted_question\": formatted_question,\n",
    "                \"cot_steps\": cot_steps,\n",
    "                \"final_answer\": final_answer,\n",
    "                \"full_answer\": answer_with_cot\n",
    "            })\n",
    "        return processed\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.processed_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.processed_data[idx]\n",
    "        \n",
    "        if self.tokenizer:\n",
    "            # Prepare input with task-specific prefix for T5\n",
    "            input_text = item[\"formatted_question\"]\n",
    "            target_text = item[\"full_answer\"]\n",
    "            \n",
    "            # Improved tokenization with more balanced token allocation\n",
    "            try:\n",
    "                inputs = self.tokenizer(\n",
    "                    input_text,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=self.max_length // 3,  # Allow more space for output\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                targets = self.tokenizer(\n",
    "                    target_text,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=self.max_length * 2 // 3,  # Allow more space for reasoning\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                return {\n",
    "                    \"input_ids\": inputs.input_ids.squeeze(),\n",
    "                    \"attention_mask\": inputs.attention_mask.squeeze(),\n",
    "                    \"labels\": targets.input_ids.squeeze(),\n",
    "                    \"raw_question\": item[\"question\"],\n",
    "                    \"raw_cot\": item[\"cot_steps\"],\n",
    "                    \"raw_answer\": item[\"final_answer\"]\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"Error tokenizing item {idx}: {e}\")\n",
    "                # Return a simple fallback\n",
    "                dummy_tensor = torch.zeros(self.max_length, dtype=torch.long)\n",
    "                return {\n",
    "                    \"input_ids\": dummy_tensor,\n",
    "                    \"attention_mask\": dummy_tensor,\n",
    "                    \"labels\": dummy_tensor,\n",
    "                    \"raw_question\": item[\"question\"],\n",
    "                    \"raw_cot\": item[\"cot_steps\"],\n",
    "                    \"raw_answer\": item[\"final_answer\"]\n",
    "                }\n",
    "        else:\n",
    "            return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T20:27:44.911820Z",
     "iopub.status.busy": "2025-03-11T20:27:44.911469Z",
     "iopub.status.idle": "2025-03-11T20:27:44.936406Z",
     "shell.execute_reply": "2025-03-11T20:27:44.935508Z",
     "shell.execute_reply.started": "2025-03-11T20:27:44.911793Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CoTGenerator:\n",
    "    def __init__(self, model_name=\"t5-base\", local_dir=\"./models/t5_base_cache\"):\n",
    "        self.model_name = model_name\n",
    "        self.local_dir = local_dir\n",
    "        \n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(self.local_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"Loading model {model_name}...\")\n",
    "        \n",
    "        # Check if model is already saved locally\n",
    "        if os.path.exists(os.path.join(self.local_dir, \"pytorch_model.bin\")) and \\\n",
    "           os.path.exists(os.path.join(self.local_dir, \"tokenizer_config.json\")):\n",
    "            print(f\"Found existing model at {self.local_dir}. Loading locally...\")\n",
    "            self._load_local_model()\n",
    "        else:\n",
    "            print(f\"Model not found locally. Downloading {model_name}...\")\n",
    "            self._download_model()\n",
    "    \n",
    "    def _download_model(self):\n",
    "        try:\n",
    "            # Download tokenizer and save it immediately\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(\n",
    "                self.model_name,\n",
    "                cache_dir=self.local_dir,\n",
    "                use_fast=True\n",
    "            )\n",
    "            print(f\"Tokenizer downloaded and saved to {self.local_dir}\")\n",
    "            \n",
    "            # Download model and save it immediately\n",
    "            try:\n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                                    \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "                self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    cache_dir=self.local_dir,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                    torch_dtype=torch.float16 if device.type != \"cpu\" else torch.float32\n",
    "                )\n",
    "                self.model = self.model.to(device)\n",
    "                print(f\"Model downloaded and moved to {device}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model to device: {e}\")\n",
    "                print(\"Falling back to CPU\")\n",
    "                self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    cache_dir=self.local_dir\n",
    "                )\n",
    "                print(f\"Model downloaded (CPU version)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading model: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def _load_local_model(self):\n",
    "        try:\n",
    "            # Load locally saved tokenizer\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(self.local_dir)\n",
    "            \n",
    "            # Load locally saved model\n",
    "            try:\n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                                    \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "                self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "                    self.local_dir,\n",
    "                    torch_dtype=torch.float16 if device.type != \"cpu\" else torch.float32\n",
    "                )\n",
    "                self.model = self.model.to(device)\n",
    "                print(f\"Model loaded from {self.local_dir} and moved to {device}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model to device: {e}\")\n",
    "                print(\"Falling back to CPU\")\n",
    "                self.model = T5ForConditionalGeneration.from_pretrained(self.local_dir)\n",
    "                print(f\"Model loaded from {self.local_dir} (CPU version)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading local model: {e}\")\n",
    "            print(\"Will attempt to download from source...\")\n",
    "            self._download_model()\n",
    "    \n",
    "    def generate_step(self, question, previous_steps=None, max_length=256):\n",
    "        \"\"\"Generate a single reasoning step based on the question and previous steps\"\"\"\n",
    "        if previous_steps is None:\n",
    "            previous_steps = []\n",
    "        \n",
    "        # Construct prompt with previous steps\n",
    "        previous_steps_text = \"\"\n",
    "        if previous_steps:\n",
    "            previous_steps_text = \"Steps so far:\\n\" + \"\\n\".join([f\"{i+1}. {step}\" for i, step in enumerate(previous_steps)])\n",
    "            previous_steps_text += \"\\nNext step:\"\n",
    "        \n",
    "        # Build the full prompt\n",
    "        if previous_steps:\n",
    "            input_text = f\"Solve this math problem step-by-step: {question}\\n{previous_steps_text}\"\n",
    "        else:\n",
    "            input_text = f\"Solve this math problem step-by-step: {question}\\nFirst step:\"\n",
    "        \n",
    "        try:\n",
    "            inputs = self.tokenizer(\n",
    "                input_text, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True,\n",
    "                max_length=max_length // 2  # Use half of max length for input\n",
    "            )\n",
    "            device = self.model.device\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Generate the next step with focused parameters\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    max_length=max_length,\n",
    "                    num_return_sequences=1,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    top_k=40,\n",
    "                    num_beams=3,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "            \n",
    "            # Decode the generated text\n",
    "            decoded_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Clean up the output (get just the next step)\n",
    "            step = decoded_output.strip()\n",
    "            \n",
    "            # Check if this is a final answer step\n",
    "            is_final_step = (\"answer is\" in step.lower() or \n",
    "                             \"therefore\" in step.lower() or \n",
    "                             \"thus\" in step.lower() or\n",
    "                             \"final answer\" in step.lower())\n",
    "            \n",
    "            return {\n",
    "                \"step\": step,\n",
    "                \"is_final_step\": is_final_step\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating step: {e}\")\n",
    "            return {\n",
    "                \"step\": \"Error generating step\",\n",
    "                \"is_final_step\": True  # Force termination on error\n",
    "            }\n",
    "    \n",
    "    def evaluate_step(self, step, question, reflection_module=None):\n",
    "        \"\"\"Evaluate if a generated step is good or needs refinement\"\"\"\n",
    "        if reflection_module:\n",
    "            # Use the reflection module if provided\n",
    "            score = reflection_module.evaluate_step(step, question)\n",
    "            return score >= 0.5  # Return True if score is good enough\n",
    "        \n",
    "        # Basic heuristic evaluation if no reflection module\n",
    "        # Check if the step contains numbers and mathematical operations\n",
    "        has_numbers = bool(re.search(r'\\d+', step))\n",
    "        has_math_ops = bool(re.search(r'[+\\-*/=]', step))\n",
    "        reasonable_length = 10 <= len(step.split()) <= 100\n",
    "        \n",
    "        return has_numbers and has_math_ops and reasonable_length\n",
    "    \n",
    "    def refine_step(self, question, step, previous_steps=None):\n",
    "        \"\"\"Refine a step that didn't pass evaluation\"\"\"\n",
    "        if previous_steps is None:\n",
    "            previous_steps = []\n",
    "        \n",
    "        # Create a prompt asking for improvement\n",
    "        previous_steps_text = \"\"\n",
    "        if previous_steps:\n",
    "            previous_steps_text = \"Steps so far:\\n\" + \"\\n\".join([f\"{i+1}. {step}\" for i, step in enumerate(previous_steps)])\n",
    "        \n",
    "        input_text = (f\"Solve this math problem step-by-step: {question}\\n\"\n",
    "                     f\"{previous_steps_text}\\n\"\n",
    "                     f\"The following step needs improvement: {step}\\n\"\n",
    "                     f\"Improved step:\")\n",
    "        \n",
    "        try:\n",
    "            inputs = self.tokenizer(\n",
    "                input_text, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            )\n",
    "            device = self.model.device\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    max_length=256,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.6\n",
    "                )\n",
    "            \n",
    "            refined_step = self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "            return refined_step\n",
    "        except Exception as e:\n",
    "            print(f\"Error refining step: {e}\")\n",
    "            return step  # Return original if refinement fails\n",
    "    \n",
    "    def generate(self, question, max_length=768, cot_prompt=None, reflection_module=None, max_steps=8):\n",
    "        \"\"\"Generate a full chain-of-thought reasoning process step by step\"\"\"\n",
    "        cot_steps = []\n",
    "        final_answer = None\n",
    "        \n",
    "        # Generate steps iteratively\n",
    "        for step_num in range(max_steps):\n",
    "            # Generate the next reasoning step\n",
    "            step_result = self.generate_step(question, cot_steps)\n",
    "            current_step = step_result[\"step\"]\n",
    "            \n",
    "            # Evaluate the step quality\n",
    "            step_is_good = self.evaluate_step(current_step, question, reflection_module)\n",
    "            \n",
    "            # Refine the step if needed (up to 2 attempts)\n",
    "            refinement_attempts = 0\n",
    "            while not step_is_good and refinement_attempts < 2:\n",
    "                refined_step = self.refine_step(question, current_step, cot_steps)\n",
    "                current_step = refined_step\n",
    "                step_is_good = self.evaluate_step(current_step, question, reflection_module)\n",
    "                refinement_attempts += 1\n",
    "            \n",
    "            # Add the step to our chain of thought\n",
    "            cot_steps.append(current_step)\n",
    "            \n",
    "            # Check if this is the final step\n",
    "            if step_result[\"is_final_step\"] or step_num == max_steps - 1:\n",
    "                break\n",
    "        \n",
    "        # Generate a final answer if needed\n",
    "        if not any(\"answer is\" in step.lower() for step in cot_steps):\n",
    "            final_step_result = self.generate_final_answer(question, cot_steps)\n",
    "            cot_steps.append(final_step_result[\"step\"])\n",
    "        \n",
    "        # Extract the final answer using the dedicated function\n",
    "        full_output = \"\\n\".join(cot_steps)\n",
    "        final_answer = extract_final_answer(full_output)\n",
    "        \n",
    "        return {\n",
    "            \"cot_steps\": cot_steps,\n",
    "            \"final_answer\": final_answer,\n",
    "            \"full_output\": full_output\n",
    "        }\n",
    "    \n",
    "    def generate_final_answer(self, question, cot_steps):\n",
    "        \"\"\"Generate a final answer step based on previous reasoning steps\"\"\"\n",
    "        steps_text = \"\\n\".join([f\"{i+1}. {step}\" for i, step in enumerate(cot_steps)])\n",
    "        \n",
    "        input_text = (f\"Solve this math problem step-by-step: {question}\\n\"\n",
    "                     f\"Steps so far:\\n{steps_text}\\n\"\n",
    "                     f\"Final answer:\")\n",
    "        \n",
    "        try:\n",
    "            inputs = self.tokenizer(\n",
    "                input_text, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            )\n",
    "            device = self.model.device\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    max_length=128,\n",
    "                    do_sample=False,  # More deterministic for final answer\n",
    "                    num_beams=3\n",
    "                )\n",
    "            \n",
    "            final_step = self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "            \n",
    "            # Make sure it starts with \"The answer is\" if it doesn't already\n",
    "            if not any(phrase in final_step.lower() for phrase in [\"answer is\", \"therefore\", \"thus\", \"final answer\"]):\n",
    "                final_step = \"The answer is \" + final_step\n",
    "                \n",
    "            return {\n",
    "                \"step\": final_step,\n",
    "                \"is_final_step\": True\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating final answer: {e}\")\n",
    "            return {\n",
    "                \"step\": \"The answer is unknown due to an error.\",\n",
    "                \"is_final_step\": True\n",
    "            }\n",
    "    \n",
    "    def save(self, path=None):\n",
    "        save_path = path if path else self.local_dir\n",
    "        try:\n",
    "            # Move to CPU before saving to avoid device-specific tensors in saved model\n",
    "            cpu_model = self.model.to(\"cpu\")\n",
    "            cpu_model.save_pretrained(save_path)\n",
    "            self.tokenizer.save_pretrained(save_path)\n",
    "            print(f\"Model saved to {save_path}\")\n",
    "            # Move back to device\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                               \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "            self.model = self.model.to(device)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {e}\")\n",
    "    \n",
    "    def load(self, path=None):\n",
    "        load_path = path if path else self.local_dir\n",
    "        try:\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(load_path)\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                               \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(load_path)\n",
    "            self.model = self.model.to(device)\n",
    "            print(f\"Model loaded from {load_path} and moved to {device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T20:27:44.976927Z",
     "iopub.status.busy": "2025-03-11T20:27:44.976645Z",
     "iopub.status.idle": "2025-03-11T20:27:44.983607Z",
     "shell.execute_reply": "2025-03-11T20:27:44.982814Z",
     "shell.execute_reply.started": "2025-03-11T20:27:44.976904Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ReflectionModule(nn.Module):\n",
    "    def __init__(self, embedding_dim=768):  # Increased embedding size\n",
    "        super(ReflectionModule, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=8, batch_first=True),\n",
    "            num_layers=2  # Increased layers for better analysis\n",
    "        )\n",
    "        self.fc = nn.Linear(embedding_dim, 1)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        \n",
    "    def forward(self, step_embeddings):\n",
    "        # step_embeddings shape: [batch_size, seq_len, embedding_dim]\n",
    "        encoded = self.encoder(step_embeddings)\n",
    "        # Take the mean over the sequence dimension\n",
    "        pooled = encoded.mean(dim=1)\n",
    "        # Output a scalar reward score for each step\n",
    "        score = torch.sigmoid(self.fc(pooled))\n",
    "        return score\n",
    "    \n",
    "    def evaluate_step(self, step_text, question_context):\n",
    "        # Tokenize and get embeddings for the reasoning step\n",
    "        combined_text = f\"{question_context} Step: {step_text}\"\n",
    "        try:\n",
    "            tokens = self.tokenizer(\n",
    "                combined_text, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True,\n",
    "                max_length=MAX_LENGTH\n",
    "            )\n",
    "            tokens = {k: to_device(v) for k, v in tokens.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                embeddings = self.get_embeddings(tokens)\n",
    "                score = self.forward(embeddings)\n",
    "            \n",
    "            return score.item()\n",
    "        except Exception as e:\n",
    "            print(f\"Error in evaluate_step: {e}\")\n",
    "            return 0.5  # Default neutral score\n",
    "    \n",
    "    def get_embeddings(self, tokens):\n",
    "        # This is a placeholder - in a real implementation, you'd use the \n",
    "        # encoder model to get token embeddings\n",
    "        batch_size = tokens[\"input_ids\"].shape[0]\n",
    "        seq_len = tokens[\"input_ids\"].shape[1]\n",
    "        return to_device(torch.randn(batch_size, seq_len, 768))  # Increased to 768 dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T20:27:45.221865Z",
     "iopub.status.busy": "2025-03-11T20:27:45.221568Z",
     "iopub.status.idle": "2025-03-11T20:27:45.231052Z",
     "shell.execute_reply": "2025-03-11T20:27:45.230191Z",
     "shell.execute_reply.started": "2025-03-11T20:27:45.221841Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RetrievalModule:\n",
    "    def __init__(self, embedding_model_name=MODEL_NAME):\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading tokenizer: {e}\")\n",
    "            self.tokenizer = None\n",
    "        self.exemplar_bank = []  # Will contain (question, embedding, cot_sequence) tuples\n",
    "        \n",
    "    def add_exemplar(self, question, cot_sequence):\n",
    "        embedding = self.compute_embedding(question)\n",
    "        self.exemplar_bank.append((question, embedding, cot_sequence))\n",
    "    \n",
    "    def initialize_from_dataset(self, dataset, max_exemplars=200):  # Increased for better retrieval\n",
    "        \"\"\"Initialize the retrieval module with examples from a dataset\"\"\"\n",
    "        for i, item in enumerate(tqdm(dataset, desc=\"Building exemplar bank\")):\n",
    "            if i >= max_exemplars:\n",
    "                break\n",
    "            try:\n",
    "                self.add_exemplar(item[\"raw_question\"], item[\"raw_cot\"])\n",
    "            except Exception as e:\n",
    "                print(f\"Error adding exemplar {i}: {e}\")\n",
    "    \n",
    "    def compute_embedding(self, text):\n",
    "        # More sophisticated embedding approach\n",
    "        try:\n",
    "            if self.tokenizer:\n",
    "                # Tokenize and encode the text\n",
    "                tokens = self.tokenizer(\n",
    "                    text, \n",
    "                    padding=True, \n",
    "                    truncation=True, \n",
    "                    max_length=MAX_LENGTH, \n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                # Generate a deterministic but improved embedding based on token IDs\n",
    "                token_ids = tokens[\"input_ids\"].numpy().flatten()\n",
    "                \n",
    "                # Use a more nuanced approach for embeddings\n",
    "                embedding = np.zeros(768)  # Increased dimension\n",
    "                \n",
    "                # Apply positional weighting (tokens at beginning and end often carry more meaning)\n",
    "                for i, token_id in enumerate(token_ids):\n",
    "                    if i < 768:\n",
    "                        # Apply higher weights to beginning and end tokens\n",
    "                        position_weight = 1.0\n",
    "                        if i < len(token_ids) * 0.2 or i > len(token_ids) * 0.8:\n",
    "                            position_weight = 1.5\n",
    "                        embedding[i % 768] += token_id * position_weight\n",
    "                \n",
    "                # Normalize\n",
    "                norm = np.linalg.norm(embedding)\n",
    "                if norm > 0:\n",
    "                    embedding = embedding / norm\n",
    "                \n",
    "                return embedding\n",
    "            else:\n",
    "                return np.random.randn(768)  # Fallback\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing embedding: {e}\")\n",
    "            return np.random.randn(768)\n",
    "    \n",
    "    def retrieve_similar_exemplars(self, question, k=5):  # Increased k for better diversity\n",
    "        query_embedding = self.compute_embedding(question)\n",
    "        \n",
    "        # Compute similarities\n",
    "        similarities = []\n",
    "        for _, exemplar_embedding, _ in self.exemplar_bank:\n",
    "            try:\n",
    "                sim = cosine_similarity([query_embedding], [exemplar_embedding])[0][0]\n",
    "                similarities.append(sim)\n",
    "            except Exception as e:\n",
    "                print(f\"Error computing similarity: {e}\")\n",
    "                similarities.append(0.0)\n",
    "        \n",
    "        # Get top-k indices\n",
    "        if not similarities:\n",
    "            return []\n",
    "        \n",
    "        top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "        \n",
    "        # Return the corresponding exemplars\n",
    "        return [self.exemplar_bank[i][2] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T20:27:45.412046Z",
     "iopub.status.busy": "2025-03-11T20:27:45.411761Z",
     "iopub.status.idle": "2025-03-11T20:27:45.421172Z",
     "shell.execute_reply": "2025-03-11T20:27:45.420285Z",
     "shell.execute_reply.started": "2025-03-11T20:27:45.412025Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TextRefinementTransformer(nn.Module):\n",
    "    def __init__(self, model_name=MODEL_NAME):\n",
    "        super(TextRefinementTransformer, self).__init__()\n",
    "        try:\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "            self.model = to_device(self.model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing refinement module: {e}\")\n",
    "            self.tokenizer = None\n",
    "            self.model = None\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        if self.model is None:\n",
    "            return None\n",
    "        try:\n",
    "            return self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error in forward pass: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def refine_text(self, input_text, max_length=MAX_LENGTH):\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            return input_text  # Fallback: return original text\n",
    "            \n",
    "        try:\n",
    "            # Add explicit instruction to improve quality\n",
    "            enhanced_input = f\"Improve this math solution with detailed step-by-step reasoning: {input_text}\"\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                enhanced_input, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True,\n",
    "                max_length=max_length // 2\n",
    "            )\n",
    "            inputs = {k: to_device(v) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():  # No grad for inference\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    max_length=max_length,\n",
    "                    num_return_sequences=1,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    min_length=100,  # Encourage detailed responses\n",
    "                    num_beams=4\n",
    "                )\n",
    "            \n",
    "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error refining text: {e}\")\n",
    "            return input_text  # Fallback\n",
    "    \n",
    "    def train_step(self, batch, optimizer):\n",
    "        if self.model is None:\n",
    "            return 0.0  # Fallback\n",
    "            \n",
    "        try:\n",
    "            self.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Move batch to device\n",
    "            device_batch = {k: to_device(v) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "            \n",
    "            outputs = self.forward(\n",
    "                input_ids=device_batch[\"input_ids\"],\n",
    "                attention_mask=device_batch[\"attention_mask\"],\n",
    "                labels=device_batch[\"labels\"]\n",
    "            )\n",
    "            \n",
    "            if outputs is None:\n",
    "                return 0.0\n",
    "                \n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            return loss.item()\n",
    "        except Exception as e:\n",
    "            print(f\"Error in train_step: {e}\")\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T20:27:45.562421Z",
     "iopub.status.busy": "2025-03-11T20:27:45.562151Z",
     "iopub.status.idle": "2025-03-11T20:27:45.573656Z",
     "shell.execute_reply": "2025-03-11T20:27:45.572739Z",
     "shell.execute_reply.started": "2025-03-11T20:27:45.562401Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RewardFunction:\n",
    "    def __init__(self, reflection_module):\n",
    "        self.reflection_module = reflection_module\n",
    "        \n",
    "    def outcome_reward(self, predicted_answer, true_answer):\n",
    "        \"\"\"Improved reward based on partial matching and numeric comparison\"\"\"\n",
    "        try:\n",
    "            # Clean up answers - remove $, commas, and whitespace\n",
    "            pred_clean = re.sub(r'[$,\\s]', '', predicted_answer).strip()\n",
    "            true_clean = re.sub(r'[$,\\s]', '', true_answer).strip()\n",
    "            \n",
    "            # Exact match\n",
    "            if pred_clean == true_clean:\n",
    "                return 1.0\n",
    "                \n",
    "            # Both are numbers - check if they're close\n",
    "            if pred_clean.replace('.', '', 1).isdigit() and true_clean.replace('.', '', 1).isdigit():\n",
    "                try:\n",
    "                    pred_num = float(pred_clean)\n",
    "                    true_num = float(true_clean)\n",
    "                    \n",
    "                    # If they're within 1% of each other\n",
    "                    if abs(pred_num - true_num) / max(abs(true_num), 1) < 0.01:\n",
    "                        return 0.9\n",
    "                        \n",
    "                    # If they're within 10% of each other\n",
    "                    if abs(pred_num - true_num) / max(abs(true_num), 1) < 0.1:\n",
    "                        return 0.5\n",
    "                    \n",
    "                    # Partial credit for getting the order of magnitude right\n",
    "                    if (true_num > 0 and pred_num > 0) or (true_num < 0 and pred_num < 0):\n",
    "                        if 0.1 <= (pred_num / true_num) <= 10:\n",
    "                            return 0.2\n",
    "                    \n",
    "                    # If they're way off\n",
    "                    return 0.0\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # No match\n",
    "            return 0.0\n",
    "        except Exception as e:\n",
    "            print(f\"Error in outcome_reward: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def process_reward(self, cot_steps, question):\n",
    "        \"\"\"Improved reward based on the quality of the reasoning process\"\"\"\n",
    "        if not cot_steps:\n",
    "            return 0.2  # Small baseline reward even with no steps\n",
    "        \n",
    "        try:\n",
    "            # Check for math expressions, operations, and equations\n",
    "            has_math_expressions = False\n",
    "            for step in cot_steps:\n",
    "                if re.search(r'[=+\\-*/]', step) and re.search(r'\\d', step):\n",
    "                    has_math_expressions = True\n",
    "                    break\n",
    "            \n",
    "            # Check for relevant keywords from the question in the steps\n",
    "            question_words = set(re.findall(r'\\b\\w{4,}\\b', question.lower()))\n",
    "            step_text = ' '.join(cot_steps).lower()\n",
    "            relevant_word_count = sum(1 for word in question_words if word in step_text)\n",
    "            relevance_score = min(relevant_word_count / max(len(question_words), 1), 1.0)\n",
    "            \n",
    "            # Evaluate each step and take the mean\n",
    "            step_scores = []\n",
    "            for step in cot_steps:\n",
    "                score = self.reflection_module.evaluate_step(step, question)\n",
    "                step_scores.append(score)\n",
    "            \n",
    "            # More generous length bonus\n",
    "            length_bonus = min(len(step_scores) / 3, 1.0)  # Bonus for 3+ steps, capped at 1.0\n",
    "            \n",
    "            # Bonus for using numbers in steps\n",
    "            has_numbers = any(bool(re.search(r'\\d+', step)) for step in cot_steps)\n",
    "            number_bonus = 0.2 if has_numbers else 0.0\n",
    "            \n",
    "            # Bonus for mathematical expressions\n",
    "            math_bonus = 0.3 if has_math_expressions else 0.0\n",
    "            \n",
    "            # Return the mean score with bonuses\n",
    "            base_score = sum(step_scores) / len(step_scores) if step_scores else 0.3\n",
    "            return min(base_score * (1.0 + 0.3 * length_bonus + number_bonus + math_bonus) + 0.2 * relevance_score, 1.0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in process_reward: {e}\")\n",
    "            return 0.3  # More generous fallback\n",
    "    \n",
    "    def combined_reward(self, cot_steps, predicted_answer, true_answer, question, alpha=0.7):\n",
    "        \"\"\"Combine process and outcome rewards with detailed logging\"\"\"\n",
    "        try:\n",
    "            outcome = self.outcome_reward(predicted_answer, true_answer)\n",
    "            process = self.process_reward(cot_steps, question)\n",
    "            \n",
    "            # Higher weight on outcome now\n",
    "            combined = alpha * outcome + (1 - alpha) * process\n",
    "            \n",
    "            # Debug info\n",
    "            if random.random() < 0.05:  # Only print for 5% of examples to avoid log flooding\n",
    "                print(f\"\\nQuestion: {question[:50]}...\")\n",
    "                print(f\"Predicted: {predicted_answer}\")\n",
    "                print(f\"True: {true_answer}\")\n",
    "                print(f\"Outcome reward: {outcome:.2f}, Process reward: {process:.2f}, Combined: {combined:.2f}\")\n",
    "                if cot_steps:\n",
    "                    print(f\"First reasoning step: {cot_steps[0][:50]}...\")\n",
    "            \n",
    "            return combined\n",
    "        except Exception as e:\n",
    "            print(f\"Error in combined_reward: {e}\")\n",
    "            return 0.3  # More generous fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T20:27:45.727432Z",
     "iopub.status.busy": "2025-03-11T20:27:45.727151Z",
     "iopub.status.idle": "2025-03-11T20:27:45.758326Z",
     "shell.execute_reply": "2025-03-11T20:27:45.757600Z",
     "shell.execute_reply.started": "2025-03-11T20:27:45.727411Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SelfTrainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        cot_generator,\n",
    "        reflection_module,\n",
    "        retrieval_module,\n",
    "        refinement_module,\n",
    "        reward_function,\n",
    "        tokenizer=None\n",
    "    ):\n",
    "        self.cot_generator = cot_generator\n",
    "        self.reflection_module = reflection_module\n",
    "        self.retrieval_module = retrieval_module\n",
    "        self.refinement_module = refinement_module\n",
    "        self.reward_function = reward_function\n",
    "        try:\n",
    "            self.tokenizer = tokenizer or AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading tokenizer: {e}\")\n",
    "            self.tokenizer = None\n",
    "        \n",
    "    def generate_pseudo_labels(self, dataset, threshold=0.3, max_samples=100):\n",
    "        \"\"\"Generate high-quality pseudo-labeled examples with better logging\"\"\"\n",
    "        pseudo_labeled = []\n",
    "        rewards = []\n",
    "        \n",
    "        # Sample more examples\n",
    "        sample_count = min(max_samples, len(dataset))\n",
    "        sample_indices = random.sample(range(len(dataset)), sample_count)\n",
    "        \n",
    "        for i in tqdm(sample_indices, desc=\"Generating pseudo-labels\"):\n",
    "            try:\n",
    "                item = dataset[i]\n",
    "                question = item[\"raw_question\"]\n",
    "                gold_answer = item[\"raw_answer\"]\n",
    "                \n",
    "                # Generate CoT with the current model using a stronger prompt\n",
    "                generated = self.cot_generator.generate(\n",
    "                    question,\n",
    "                    reflection_module=self.reflection_module  # Pass the reflection module\n",
    "                )\n",
    "\n",
    "                # Retrieve similar exemplars to guide generation\n",
    "                similar_exemplars = self.retrieval_module.retrieve_similar_exemplars(question, k=3)\n",
    "                exemplar_text = \"\"\n",
    "                if similar_exemplars:\n",
    "                    exemplar_text = \"Here are examples of good reasoning:\\n\"\n",
    "                    for i, exemplar in enumerate(similar_exemplars[:2]):  # Use top 2 exemplars\n",
    "                        steps_text = \"\\n\".join([f\"Step {j+1}: {step}\" for j, step in enumerate(exemplar)])\n",
    "                        exemplar_text += f\"Example {i+1}:\\n{steps_text}\\n\\n\"\n",
    "                \n",
    "                # Try to improve generation with exemplars if available\n",
    "                if exemplar_text:\n",
    "                    enhanced_prompt = f\"{exemplar_text}\\nNow solve this problem:\\n{question}\"\n",
    "                    enhanced_generation = self.cot_generator.generate(\n",
    "                        enhanced_prompt,\n",
    "                        reflection_module=self.reflection_module\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate rewards for both generations and use the better one\n",
    "                    base_reward = self.reward_function.combined_reward(\n",
    "                        generated[\"cot_steps\"], \n",
    "                        generated[\"final_answer\"], \n",
    "                        gold_answer, \n",
    "                        question\n",
    "                    )\n",
    "                    \n",
    "                    enhanced_reward = self.reward_function.combined_reward(\n",
    "                        enhanced_generation[\"cot_steps\"], \n",
    "                        enhanced_generation[\"final_answer\"], \n",
    "                        gold_answer, \n",
    "                        question\n",
    "                    )\n",
    "                    \n",
    "                    if enhanced_reward > base_reward:\n",
    "                        generated = enhanced_generation\n",
    "                        reward = enhanced_reward\n",
    "                    else:\n",
    "                        reward = base_reward\n",
    "                else:\n",
    "                    reward = self.reward_function.combined_reward(\n",
    "                        generated[\"cot_steps\"], \n",
    "                        generated[\"final_answer\"], \n",
    "                        gold_answer, \n",
    "                        question\n",
    "                    )\n",
    "                \n",
    "                # Try to refine the generated text if reward is mediocre\n",
    "                if 0.3 <= reward <= 0.7:\n",
    "                    refined_text = self.refinement_module.refine_text(generated[\"full_output\"])\n",
    "                    refined_steps = extract_cot_steps(refined_text)\n",
    "                    refined_answer = extract_final_answer(refined_text)\n",
    "                    \n",
    "                    refined_reward = self.reward_function.combined_reward(\n",
    "                        refined_steps, \n",
    "                        refined_answer, \n",
    "                        gold_answer, \n",
    "                        question\n",
    "                    )\n",
    "                    \n",
    "                    # Use the refined version if it's better\n",
    "                    if refined_reward > reward:\n",
    "                        generated[\"cot_steps\"] = refined_steps\n",
    "                        generated[\"final_answer\"] = refined_answer\n",
    "                        generated[\"full_output\"] = refined_text\n",
    "                        reward = refined_reward\n",
    "                \n",
    "                # Keep examples that exceed the quality threshold\n",
    "                if reward >= threshold:\n",
    "                    pseudo_labeled.append({\n",
    "                        \"question\": question,\n",
    "                        \"cot_steps\": generated[\"cot_steps\"],\n",
    "                        \"final_answer\": generated[\"final_answer\"],\n",
    "                        \"full_output\": generated[\"full_output\"],\n",
    "                        \"gold_answer\": gold_answer,\n",
    "                        \"reward\": reward\n",
    "                    })\n",
    "                    rewards.append(reward)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing item {i}: {e}\")\n",
    "        \n",
    "        # Log statistics\n",
    "        if rewards:\n",
    "            avg_reward = sum(rewards) / len(rewards)\n",
    "            print(f\"Generated {len(pseudo_labeled)} pseudo-labeled examples with average reward {avg_reward:.3f}\")\n",
    "        else:\n",
    "            print(\"Failed to generate any valid pseudo-labeled examples\")\n",
    "            \n",
    "        return pseudo_labeled\n",
    "    \n",
    "    def train_with_pseudo_labels(self, pseudo_labeled, learning_rate=1e-5, epochs=2, batch_size=4):\n",
    "        \"\"\"Train the model with pseudo-labeled examples\"\"\"\n",
    "        if not pseudo_labeled:\n",
    "            print(\"No pseudo-labeled examples available for training\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Create a simple dataset from pseudo-labeled examples\n",
    "            class PseudoLabeledDataset(Dataset):\n",
    "                def __init__(self, examples, tokenizer, max_length=MAX_LENGTH):\n",
    "                    self.examples = examples\n",
    "                    self.tokenizer = tokenizer\n",
    "                    self.max_length = max_length\n",
    "                \n",
    "                def __len__(self):\n",
    "                    return len(self.examples)\n",
    "                \n",
    "                def __getitem__(self, idx):\n",
    "                    item = self.examples[idx]\n",
    "                    \n",
    "                    input_text = f\"Solve this math problem step-by-step: {item['question']}\"\n",
    "                    target_text = item[\"full_output\"]\n",
    "                    \n",
    "                    inputs = self.tokenizer(\n",
    "                        input_text,\n",
    "                        padding=\"max_length\",\n",
    "                        truncation=True,\n",
    "                        max_length=self.max_length // 3,\n",
    "                        return_tensors=\"pt\"\n",
    "                    )\n",
    "                    \n",
    "                    targets = self.tokenizer(\n",
    "                        target_text,\n",
    "                        padding=\"max_length\",\n",
    "                        truncation=True,\n",
    "                        max_length=self.max_length * 2 // 3,\n",
    "                        return_tensors=\"pt\"\n",
    "                    )\n",
    "                    \n",
    "                    return {\n",
    "                        \"input_ids\": inputs.input_ids.squeeze(),\n",
    "                        \"attention_mask\": inputs.attention_mask.squeeze(),\n",
    "                        \"labels\": targets.input_ids.squeeze(),\n",
    "                    }\n",
    "            \n",
    "            # Initialize dataset and dataloader\n",
    "            pseudo_dataset = PseudoLabeledDataset(pseudo_labeled, self.tokenizer)\n",
    "            pseudo_dataloader = DataLoader(\n",
    "                pseudo_dataset, \n",
    "                batch_size=batch_size,\n",
    "                shuffle=True\n",
    "            )\n",
    "            \n",
    "            # Set up optimizer\n",
    "            optimizer = AdamW(self.cot_generator.model.parameters(), lr=learning_rate)\n",
    "            \n",
    "            # Training loop\n",
    "            total_steps = len(pseudo_dataloader) * epochs\n",
    "            print(f\"Starting training on {len(pseudo_labeled)} examples for {epochs} epochs ({total_steps} steps)\")\n",
    "            \n",
    "            device = self.cot_generator.model.device\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                epoch_loss = 0.0\n",
    "                \n",
    "                for step, batch in enumerate(tqdm(pseudo_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n",
    "                    # Move batch to device\n",
    "                    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    self.cot_generator.model.train()\n",
    "                    outputs = self.cot_generator.model(\n",
    "                        input_ids=batch[\"input_ids\"],\n",
    "                        attention_mask=batch[\"attention_mask\"],\n",
    "                        labels=batch[\"labels\"]\n",
    "                    )\n",
    "                    \n",
    "                    loss = outputs.loss\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    epoch_loss += loss.item()\n",
    "                    \n",
    "                    # Log progress\n",
    "                    if step % 10 == 0:\n",
    "                        print(f\"Step {step}/{len(pseudo_dataloader)}, Loss: {loss.item():.4f}\")\n",
    "                \n",
    "                # End of epoch stats\n",
    "                avg_epoch_loss = epoch_loss / len(pseudo_dataloader)\n",
    "                print(f\"Epoch {epoch+1}/{epochs} completed. Average loss: {avg_epoch_loss:.4f}\")\n",
    "                \n",
    "                # Save checkpoint\n",
    "                self.cot_generator.save(f\"./models/recot_checkpoint_epoch_{epoch+1}\")\n",
    "                \n",
    "            print(\"Training completed successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in train_with_pseudo_labels: {e}\")\n",
    "    \n",
    "    def run_ppo_update(self, batch, ppo_steps=3, clip_param=0.2, value_loss_coef=0.5):\n",
    "        \"\"\"Perform PPO update on the model using the given batch\"\"\"\n",
    "        try:\n",
    "            device = self.cot_generator.model.device\n",
    "            \n",
    "            # Prepare data structures\n",
    "            states = []  # Questions\n",
    "            actions = []  # Generated CoT steps\n",
    "            old_probs = []  # Log probs of the old policy\n",
    "            rewards = []  # Calculated rewards\n",
    "            \n",
    "            # Collect trajectories\n",
    "            for item in tqdm(batch, desc=\"Collecting trajectories\"):\n",
    "                question = item[\"question\"]\n",
    "                gold_answer = item[\"gold_answer\"]\n",
    "                \n",
    "                # Generate CoT with current policy and record log probs\n",
    "                with torch.no_grad():\n",
    "                    input_text = f\"Solve this math problem step-by-step: {question}\"\n",
    "                    inputs = self.tokenizer(\n",
    "                        input_text, \n",
    "                        return_tensors=\"pt\",\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        max_length=MAX_LENGTH // 3\n",
    "                    ).to(device)\n",
    "                    \n",
    "                    # Generate with model (record log probs)\n",
    "                    self.cot_generator.model.eval()\n",
    "                    outputs = self.cot_generator.model.generate(\n",
    "                        input_ids=inputs[\"input_ids\"],\n",
    "                        attention_mask=inputs[\"attention_mask\"],\n",
    "                        max_length=MAX_LENGTH,\n",
    "                        output_scores=True,\n",
    "                        return_dict_in_generate=True,\n",
    "                        do_sample=True,\n",
    "                        temperature=0.7\n",
    "                    )\n",
    "                    \n",
    "                    # Extract sequences and scores\n",
    "                    sequences = outputs.sequences\n",
    "                    generated_text = self.tokenizer.decode(sequences[0], skip_special_tokens=True)\n",
    "                    \n",
    "                    # Extract CoT steps and final answer\n",
    "                    cot_steps = extract_cot_steps(generated_text)\n",
    "                    final_answer = extract_final_answer(generated_text)\n",
    "                    \n",
    "                    # Calculate reward\n",
    "                    reward = self.reward_function.combined_reward(\n",
    "                        cot_steps, \n",
    "                        final_answer, \n",
    "                        gold_answer, \n",
    "                        question\n",
    "                    )\n",
    "                    \n",
    "                    # Store trajectory\n",
    "                    states.append(inputs)\n",
    "                    actions.append(sequences)\n",
    "                    old_probs.append(outputs.scores)\n",
    "                    rewards.append(reward)\n",
    "            \n",
    "            # PPO update loop\n",
    "            for _ in range(ppo_steps):\n",
    "                # Randomly sample mini-batches for update\n",
    "                indices = torch.randperm(len(states))\n",
    "                \n",
    "                for idx in indices:\n",
    "                    # Get trajectory\n",
    "                    state = states[idx]\n",
    "                    action = actions[idx]\n",
    "                    old_prob = old_probs[idx]\n",
    "                    reward = rewards[idx]\n",
    "                    \n",
    "                    # Forward pass with current policy\n",
    "                    self.cot_generator.model.train()\n",
    "                    current_outputs = self.cot_generator.model.forward(\n",
    "                        input_ids=state[\"input_ids\"],\n",
    "                        attention_mask=state[\"attention_mask\"],\n",
    "                        labels=action\n",
    "                    )\n",
    "                    \n",
    "                    # Extract policy loss (simplified PPO implementation)\n",
    "                    current_loss = current_outputs.loss\n",
    "                    \n",
    "                    # Calculate ratio and clipped surrogate objective\n",
    "                    ratio = torch.exp(current_loss - old_prob.mean())\n",
    "                    clipped_ratio = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param)\n",
    "                    \n",
    "                    # Calculate losses\n",
    "                    policy_loss = -torch.min(ratio * reward, clipped_ratio * reward).mean()\n",
    "                    \n",
    "                    # Simple value function (could be more sophisticated)\n",
    "                    value_loss = value_loss_coef * ((current_outputs.loss - reward) ** 2).mean()\n",
    "                    \n",
    "                    # Total loss\n",
    "                    total_loss = policy_loss + value_loss\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    self.cot_generator.model.zero_grad()\n",
    "                    total_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.cot_generator.model.parameters(), 1.0)\n",
    "                    \n",
    "                    # Optimizer step (using the generator's optimizer)\n",
    "                    optimizer = AdamW(self.cot_generator.model.parameters(), lr=1e-5)\n",
    "                    optimizer.step()\n",
    "            \n",
    "            print(\"PPO update completed\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in PPO update: {e}\")\n",
    "    \n",
    "    def evaluate(self, test_dataset, num_samples=50):\n",
    "        \"\"\"Evaluate the current model on a test dataset\"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        rewards = []\n",
    "        \n",
    "        # Sample a subset for efficient evaluation\n",
    "        sample_indices = random.sample(range(len(test_dataset)), min(num_samples, len(test_dataset)))\n",
    "        \n",
    "        for idx in tqdm(sample_indices, desc=\"Evaluating\"):\n",
    "            try:\n",
    "                item = test_dataset[idx]\n",
    "                question = item[\"raw_question\"]\n",
    "                gold_answer = item[\"raw_answer\"]\n",
    "                \n",
    "                # Generate answer\n",
    "                generation = self.cot_generator.generate(question)\n",
    "                predicted_answer = generation[\"final_answer\"]\n",
    "                \n",
    "                # Check if correct\n",
    "                reward = self.reward_function.outcome_reward(predicted_answer, gold_answer)\n",
    "                rewards.append(reward)\n",
    "                \n",
    "                # Binary correctness (1.0 means exactly correct)\n",
    "                if reward >= 0.9:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating example {idx}: {e}\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        avg_reward = sum(rewards) / len(rewards) if rewards else 0\n",
    "        \n",
    "        print(f\"Evaluation results:\")\n",
    "        print(f\"  Accuracy: {accuracy:.2f} ({correct}/{total})\")\n",
    "        print(f\"  Average reward: {avg_reward:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"avg_reward\": avg_reward,\n",
    "            \"correct\": correct,\n",
    "            \"total\": total\n",
    "        }\n",
    "    \n",
    "    def run_training_loop(self, train_dataset, test_dataset, num_iterations=5, pseudo_samples=100, batch_size=4):\n",
    "        \"\"\"Run the full training loop with pseudo-labeling and RL updates\"\"\"\n",
    "        try:\n",
    "            results_history = []\n",
    "            \n",
    "            # Initial evaluation\n",
    "            print(\"Initial model evaluation:\")\n",
    "            results = self.evaluate(test_dataset)\n",
    "            results_history.append(results)\n",
    "            \n",
    "            for iteration in range(num_iterations):\n",
    "                print(f\"\\n===== Iteration {iteration+1}/{num_iterations} =====\")\n",
    "                \n",
    "                # 1. Generate pseudo-labeled examples\n",
    "                print(f\"Generating {pseudo_samples} pseudo-labeled examples...\")\n",
    "                pseudo_labeled = self.generate_pseudo_labels(\n",
    "                    train_dataset, \n",
    "                    threshold=0.3,  # Lower threshold initially to get more examples \n",
    "                    max_samples=pseudo_samples\n",
    "                )\n",
    "                \n",
    "                if not pseudo_labeled:\n",
    "                    print(\"No pseudo-labeled examples generated. Skipping iteration.\")\n",
    "                    continue\n",
    "                \n",
    "                # 2. Train with pseudo-labeled examples\n",
    "                print(f\"Training with {len(pseudo_labeled)} pseudo-labeled examples...\")\n",
    "                self.train_with_pseudo_labels(\n",
    "                    pseudo_labeled,\n",
    "                    learning_rate=2e-5,\n",
    "                    epochs=1,\n",
    "                    batch_size=batch_size\n",
    "                )\n",
    "                \n",
    "                # 3. Perform PPO update\n",
    "                print(\"Performing PPO update...\")\n",
    "                # Filter high-quality examples for PPO\n",
    "                high_quality = [ex for ex in pseudo_labeled if ex[\"reward\"] >= 0.5]\n",
    "                if high_quality:\n",
    "                    self.run_ppo_update(high_quality)\n",
    "                \n",
    "                # 4. Update retrieval module with new exemplars\n",
    "                print(\"Updating retrieval module...\")\n",
    "                for example in pseudo_labeled:\n",
    "                    if example[\"reward\"] >= 0.7:  # Only add high-quality examples\n",
    "                        self.retrieval_module.add_exemplar(\n",
    "                            example[\"question\"],\n",
    "                            example[\"cot_steps\"]\n",
    "                        )\n",
    "                \n",
    "                # 5. Save checkpoint\n",
    "                checkpoint_path = f\"./models/recot_checkpoint_iteration_{iteration+1}\"\n",
    "                print(f\"Saving checkpoint to {checkpoint_path}...\")\n",
    "                self.cot_generator.save(checkpoint_path)\n",
    "                \n",
    "                # 6. Evaluate progress\n",
    "                print(\"Evaluating current model:\")\n",
    "                results = self.evaluate(test_dataset)\n",
    "                results_history.append(results)\n",
    "                \n",
    "                # Print improvement\n",
    "                if len(results_history) > 1:\n",
    "                    prev = results_history[-2][\"accuracy\"]\n",
    "                    curr = results_history[-1][\"accuracy\"]\n",
    "                    diff = curr - prev\n",
    "                    print(f\"Accuracy change: {diff:+.2f} ({prev:.2f}  {curr:.2f})\")\n",
    "            \n",
    "            # Final evaluation\n",
    "            print(\"\\n===== Final Evaluation =====\")\n",
    "            final_results = self.evaluate(test_dataset, num_samples=100)  # More samples for final eval\n",
    "            \n",
    "            # Print training summary\n",
    "            print(\"\\n===== Training Summary =====\")\n",
    "            print(f\"Initial accuracy: {results_history[0]['accuracy']:.2f}\")\n",
    "            print(f\"Final accuracy: {final_results['accuracy']:.2f}\")\n",
    "            print(f\"Improvement: {final_results['accuracy'] - results_history[0]['accuracy']:+.2f}\")\n",
    "            \n",
    "            return final_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in training loop: {e}\")\n",
    "            return None\n",
    "        \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count the number of trainable parameters in the model\"\"\"\n",
    "        model = self.cot_generator.model\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T20:27:45.857330Z",
     "iopub.status.busy": "2025-03-11T20:27:45.857068Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA device: Tesla P100-PCIE-16GB\n",
      "CUDA memory available: 17.06 GB\n",
      "\n",
      "===== Initializing Components =====\n",
      "Initializing tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6303a7fcd2e34192a8597c0a938a29e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing data:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11897ffe766f4fbab0a1c4581b9aa943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing data:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400 training examples and 100 test examples\n",
      "Initializing CoT Generator...\n",
      "Loading model t5-base...\n",
      "Model not found locally. Downloading t5-base...\n",
      "Tokenizer downloaded and saved to ./models/t5_base_cache\n",
      "Model downloaded and moved to cuda\n",
      "Initializing Reflection Module...\n",
      "Initializing Retrieval Module...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6a3b13f0e5472c9f93118509b4a1d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building exemplar bank:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Refinement Module...\n",
      "Initializing Reward Function...\n",
      "Initializing Self-Trainer...\n",
      "Model has 222,903,552 trainable parameters\n",
      "\n",
      "===== Running Quick Test =====\n",
      "Test question: John has 5 apples. Mary gives him 3 more apples. How many apples does John have now?\n",
      "Generated CoT:\n",
      "  Step 1: John has 5 apples. Mary gives him 3 more apples. John has 5 apples. Mary gives him 3 more apples. How many apples does John have now? Solve this math problem step-by-step: John has 5 apples. Mary gives him 3 more apples.\n",
      "  Step 2: John has 5 apples. Mary gives him 3 more apples. How many apples does John have now? Steps so far: 1. John has 5 apples. Mary gives him 3 more apples. How many apples does John have now?\n",
      "  Step 3: Steps so far: 1. John has 5 apples. Mary gives him 3 more apples. 2. John has 5 apples. Mary gives him 3 more apples. How many apples does John have now? Steps so far: 1. John has 5 apples. Mary gives him 3 more apples. 2. John has 5 apples. Mary gives him 3 more apples. How many apples does John have now? Steps so far: 1. John has 5 apples. Mary gives him 3 more apples. How many apples does John have now? Steps so far: 1. John has\n",
      "  Step 4: 1. John has 5 apples. Mary gives him 3 more apples. 2. John has 5 apples. Mary gives him 3 more apples. How many apples does John have now? Steps so far: 1. John has The following step needs improvement: John has 5 apples. Mary gives him 3 more apples. 2. John has 5 apples. Mary gives him 3 more apples. How many apples does John have now? 3. Steps so far: 1. John has Improved step: 1. John has John has 5 apples. Mary gives him 3 more apples. How many apples\n",
      "  Step 5: Steps so far: 1. John has 5 apples. Mary gives him 3 more apples. 2. John has 5 apples. Mary gives him 3 more apples. How many apples does John have now? Steps so far: 1. John has 5 apples. Mary gives him 3 more apples. 2. John has 5 apples. Mary gives him 3 more apples. How many apples does John have now? Steps so far: 1. John has 5 apples. Mary gives him 3 more apples. How many apples does John have now? Steps so far: 1. John has\n",
      "  Step 6: John has 3. Mary gives him 3 more apples. How many apples now? How many apples is John have now? Steps so far: 1. John has 5. Mary gives him 3 more apples. 2. John has 5 apples. Mary gives him 3 more apples. How many apples does John have now? Steps so far: 1. John has The following step needs improvement: John has 5. Mary gives him 3 more apples. 2. John has 5 apples. Mary gives him 3 more apples. How many apples does John have now? 3. Steps so far: 1.\n",
      "  Step 7: Steps so far: 1. John has 5 apples. Mary gives him 3 more apples. 2. John has 5 apples. Mary gives him 3 more apples. How many apples does John have now? Steps so far: 1. John has 5 apples. Mary gives him 3 more apples. 2. John has 5 apples. Mary gives him 3 more apples. How many apples does John have now? Steps so far: 1. John has 5 apples. Mary gives him 3 more apples. How many apples does John have now? Steps so far: 1. John has\n",
      "  Step 8: Steps so far: 1. John has 5 apples. Mary gives him 3 more apples. 2. John has 5 apples. Mary gives him 3 more apples. How many apples does John have now? Steps so far: 1. John has 5 apples. Mary gives him 3 more apples. 2. John has 5 apples. Mary gives him 3 more apples. How many apples does John have now? Steps so far: 1. John has 5 apples. Mary gives him 3 more apples. How many apples does John have now? Steps so far: 1. John has\n",
      "  Step 9: The answer is Mary gives him 3 more apples. How many apples does John have now? Steps so far: 1. John has 5. Mary gives him 3 more apples. 2. John has 5 apples. Mary gives him 3 more apples. How many apples does John have now? Steps so far: 1. John has 5. Mary gives him 3 more apples. How many apples does John have now? Steps so far: 1. John has 6. John has 3. Mary gives him 3 more apples. How many apples is John have now? Steps so far: 1. John has 5.\n",
      "Final answer: 1.\n",
      "Testing evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a431756d03a4765bc3fbe215964f184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n",
      "  Accuracy: 0.00 (0/5)\n",
      "  Average reward: 0.040\n",
      "Test evaluation completed with accuracy: 0.00\n",
      "\n",
      "===== Starting Main Training Loop =====\n",
      "Initial model evaluation:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56567933bf8241a38a59213888567073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n",
      "  Accuracy: 0.04 (2/50)\n",
      "  Average reward: 0.082\n",
      "\n",
      "===== Iteration 1/3 =====\n",
      "Generating 50 pseudo-labeled examples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020d8d8f045e4d9da15511521ced2251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating pseudo-labels:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: There is very little car traffic on Happy Street. ...\n",
      "Predicted: .\n",
      "True: ,\n",
      "Outcome reward: 0.00, Process reward: 0.20, Combined: 0.06\n",
      "\n",
      "Question: In 6 months Bella and Bob will be celebrating thei...\n",
      "Predicted: 86\n",
      "True: 18\n",
      "Outcome reward: 0.20, Process reward: 0.20, Combined: 0.20\n",
      "\n",
      "Question: The girl scouts earned $30 total from selling deli...\n",
      "Predicted: 30\n",
      "True: 5\n",
      "Outcome reward: 0.20, Process reward: 0.20, Combined: 0.20\n",
      "\n",
      "Question: Adam earns $40 daily in his job. 10% of his money ...\n",
      "Predicted: .\n",
      "True: 10\n",
      "Outcome reward: 0.00, Process reward: 1.00, Combined: 0.30\n",
      "First reasoning step: Adam earns $40 daily in his job. 10% of his money ...\n",
      "\n",
      "Question: A gallon of whole milk that normally costs $3 is n...\n",
      "Predicted: 2\n",
      "True: 3\n",
      "Outcome reward: 0.20, Process reward: 1.00, Combined: 0.44\n",
      "First reasoning step: step-by-step: Improved step: step-by-step: Improve...\n",
      "\n",
      "Question: Jack is running a bake sale to help pay for his ba...\n",
      "Predicted: .\n",
      "True: 28\n",
      "Outcome reward: 0.00, Process reward: 0.87, Combined: 0.26\n",
      "First reasoning step: solve a math problem step-by-step with these examp...\n",
      "\n",
      "Question: A farmer has twice as many pigs as cows, and 4 mor...\n",
      "Predicted: 18\n",
      "True: 11\n",
      "Outcome reward: 0.20, Process reward: 1.00, Combined: 0.44\n",
      "First reasoning step: True...\n",
      "Generated 47 pseudo-labeled examples with average reward 0.462\n",
      "Training with 47 pseudo-labeled examples...\n",
      "Starting training on 47 examples for 1 epochs (12 steps)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a7588b69164d258ebb935b67d7f8bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/12, Loss: 15.1406\n",
      "Step 10/12, Loss: 2.4375\n",
      "Epoch 1/1 completed. Average loss: 3.5478\n",
      "Model saved to ./models/recot_checkpoint_epoch_1\n",
      "Training completed successfully!\n",
      "Performing PPO update...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d599b9e2874a37b2cce8699cb21deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting trajectories:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in PPO update: 'tuple' object has no attribute 'mean'\n",
      "Updating retrieval module...\n",
      "Saving checkpoint to ./models/recot_checkpoint_iteration_1...\n",
      "Model saved to ./models/recot_checkpoint_iteration_1\n",
      "Evaluating current model:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37f928420684a24ba67d31cf98a912e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n",
      "  Accuracy: 0.04 (2/50)\n",
      "  Average reward: 0.088\n",
      "Accuracy change: +0.00 (0.04  0.04)\n",
      "\n",
      "===== Iteration 2/3 =====\n",
      "Generating 50 pseudo-labeled examples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6585a4f57eff4d1fbc6b578d7f5fc4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating pseudo-labels:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Marcy is a makeup artist and has agreed to do some...\n",
      "Predicted: 1.\n",
      "True: 12\n",
      "Outcome reward: 0.00, Process reward: 0.89, Combined: 0.27\n",
      "First reasoning step: step-by-step: Step-by-step: Step-by-step: Step-by-...\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random.seed(11)\n",
    "np.random.seed(11)\n",
    "torch.manual_seed(11)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(11)\n",
    "\n",
    "# Print system information\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                        \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "\n",
    "print(\"\\n===== Initializing Components =====\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "print(\"Initializing tokenizer...\")\n",
    "try:\n",
    "    tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing tokenizer: {e}\")\n",
    "    tokenizer = None\n",
    "\n",
    "# Initialize data\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    train_dataset = GSM8KDataset(\n",
    "        split=\"train\", \n",
    "        tokenizer=tokenizer, \n",
    "        max_length=MAX_LENGTH, \n",
    "        max_samples=MAX_SAMPLES\n",
    "    )\n",
    "    test_dataset = GSM8KDataset(\n",
    "        split=\"test\", \n",
    "        tokenizer=tokenizer, \n",
    "        max_length=MAX_LENGTH, \n",
    "        max_samples=MAX_SAMPLES // 4\n",
    "    )\n",
    "    print(f\"Loaded {len(train_dataset)} training examples and {len(test_dataset)} test examples\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading datasets: {e}\")\n",
    "    raise e\n",
    "\n",
    "# Initialize CoT Generator (main model)\n",
    "print(\"Initializing CoT Generator...\")\n",
    "cot_generator = CoTGenerator(MODEL_NAME)\n",
    "\n",
    "# Initialize Reflection Module\n",
    "print(\"Initializing Reflection Module...\")\n",
    "reflection_module = ReflectionModule()\n",
    "reflection_module = to_device(reflection_module)\n",
    "\n",
    "# Initialize Retrieval Module\n",
    "print(\"Initializing Retrieval Module...\")\n",
    "retrieval_module = RetrievalModule()\n",
    "# Populate retrieval module with examples from the dataset\n",
    "retrieval_module.initialize_from_dataset(train_dataset, max_exemplars=100)\n",
    "\n",
    "# Initialize Refinement Module (transformer for text refinement)\n",
    "print(\"Initializing Refinement Module...\")\n",
    "refinement_module = TextRefinementTransformer()\n",
    "\n",
    "# Initialize Reward Function\n",
    "print(\"Initializing Reward Function...\")\n",
    "reward_function = RewardFunction(reflection_module)\n",
    "\n",
    "# Initialize Self-Trainer\n",
    "print(\"Initializing Self-Trainer...\")\n",
    "self_trainer = SelfTrainer(\n",
    "    cot_generator=cot_generator,\n",
    "    reflection_module=reflection_module,\n",
    "    retrieval_module=retrieval_module,\n",
    "    refinement_module=refinement_module,\n",
    "    reward_function=reward_function,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "num_params = self_trainer.count_parameters()\n",
    "print(f\"Model has {num_params:,} trainable parameters\")\n",
    "\n",
    "\n",
    "# Run a quick test to ensure everything is working\n",
    "print(\"\\n===== Running Quick Test =====\")\n",
    "try:\n",
    "    # Test CoT generation\n",
    "    test_question = \"John has 5 apples. Mary gives him 3 more apples. How many apples does John have now?\"\n",
    "    print(f\"Test question: {test_question}\")\n",
    "    \n",
    "    generation = cot_generator.generate(test_question, reflection_module=reflection_module)\n",
    "    print(f\"Generated CoT:\")\n",
    "    for i, step in enumerate(generation[\"cot_steps\"]):\n",
    "        print(f\"  Step {i+1}: {step}\")\n",
    "    print(f\"Final answer: {generation['final_answer']}\")\n",
    "    \n",
    "    # Test evaluation\n",
    "    print(\"Testing evaluation...\")\n",
    "    test_results = self_trainer.evaluate(test_dataset, num_samples=5)\n",
    "    print(f\"Test evaluation completed with accuracy: {test_results['accuracy']:.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in quick test: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Run the main training loop\n",
    "print(\"\\n===== Starting Main Training Loop =====\")\n",
    "try:\n",
    "    training_results = self_trainer.run_training_loop(\n",
    "        train_dataset=train_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        num_iterations=3,  # Reduced number of iterations for initial testing\n",
    "        pseudo_samples=50,  # Start with fewer samples\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    # Save final model\n",
    "    print(\"Saving final model...\")\n",
    "    cot_generator.save(\"./models/recot_final_model\")\n",
    "    \n",
    "    print(\"\\n===== Training Complete =====\")\n",
    "    if training_results:\n",
    "        print(f\"Final accuracy: {training_results['accuracy']:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in training loop: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Example of using the trained model\n",
    "print(\"\\n===== Example Usage =====\")\n",
    "try:\n",
    "    example_questions = [\n",
    "        \"Janet's ducks lay 16 eggs per day. She eats 3 for breakfast every morning and bakes muffins for her friends every day with 4 eggs per batch. She bakes 2 batches of muffins per day. How many eggs does Janet have left each day?\",\n",
    "        \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\",\n",
    "        \"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\"\n",
    "    ]\n",
    "    \n",
    "    for i, question in enumerate(example_questions):\n",
    "        print(f\"\\nExample {i+1}: {question}\")\n",
    "        result = cot_generator.generate(question, reflection_module=reflection_module)\n",
    "        print(\"Generated reasoning:\")\n",
    "        for j, step in enumerate(result[\"cot_steps\"]):\n",
    "            print(f\"  Step {j+1}: {step}\")\n",
    "        print(f\"Final answer: {result['final_answer']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in example usage: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30920,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
