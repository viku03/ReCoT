{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer\n",
    ")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from torch.optim import Adam\n",
    "import random\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=11):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple M1 MPS device\n"
     ]
    }
   ],
   "source": [
    "# Device configuration - M1 Mac specific\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple M1 MPS device\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 768\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 3\n",
    "COT_PROMPT = \"Let's solve this step-by-step. To find the answer, I'll break down the problem into smaller parts.\"\n",
    "MODEL_NAME = \"t5-base\"\n",
    "MAX_SAMPLES = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for extracting final answers and CoT steps\n",
    "def extract_final_answer(answer_text):\n",
    "    # Look for patterns like \"The answer is X\" or \"Therefore, the answer is X\"\n",
    "    patterns = [\n",
    "        r\"The answer is\\s*[-]?\\s*\\$?\\s*([\\d,\\.]+)\",\n",
    "        r\"Therefore,?\\s.*?[-]?\\s*\\$?\\s*([\\d,\\.]+)\",\n",
    "        r\"So,?\\s.*?[-]?\\s*\\$?\\s*([\\d,\\.]+)\",\n",
    "        r\"Thus,?\\s.*?[-]?\\s*\\$?\\s*([\\d,\\.]+)\",\n",
    "        r\"The final answer is\\s*[-]?\\s*\\$?\\s*([\\d,\\.]+)\",\n",
    "        # Add a pattern to catch just the last number in the text\n",
    "        r\".*?([\\d,\\.]+)$\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.search(pattern, answer_text, re.DOTALL | re.IGNORECASE)\n",
    "        if matches:\n",
    "            return matches.group(1).strip()\n",
    "    \n",
    "    # If no patterns match, extract the last number in the text\n",
    "    numbers = re.findall(r\"\\d+(?:,\\d+)*(?:\\.\\d+)?\", answer_text)\n",
    "    if numbers:\n",
    "        return numbers[-1].strip()\n",
    "    \n",
    "    # Last resort fallback\n",
    "    return answer_text.strip().split(\"\\n\")[-1]\n",
    "\n",
    "def extract_cot_steps(answer_text):\n",
    "    final_answer_patterns = [\n",
    "        r\"The answer is.*$\",\n",
    "        r\"Therefore, the answer is.*$\",\n",
    "        r\"Final answer:.*$\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in final_answer_patterns:\n",
    "        answer_text = re.sub(pattern, '', answer_text, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    \n",
    "    # Split and clean steps, focusing on lines with mathematical operations\n",
    "    steps = []\n",
    "    for line in answer_text.split('\\n'):\n",
    "        # Look for lines with mathematical operations or reasoning\n",
    "        if re.search(r'[+\\-*/=]|because|means|so|thus', line, re.IGNORECASE):\n",
    "            # Remove any inline calculation markers\n",
    "            clean_line = re.sub(r'<<.*?>>', '', line).strip()\n",
    "            if clean_line:\n",
    "                steps.append(clean_line)\n",
    "    \n",
    "    return steps\n",
    "\n",
    "def test_cot_steps():\n",
    "    test_cases = [\n",
    "        \"\"\"Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
    "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
    "The answer is 72.\"\"\",\n",
    "        \n",
    "        \"\"\"Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\n",
    "Working 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\n",
    "The answer is 10.\"\"\"\n",
    "    ]\n",
    "    \n",
    "    for case in test_cases:\n",
    "        print(\"Original:\")\n",
    "        print(case)\n",
    "        print(\"\\nExtracted Steps:\")\n",
    "        print(extract_cot_steps(case))\n",
    "        print(\"\\n---\\n\")\n",
    "\n",
    "# Safe device transfer function for M1 MPS\n",
    "def to_device(tensor_or_module):\n",
    "    \"\"\"Safely move tensors or modules to the selected device\"\"\"\n",
    "    if tensor_or_module is None:\n",
    "        return None\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                         \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    \n",
    "    try:\n",
    "        return tensor_or_module.to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not move to {device}: {e}\")\n",
    "        return tensor_or_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
      "The answer is 72.\n",
      "\n",
      "Extracted Steps:\n",
      "['Natalia sold 48/2 = 24 clips in May.', 'Natalia sold 48+24 = 72 clips altogether in April and May.']\n",
      "\n",
      "---\n",
      "\n",
      "Original:\n",
      "Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\n",
      "Working 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\n",
      "The answer is 10.\n",
      "\n",
      "Extracted Steps:\n",
      "['Weng earns 12/60 = $0.2 per minute.', 'Working 50 minutes, she earned 0.2 x 50 = $10.']\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_cot_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSM8KDataset(Dataset):\n",
    "    def __init__(self, split=\"train\", tokenizer=None, max_length=768, max_samples=None):\n",
    "        self.data = load_dataset(\"gsm8k\", \"main\")[split]\n",
    "        if max_samples:\n",
    "            self.data = self.data.select(range(min(max_samples, len(self.data))))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.processed_data = self.preprocess_data()\n",
    "        \n",
    "    def preprocess_data(self):\n",
    "        processed = []\n",
    "        for item in tqdm(self.data, desc=\"Preprocessing data\"):\n",
    "            question = item[\"question\"]\n",
    "            answer_with_cot = item[\"answer\"]\n",
    "            \n",
    "            # Extract the CoT steps and the final answer\n",
    "            final_answer = extract_final_answer(answer_with_cot)\n",
    "            cot_steps = extract_cot_steps(answer_with_cot)\n",
    "\n",
    "            print( cot_steps )\n",
    "            print( final_answer )\n",
    "            \n",
    "            # Format for T5 training - improved prompt to guide the model\n",
    "            formatted_question = f\"Solve this math problem step-by-step: {question} {COT_PROMPT}\"\n",
    "            \n",
    "            processed.append({\n",
    "                \"question\": question,\n",
    "                \"formatted_question\": formatted_question,\n",
    "                \"cot_steps\": cot_steps,\n",
    "                \"final_answer\": final_answer,\n",
    "                \"full_answer\": answer_with_cot\n",
    "            })\n",
    "        return processed\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.processed_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.processed_data[idx]\n",
    "        \n",
    "        if self.tokenizer:\n",
    "            # Prepare input with task-specific prefix for T5\n",
    "            input_text = item[\"formatted_question\"]\n",
    "            target_text = item[\"full_answer\"]\n",
    "            \n",
    "            # Improved tokenization with more balanced token allocation\n",
    "            try:\n",
    "                inputs = self.tokenizer(\n",
    "                    input_text,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=self.max_length // 3,  # Allow more space for output\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                targets = self.tokenizer(\n",
    "                    target_text,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=self.max_length * 2 // 3,  # Allow more space for reasoning\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                return {\n",
    "                    \"input_ids\": inputs.input_ids.squeeze(),\n",
    "                    \"attention_mask\": inputs.attention_mask.squeeze(),\n",
    "                    \"labels\": targets.input_ids.squeeze(),\n",
    "                    \"raw_question\": item[\"question\"],\n",
    "                    \"raw_cot\": item[\"cot_steps\"],\n",
    "                    \"raw_answer\": item[\"final_answer\"]\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"Error tokenizing item {idx}: {e}\")\n",
    "                # Return a simple fallback\n",
    "                dummy_tensor = torch.zeros(self.max_length, dtype=torch.long)\n",
    "                return {\n",
    "                    \"input_ids\": dummy_tensor,\n",
    "                    \"attention_mask\": dummy_tensor,\n",
    "                    \"labels\": dummy_tensor,\n",
    "                    \"raw_question\": item[\"question\"],\n",
    "                    \"raw_cot\": item[\"cot_steps\"],\n",
    "                    \"raw_answer\": item[\"final_answer\"]\n",
    "                }\n",
    "        else:\n",
    "            return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedCoTGenerator:\n",
    "    def __init__(self, \n",
    "                 model_name=\"t5-base\", \n",
    "                 local_dir=\"./models/t5_base_cache\",\n",
    "                 max_steps=8):\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.local_dir = local_dir\n",
    "        self.max_steps = max_steps\n",
    "        \n",
    "        os.makedirs(self.local_dir, exist_ok=True)\n",
    "        \n",
    "        self._load_model()\n",
    "        \n",
    "        self.memory_bank = []\n",
    "    \n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                                  \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "            \n",
    "            # Use T5 tokenizer for encoder-decoder models\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_name, \n",
    "                cache_dir=self.local_dir,\n",
    "                use_fast=True\n",
    "            )\n",
    "            \n",
    "            # For T5 models, use AutoModelForSeq2SeqLM instead of AutoModelForCausalLM\n",
    "            self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                cache_dir=self.local_dir,\n",
    "                torch_dtype=torch.float16 if device.type != \"cpu\" else torch.float32,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            \n",
    "            self.model = self.model.to(device)\n",
    "            print(f\"Model loaded and moved to {device}\")\n",
    "            \n",
    "            # Check if we're using a T5 or Flan-T5 model which are good for math reasoning\n",
    "            if not any(t5_name in self.model_name.lower() for t5_name in [\"t5\", \"flan-t5\"]):\n",
    "                print(\"Warning: This generator is optimized for T5 or Flan-T5 models. Consider using t5-base, t5-large, flan-t5-base, or flan-t5-large for better mathematical reasoning.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _create_prompt(self, question, previous_steps=None):\n",
    "        # T5 models work best with task-specific prefixes\n",
    "        prompt = f\"Solve step by step: {question}\"\n",
    "        \n",
    "        # Add previous steps if available\n",
    "        if previous_steps and len(previous_steps) > 0:\n",
    "            steps_text = \" \".join([f\"Step {i+1}: {step}\" for i, step in enumerate(previous_steps)])\n",
    "            prompt = f\"{prompt} {steps_text} Next step:\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def _extract_next_step(self, generated_text, previous_steps):\n",
    "        # Clean up the generated text\n",
    "        clean_text = generated_text.strip()\n",
    "        \n",
    "        # Check if this looks like a final answer\n",
    "        is_final_step = any(phrase in clean_text.lower() \n",
    "                           for phrase in ['answer is', 'final answer', 'thus', 'therefore', \n",
    "                                          'the answer', 'so the answer', 'so,', 'equals'])\n",
    "        \n",
    "        # If there's a numerical answer with an equals sign, it's likely a final step\n",
    "        if re.search(r'=\\s*-?\\d+(\\.\\d+)?', clean_text):\n",
    "            is_final_step = True\n",
    "            \n",
    "        return {\n",
    "            \"step\": clean_text,\n",
    "            \"is_final_step\": is_final_step\n",
    "        }\n",
    "    \n",
    "    def generate(self, question, reflection_module=None):\n",
    "        cot_steps = []\n",
    "        \n",
    "        for step_num in range(self.max_steps):\n",
    "            prompt = self._create_prompt(question, cot_steps)\n",
    "            \n",
    "            try:\n",
    "                # Encode the input for T5\n",
    "                inputs = self.tokenizer(\n",
    "                    prompt, \n",
    "                    return_tensors=\"pt\", \n",
    "                    padding=True, \n",
    "                    truncation=True,\n",
    "                    max_length=512\n",
    "                ).to(self.model.device)\n",
    "                \n",
    "                # Generate with T5\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model.generate(\n",
    "                        input_ids=inputs.input_ids,\n",
    "                        attention_mask=inputs.attention_mask,\n",
    "                        max_length=150,  # T5 typically needs shorter output lengths\n",
    "                        num_return_sequences=1,\n",
    "                        do_sample=True,\n",
    "                        temperature=0.7,\n",
    "                        top_p=0.9,\n",
    "                        top_k=50\n",
    "                    )\n",
    "                \n",
    "                # Decode the generated output\n",
    "                generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                \n",
    "                step_result = self._extract_next_step(generated_text, cot_steps)\n",
    "                current_step = step_result[\"step\"]\n",
    "                \n",
    "                # Apply reflection if available\n",
    "                if reflection_module:\n",
    "                    if not reflection_module.evaluate_step(current_step, question):\n",
    "                        current_step = reflection_module.refine_step(question, current_step, cot_steps)\n",
    "                \n",
    "                cot_steps.append(current_step)\n",
    "                \n",
    "                if step_result[\"is_final_step\"]:\n",
    "                    # Extract numerical answer if present\n",
    "                    answer_match = re.search(r'=\\s*(-?\\d+(?:\\.\\d+)?)', current_step)\n",
    "                    if answer_match:\n",
    "                        final_answer = answer_match.group(1)\n",
    "                    else:\n",
    "                        # Try to find the number at the end of the step\n",
    "                        number_match = re.search(r'(-?\\d+(?:\\.\\d+)?)\\s*$', current_step)\n",
    "                        final_answer = number_match.group(1) if number_match else current_step\n",
    "                    break\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error generating step {step_num + 1}: {e}\")\n",
    "                break\n",
    "        \n",
    "        full_output = \"\\n\".join(cot_steps)\n",
    "        \n",
    "        # Extract final answer if not already done\n",
    "        if 'final_answer' not in locals():\n",
    "            final_answer = extract_final_answer(full_output)\n",
    "        \n",
    "        return {\n",
    "            \"cot_steps\": cot_steps,\n",
    "            \"final_answer\": final_answer,\n",
    "            \"full_output\": full_output\n",
    "        }\n",
    "    \n",
    "    def add_to_memory_bank(self, cot_sequence):\n",
    "        self.memory_bank.append(cot_sequence)\n",
    "    \n",
    "    def retrieve_similar_cot(self, question, top_k=3):\n",
    "        # Need to write mebeddings model here.\n",
    "        # For now, returning the most recent examples\n",
    "        return self.memory_bank[-top_k:] if len(self.memory_bank) >= top_k else self.memory_bank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cot_generator():\n",
    "        try:\n",
    "            # Sample math problems to test\n",
    "            test_problems = [\n",
    "                \"Natalia sold clips in May. If she sold 48/2 clips in May, how many clips did she sell?\",\n",
    "                \"Weng earns money per minute. If she earns 12/60 dollars per minute and works 50 minutes, how much does she earn?\",\n",
    "                \"Betty wants to save $100. Her grandparents gave her $30. She already has $50. How much more does she need to save?\",\n",
    "                \"Maila wants to read a 120-page book. She read 12 pages today and will read half the remaining pages tomorrow. How many pages will she read tomorrow?\"\n",
    "            ]\n",
    "            \n",
    "            # Initialize the CoT Generator\n",
    "            generator = EnhancedCoTGenerator(\n",
    "                model_name=\"t5-base\",\n",
    "                local_dir=\"./models/t5-base-cache\"\n",
    "            )\n",
    "            \n",
    "            # Test each problem\n",
    "            for i, problem in enumerate(test_problems, 1):\n",
    "                print(f\"\\n{'='*50}\")\n",
    "                print(f\"Problem {i}: {problem}\")\n",
    "                print(f\"{'='*50}\")\n",
    "                \n",
    "                # Generate Chain of Thought\n",
    "                result = generator.generate(problem)\n",
    "                \n",
    "                # Print results\n",
    "                print(\"\\nChain of Thought Steps:\")\n",
    "                for j, step in enumerate(result['cot_steps'], 1):\n",
    "                    print(f\"{j}. {step}\")\n",
    "                \n",
    "                print(f\"\\nFinal Answer: {result['final_answer']}\")\n",
    "                print(f\"{'='*50}\\n\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"Error running test:\")\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model: Unrecognized configuration class <class 'transformers.models.t5.configuration_t5.T5Config'> for this kind of AutoModel: AutoModelForCausalLM.\n",
      "Model type should be one of AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, DiffLlamaConfig, ElectraConfig, Emu3Config, ErnieConfig, FalconConfig, FalconMambaConfig, FuyuConfig, GemmaConfig, Gemma2Config, GitConfig, GlmConfig, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeSharedConfig, HeliumConfig, JambaConfig, JetMoeConfig, LlamaConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MllamaConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, ZambaConfig, Zamba2Config.\n",
      "Error running test:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/_2/xx5z8xdj6j98wh4vt2b59jz80000gp/T/ipykernel_19783/2416326621.py\", line 12, in test_cot_generator\n",
      "    generator = EnhancedCoTGenerator(\n",
      "  File \"/var/folders/_2/xx5z8xdj6j98wh4vt2b59jz80000gp/T/ipykernel_19783/1004135450.py\", line 13, in __init__\n",
      "    self._load_model()\n",
      "  File \"/var/folders/_2/xx5z8xdj6j98wh4vt2b59jz80000gp/T/ipykernel_19783/1004135450.py\", line 31, in _load_model\n",
      "    self.model = AutoModelForCausalLM.from_pretrained(\n",
      "  File \"/Users/Viku/GitHub/ReCoT/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 567, in from_pretrained\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized configuration class <class 'transformers.models.t5.configuration_t5.T5Config'> for this kind of AutoModel: AutoModelForCausalLM.\n",
      "Model type should be one of AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, DiffLlamaConfig, ElectraConfig, Emu3Config, ErnieConfig, FalconConfig, FalconMambaConfig, FuyuConfig, GemmaConfig, Gemma2Config, GitConfig, GlmConfig, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeSharedConfig, HeliumConfig, JambaConfig, JetMoeConfig, LlamaConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MllamaConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, ZambaConfig, Zamba2Config.\n"
     ]
    }
   ],
   "source": [
    "test_cot_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "test_cot_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30920,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
