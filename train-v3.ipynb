{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer\n",
    ")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from torch.optim import Adam\n",
    "import random\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=11):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple M1 MPS device\n"
     ]
    }
   ],
   "source": [
    "# Device configuration - M1 Mac specific\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple M1 MPS device\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 768\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 3\n",
    "COT_PROMPT = \"Let's solve this step-by-step. To find the answer, I'll break down the problem into smaller parts.\"\n",
    "MODEL_NAME = \"t5-base\"\n",
    "MAX_SAMPLES = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for extracting final answers and CoT steps\n",
    "def extract_final_answer(answer_text):\n",
    "    # Look for patterns like \"The answer is X\" or \"Therefore, the answer is X\"\n",
    "    patterns = [\n",
    "        r\"The answer is\\s*[-]?\\s*\\$?\\s*([\\d,\\.]+)\",\n",
    "        r\"Therefore,?\\s.*?[-]?\\s*\\$?\\s*([\\d,\\.]+)\",\n",
    "        r\"So,?\\s.*?[-]?\\s*\\$?\\s*([\\d,\\.]+)\",\n",
    "        r\"Thus,?\\s.*?[-]?\\s*\\$?\\s*([\\d,\\.]+)\",\n",
    "        r\"The final answer is\\s*[-]?\\s*\\$?\\s*([\\d,\\.]+)\",\n",
    "        # Add a pattern to catch just the last number in the text\n",
    "        r\".*?([\\d,\\.]+)$\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.search(pattern, answer_text, re.DOTALL | re.IGNORECASE)\n",
    "        if matches:\n",
    "            return matches.group(1).strip()\n",
    "    \n",
    "    # If no patterns match, extract the last number in the text\n",
    "    numbers = re.findall(r\"\\d+(?:,\\d+)*(?:\\.\\d+)?\", answer_text)\n",
    "    if numbers:\n",
    "        return numbers[-1].strip()\n",
    "    \n",
    "    # Last resort fallback\n",
    "    return answer_text.strip().split(\"\\n\")[-1]\n",
    "\n",
    "def extract_cot_steps(answer_text):\n",
    "    final_answer_patterns = [\n",
    "        r\"The answer is.*$\",\n",
    "        r\"Therefore, the answer is.*$\",\n",
    "        r\"Final answer:.*$\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in final_answer_patterns:\n",
    "        answer_text = re.sub(pattern, '', answer_text, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    \n",
    "    # Split and clean steps, focusing on lines with mathematical operations\n",
    "    steps = []\n",
    "    for line in answer_text.split('\\n'):\n",
    "        # Look for lines with mathematical operations or reasoning\n",
    "        if re.search(r'[+\\-*/=]|because|means|so|thus', line, re.IGNORECASE):\n",
    "            # Remove any inline calculation markers\n",
    "            clean_line = re.sub(r'<<.*?>>', '', line).strip()\n",
    "            if clean_line:\n",
    "                steps.append(clean_line)\n",
    "    \n",
    "    return steps\n",
    "\n",
    "def test_cot_steps():\n",
    "    test_cases = [\n",
    "        \"\"\"Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
    "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
    "The answer is 72.\"\"\",\n",
    "        \n",
    "        \"\"\"Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\n",
    "Working 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\n",
    "The answer is 10.\"\"\"\n",
    "    ]\n",
    "    \n",
    "    for case in test_cases:\n",
    "        print(\"Original:\")\n",
    "        print(case)\n",
    "        print(\"\\nExtracted Steps:\")\n",
    "        print(extract_cot_steps(case))\n",
    "        print(\"\\n---\\n\")\n",
    "\n",
    "# Safe device transfer function for M1 MPS\n",
    "def to_device(tensor_or_module):\n",
    "    \"\"\"Safely move tensors or modules to the selected device\"\"\"\n",
    "    if tensor_or_module is None:\n",
    "        return None\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                         \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    \n",
    "    try:\n",
    "        return tensor_or_module.to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not move to {device}: {e}\")\n",
    "        return tensor_or_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
      "The answer is 72.\n",
      "\n",
      "Extracted Steps:\n",
      "['Natalia sold 48/2 = 24 clips in May.', 'Natalia sold 48+24 = 72 clips altogether in April and May.']\n",
      "\n",
      "---\n",
      "\n",
      "Original:\n",
      "Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\n",
      "Working 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\n",
      "The answer is 10.\n",
      "\n",
      "Extracted Steps:\n",
      "['Weng earns 12/60 = $0.2 per minute.', 'Working 50 minutes, she earned 0.2 x 50 = $10.']\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_cot_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSM8KDataset(Dataset):\n",
    "    def __init__(self, split=\"train\", tokenizer=None, max_length=768, max_samples=None):\n",
    "        self.data = load_dataset(\"gsm8k\", \"main\")[split]\n",
    "        if max_samples:\n",
    "            self.data = self.data.select(range(min(max_samples, len(self.data))))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.processed_data = self.preprocess_data()\n",
    "        \n",
    "    def preprocess_data(self):\n",
    "        processed = []\n",
    "        for item in tqdm(self.data, desc=\"Preprocessing data\"):\n",
    "            question = item[\"question\"]\n",
    "            answer_with_cot = item[\"answer\"]\n",
    "            \n",
    "            # Extract the CoT steps and the final answer\n",
    "            final_answer = extract_final_answer(answer_with_cot)\n",
    "            cot_steps = extract_cot_steps(answer_with_cot)\n",
    "\n",
    "            print( cot_steps )\n",
    "            print( final_answer )\n",
    "            \n",
    "            # Format for T5 training - improved prompt to guide the model\n",
    "            formatted_question = f\"Solve this math problem step-by-step: {question} {COT_PROMPT}\"\n",
    "            \n",
    "            processed.append({\n",
    "                \"question\": question,\n",
    "                \"formatted_question\": formatted_question,\n",
    "                \"cot_steps\": cot_steps,\n",
    "                \"final_answer\": final_answer,\n",
    "                \"full_answer\": answer_with_cot\n",
    "            })\n",
    "        return processed\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.processed_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.processed_data[idx]\n",
    "        \n",
    "        if self.tokenizer:\n",
    "            # Prepare input with task-specific prefix for T5\n",
    "            input_text = item[\"formatted_question\"]\n",
    "            target_text = item[\"full_answer\"]\n",
    "            \n",
    "            # Improved tokenization with more balanced token allocation\n",
    "            try:\n",
    "                inputs = self.tokenizer(\n",
    "                    input_text,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=self.max_length // 3,  # Allow more space for output\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                targets = self.tokenizer(\n",
    "                    target_text,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=self.max_length * 2 // 3,  # Allow more space for reasoning\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                return {\n",
    "                    \"input_ids\": inputs.input_ids.squeeze(),\n",
    "                    \"attention_mask\": inputs.attention_mask.squeeze(),\n",
    "                    \"labels\": targets.input_ids.squeeze(),\n",
    "                    \"raw_question\": item[\"question\"],\n",
    "                    \"raw_cot\": item[\"cot_steps\"],\n",
    "                    \"raw_answer\": item[\"final_answer\"]\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"Error tokenizing item {idx}: {e}\")\n",
    "                # Return a simple fallback\n",
    "                dummy_tensor = torch.zeros(self.max_length, dtype=torch.long)\n",
    "                return {\n",
    "                    \"input_ids\": dummy_tensor,\n",
    "                    \"attention_mask\": dummy_tensor,\n",
    "                    \"labels\": dummy_tensor,\n",
    "                    \"raw_question\": item[\"question\"],\n",
    "                    \"raw_cot\": item[\"cot_steps\"],\n",
    "                    \"raw_answer\": item[\"final_answer\"]\n",
    "                }\n",
    "        else:\n",
    "            return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedCoTGenerator:\n",
    "    def __init__(self, \n",
    "                 model_name=\"t5-base\", \n",
    "                 local_dir=\"./models/t5_base_cache\",\n",
    "                 max_steps=8):\n",
    "        \n",
    "        self.max_steps = max_steps\n",
    "        self.model_name = model_name\n",
    "        self.local_dir = local_dir\n",
    "        \n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(self.local_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"Loading model {model_name}...\")\n",
    "        \n",
    "        # Check if model is already saved locally\n",
    "        if os.path.exists(os.path.join(self.local_dir, \"pytorch_model.bin\")) and \\\n",
    "           os.path.exists(os.path.join(self.local_dir, \"tokenizer_config.json\")):\n",
    "            print(f\"Found existing model at {self.local_dir}. Loading locally...\")\n",
    "            self._load_local_model()\n",
    "        else:\n",
    "            print(f\"Model not found locally. Downloading {model_name}...\")\n",
    "            self._download_model()\n",
    "    \n",
    "    def _download_model(self):\n",
    "        try:\n",
    "            # Download tokenizer and save it immediately\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(\n",
    "                self.model_name,\n",
    "                cache_dir=self.local_dir,\n",
    "                use_fast=True\n",
    "            )\n",
    "            print(f\"Tokenizer downloaded and saved to {self.local_dir}\")\n",
    "            \n",
    "            # Download model and save it immediately\n",
    "            try:\n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                                    \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "                self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    cache_dir=self.local_dir,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                    torch_dtype=torch.float16 if device.type != \"cpu\" else torch.float32\n",
    "                )\n",
    "                self.model = self.model.to(device)\n",
    "                print(f\"Model downloaded and moved to {device}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model to device: {e}\")\n",
    "                print(\"Falling back to CPU\")\n",
    "                self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    cache_dir=self.local_dir\n",
    "                )\n",
    "                print(f\"Model downloaded (CPU version)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading model: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def _load_local_model(self):\n",
    "        try:\n",
    "            # Load locally saved tokenizer\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(self.local_dir)\n",
    "            \n",
    "            # Load locally saved model\n",
    "            try:\n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                                    \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "                self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "                    self.local_dir,\n",
    "                    torch_dtype=torch.float16 if device.type != \"cpu\" else torch.float32\n",
    "                )\n",
    "                self.model = self.model.to(device)\n",
    "                print(f\"Model loaded from {self.local_dir} and moved to {device}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model to device: {e}\")\n",
    "                print(\"Falling back to CPU\")\n",
    "                self.model = T5ForConditionalGeneration.from_pretrained(self.local_dir)\n",
    "                print(f\"Model loaded from {self.local_dir} (CPU version)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading local model: {e}\")\n",
    "            print(\"Will attempt to download from source...\")\n",
    "            self._download_model()\n",
    "    \n",
    "    def _create_prompt(self, question, previous_steps=None):\n",
    "        # T5 models work best with task-specific prefixes\n",
    "        prompt = f\"Solve step by step: {question}\"\n",
    "        \n",
    "        # Add previous steps if available\n",
    "        if previous_steps and len(previous_steps) > 0:\n",
    "            steps_text = \" \".join([f\"Step {i+1}: {step}\" for i, step in enumerate(previous_steps)])\n",
    "            prompt = f\"{prompt} {steps_text} Next step:\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def _extract_next_step(self, generated_text, previous_steps):\n",
    "        # Clean up the generated text\n",
    "        clean_text = generated_text.strip()\n",
    "        \n",
    "        # Check if this looks like a final answer\n",
    "        is_final_step = any(phrase in clean_text.lower() \n",
    "                           for phrase in ['answer is', 'final answer', 'thus', 'therefore', \n",
    "                                          'the answer', 'so the answer', 'so,', 'equals'])\n",
    "        \n",
    "        # If there's a numerical answer with an equals sign, it's likely a final step\n",
    "        if re.search(r'=\\s*-?\\d+(\\.\\d+)?', clean_text):\n",
    "            is_final_step = True\n",
    "            \n",
    "        return {\n",
    "            \"step\": clean_text,\n",
    "            \"is_final_step\": is_final_step\n",
    "        }\n",
    "    \n",
    "    def generate(self, question, reflection_module=None):\n",
    "        cot_steps = []\n",
    "        \n",
    "        for step_num in range(self.max_steps):\n",
    "            prompt = self._create_prompt(question, cot_steps)\n",
    "            \n",
    "            try:\n",
    "                # Encode the input for T5\n",
    "                inputs = self.tokenizer(\n",
    "                    prompt, \n",
    "                    return_tensors=\"pt\", \n",
    "                    padding=True, \n",
    "                    truncation=True,\n",
    "                    max_length=512\n",
    "                ).to(self.model.device)\n",
    "                \n",
    "                # Generate with T5\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model.generate(\n",
    "                        input_ids=inputs.input_ids,\n",
    "                        attention_mask=inputs.attention_mask,\n",
    "                        max_length=150,  # T5 typically needs shorter output lengths\n",
    "                        num_return_sequences=1,\n",
    "                        do_sample=True,\n",
    "                        temperature=0.7,\n",
    "                        top_p=0.9,\n",
    "                        top_k=50\n",
    "                    )\n",
    "                \n",
    "                # Decode the generated output\n",
    "                generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                \n",
    "                step_result = self._extract_next_step(generated_text, cot_steps)\n",
    "                current_step = step_result[\"step\"]\n",
    "                \n",
    "                # Apply reflection if available\n",
    "                if reflection_module:\n",
    "                    if not reflection_module.evaluate_step(current_step, question):\n",
    "                        current_step = reflection_module.refine_step(question, current_step, cot_steps)\n",
    "                \n",
    "                cot_steps.append(current_step)\n",
    "                \n",
    "                if step_result[\"is_final_step\"]:\n",
    "                    # Extract numerical answer if present\n",
    "                    answer_match = re.search(r'=\\s*(-?\\d+(?:\\.\\d+)?)', current_step)\n",
    "                    if answer_match:\n",
    "                        final_answer = answer_match.group(1)\n",
    "                    else:\n",
    "                        # Try to find the number at the end of the step\n",
    "                        number_match = re.search(r'(-?\\d+(?:\\.\\d+)?)\\s*$', current_step)\n",
    "                        final_answer = number_match.group(1) if number_match else current_step\n",
    "                    break\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error generating step {step_num + 1}: {e}\")\n",
    "                break\n",
    "        \n",
    "        full_output = \"\\n\".join(cot_steps)\n",
    "        \n",
    "        # Extract final answer if not already done\n",
    "        if 'final_answer' not in locals():\n",
    "            final_answer = extract_final_answer(full_output)\n",
    "        \n",
    "        return {\n",
    "            \"cot_steps\": cot_steps,\n",
    "            \"final_answer\": final_answer,\n",
    "            \"full_output\": full_output\n",
    "        }\n",
    "    \n",
    "    def add_to_memory_bank(self, cot_sequence):\n",
    "        self.memory_bank.append(cot_sequence)\n",
    "    \n",
    "    def retrieve_similar_cot(self, question, top_k=3):\n",
    "        # Need to write mebeddings model here.\n",
    "        # For now, returning the most recent examples\n",
    "        return self.memory_bank[-top_k:] if len(self.memory_bank) >= top_k else self.memory_bank\n",
    "    \n",
    "    def save(self, path=None):\n",
    "        save_path = path if path else self.local_dir\n",
    "        try:\n",
    "            # Move to CPU before saving to avoid device-specific tensors in saved model\n",
    "            cpu_model = self.model.to(\"cpu\")\n",
    "            cpu_model.save_pretrained(save_path)\n",
    "            self.tokenizer.save_pretrained(save_path)\n",
    "            print(f\"Model saved to {save_path}\")\n",
    "            # Move back to device\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                               \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "            self.model = self.model.to(device)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {e}\")\n",
    "    \n",
    "    def load(self, path=None):\n",
    "        load_path = path if path else self.local_dir\n",
    "        try:\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(load_path)\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                               \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(load_path)\n",
    "            self.model = self.model.to(device)\n",
    "            print(f\"Model loaded from {load_path} and moved to {device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cot_generator():\n",
    "        try:\n",
    "            # Sample math problems to test\n",
    "            test_problems = [\n",
    "                \"Natalia sold clips in May. If she sold 48/2 clips in May, how many clips did she sell?\",\n",
    "                \"Weng earns money per minute. If she earns 12/60 dollars per minute and works 50 minutes, how much does she earn?\",\n",
    "                \"Betty wants to save $100. Her grandparents gave her $30. She already has $50. How much more does she need to save?\",\n",
    "                \"Maila wants to read a 120-page book. She read 12 pages today and will read half the remaining pages tomorrow. How many pages will she read tomorrow?\"\n",
    "            ]\n",
    "            \n",
    "            # Initialize the CoT Generator\n",
    "            generator = EnhancedCoTGenerator(\n",
    "                model_name=\"t5-base\",\n",
    "                local_dir=\"./models/t5_base_cache\"\n",
    "            )\n",
    "            \n",
    "            # Test each problem\n",
    "            for i, problem in enumerate(test_problems, 1):\n",
    "                print(f\"\\n{'='*50}\")\n",
    "                print(f\"Problem {i}: {problem}\")\n",
    "                print(f\"{'='*50}\")\n",
    "                \n",
    "                # Generate Chain of Thought\n",
    "                result = generator.generate(problem)\n",
    "                \n",
    "                # Print results\n",
    "                print(\"\\nChain of Thought Steps:\")\n",
    "                for j, step in enumerate(result['cot_steps'], 1):\n",
    "                    print(f\"{j}. {step}\")\n",
    "                \n",
    "                print(f\"\\nFinal Answer: {result['final_answer']}\")\n",
    "                print(f\"{'='*50}\\n\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"Error running test:\")\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model t5-base...\n",
      "Model not found locally. Downloading t5-base...\n",
      "Tokenizer downloaded and saved to ./models/t5_base_cache\n",
      "Model downloaded and moved to mps\n",
      "\n",
      "==================================================\n",
      "Problem 1: Natalia sold clips in May. If she sold 48/2 clips in May, how many clips did she sell?\n",
      "==================================================\n",
      "\n",
      "Chain of Thought Steps:\n",
      "1. Step by step: Natalia sold clips in May. How many clips did she sell?\n",
      "2. Step 1: Natalia sold clips in May. How many clips did she sell? Step 2: Natalia sold clips in May. If she sold 48/2 clips in May, how many clips did she sell? Step 2: Natalia sold clips in May. If she sold 48/2 clips in May, how many clips did she sell? Step 2: Step by step: Natalia sold clips in May. If she sold 48/2 clips in May, how many clips did she sell? Step 1: Natalia sold clips in May. If she sold\n",
      "3. in May. How many clips did she sell? Step 2: Step by step: Natalia sold clips in May. If she sold 48/2 clips in May, how many clips did she sell? Step 1: Step by step: Natalia sold clips in May. If she sold 48/2 clips in May, how many clips did she sell? Step 3: Step by step: Natalia sold 48/2 clips in May. If she sold 48/2 clips in May, how many clips did she sell? Step 4: Step by step: Natalia sold\n",
      "4. clips in May. How many clips did she sell? Step 1: Step by step: Natalia sold clips in May. How many clips did she sell? Step 2: Step by step: Natalia sold clips in May. If she sold 48/2 clips in May, how many clips did she sell? Step 3: in May. How many clips did she sell? Step 1: Step by step: Natalia sold clips in May. If she sold 48/2 clips in May, how many clips did she sell? Step 5: Step by step:\n",
      "5. in May. How many clips did she sell? Step 1: Step by step: Natalia sold clips in May. How many clips did she sell? Step 3: in May. How many clips did she sell? Step 4: Step by step: Natalia sold Step 4: clips in May. How many clips did she sell? Step 4: Step by step: Natalia sold Step 4: clips in May. How many clips did she sell? Step 5: Step by step: Natalia sold Step 4: clips in May. How many clips\n",
      "6. in May. How many clips did she sell? Step 2: Step by step: Natalia sold clips in May. How many clips did she sell? Step 3: in May. How many clips did she sell? Step 4: Step by step: Natalia sold Step 4: clips in May. How many clips did she sell? Step 5: Step by step: Step 5: Step 5: Step 6: in May. How many clips Did she sell? Step 8: Step by step: Natalia sold Step 7: in May. How many\n",
      "7. clips in May. How many clips did she sell? Step 1: Step by step: Natalia sold clips in May. How many clips did she sell? Step 2: Step by step: Natalia sold clips in May. If she sold 48/2 clips in May, how many clips did she sell? Step 3: in May. How many clips did she sell? Step 5: Step by step: Step 4: clips in May. How many clips did she sell? Step 7: Step by step: Natalia sold Step 6: clips in May\n",
      "8. in May. How many clips did she sell? Step 1: Step by step: Natalia sold clips in May. How many clips did she sell? Step 2: Step by step: Natalia sold clips in May. If she sold 48/2 clips in May, how many clips did she sell? Step 3: in May. How many clips did she sell? Step 5: Step by step: Step 4: clips in May. How many clips did she sell? Step 6: Step by step: Natalia sold Step 4: clips in May.\n",
      "\n",
      "Final Answer: .\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "Problem 2: Weng earns money per minute. If she earns 12/60 dollars per minute and works 50 minutes, how much does she earn?\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_cot_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[46], line 24\u001b[0m, in \u001b[0;36mtest_cot_generator\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Generate Chain of Thought\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mChain of Thought Steps:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[45], line 131\u001b[0m, in \u001b[0;36mEnhancedCoTGenerator.generate\u001b[0;34m(self, question, reflection_module)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Generate with T5\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 131\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# T5 typically needs shorter output lengths\u001b[39;49;00m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Decode the generated output\u001b[39;00m\n\u001b[1;32m    143\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2215\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2216\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2217\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2218\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2219\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2220\u001b[0m     )\n\u001b[1;32m   2222\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2223\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2236\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2237\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2242\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2243\u001b[0m     )\n",
      "File \u001b[0;32m~/GitHub/ReCoT/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:3271\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3268\u001b[0m     streamer\u001b[38;5;241m.\u001b[39mput(next_tokens\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m   3270\u001b[0m unfinished_sequences \u001b[38;5;241m=\u001b[39m unfinished_sequences \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m~\u001b[39mstopping_criteria(input_ids, scores)\n\u001b[0;32m-> 3271\u001b[0m this_peer_finished \u001b[38;5;241m=\u001b[39m \u001b[43munfinished_sequences\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3272\u001b[0m cur_len \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   3274\u001b[0m \u001b[38;5;66;03m# This is needed to properly delete outputs.logits which may be very large for first iteration\u001b[39;00m\n\u001b[1;32m   3275\u001b[0m \u001b[38;5;66;03m# Otherwise a reference to outputs is kept which keeps the logits alive in the next iteration\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_cot_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30920,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
