{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Viku/GitHub/ReCoT/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer\n",
    ")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from torch.optim import AdamW\n",
    "import random\n",
    "import traceback\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=11):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple M1 MPS device\n"
     ]
    }
   ],
   "source": [
    "# Device configuration - M1 Mac specific\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple M1 MPS device\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MAX_LENGTH = 768\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 3\n",
    "MODEL_NAME = \"t5-base\"\n",
    "MAX_SAMPLES = 400\n",
    "\n",
    "COT_PROMPT = \"\"\"Solve this step by step:\n",
    "1. Understand what the problem is asking for\n",
    "2. Extract the relevant information and variables\n",
    "3. Choose the appropriate mathematical operations\n",
    "4. Perform the calculations step by step\n",
    "5. Make sure your solution answers the question\n",
    "6. Double-check your work\n",
    "\n",
    "Format your answer like this:\n",
    "Step 1: [First reasoning step with clear explanation]\n",
    "Step 2: [Second reasoning step with calculations shown]\n",
    "...\n",
    "Final Answer: [The answer with units if applicable]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_answer(answer_text):\n",
    "    \"\"\"Extract the final numerical answer from text.\"\"\"\n",
    "    # First try to find explicit \"Final Answer: X\" pattern\n",
    "    final_answer_match = re.search(r\"Final Answer:\\s*([\\d\\.\\$]+)\", answer_text, re.IGNORECASE)\n",
    "    if final_answer_match:\n",
    "        return final_answer_match.group(1).strip().strip('$')\n",
    "    \n",
    "    # Try other common patterns\n",
    "    patterns = [\n",
    "        r\"The answer is\\s*[-]?\\s*\\$?\\s*([\\d,\\.]+)\",\n",
    "        r\"Therefore,?\\s.*?[-]?\\s*\\$?\\s*([\\d,\\.]+)\",\n",
    "        r\"So,?\\s.*?[-]?\\s*\\$?\\s*([\\d,\\.]+)\",\n",
    "        r\"Thus,?\\s.*?[-]?\\s*\\$?\\s*([\\d,\\.]+)\",\n",
    "        r\".*?([\\d,\\.]+)$\"  # Last number in text as fallback\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.search(pattern, answer_text, re.DOTALL | re.IGNORECASE)\n",
    "        if matches:\n",
    "            return matches.group(1).strip().replace(',', '')\n",
    "    \n",
    "    # Extract the last number if nothing else works\n",
    "    numbers = re.findall(r\"\\d+(?:,\\d+)*(?:\\.\\d+)?\", answer_text)\n",
    "    if numbers:\n",
    "        return numbers[-1].strip().replace(',', '')\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def extract_cot_steps(answer_text):\n",
    "    \"\"\"Extract structured reasoning steps from text.\"\"\"\n",
    "    # Look for step patterns\n",
    "    step_pattern = r\"Step\\s*(\\d+):\\s*(.*?)(?=Step\\s*\\d+:|Final Answer:|$)\"\n",
    "    steps = re.findall(step_pattern, answer_text, re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    # If we found structured steps, use them\n",
    "    if steps:\n",
    "        return [step[1].strip() for step in steps]\n",
    "    \n",
    "    # Otherwise, try to split by lines and process\n",
    "    lines = answer_text.split('\\n')\n",
    "    cleaned_steps = []\n",
    "    \n",
    "    current_step = \"\"\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        # Skip empty lines or final answer lines\n",
    "        if not line or re.search(r\"(answer|final).*?is\", line, re.IGNORECASE):\n",
    "            continue\n",
    "        \n",
    "        # Check if this is a new step indicator\n",
    "        new_step_match = re.match(r'(\\d+\\)|\\(\\d+\\)|[\\d]+\\.)', line)\n",
    "        if new_step_match and current_step:\n",
    "            cleaned_steps.append(current_step)\n",
    "            current_step = line\n",
    "        else:\n",
    "            # Look for lines with mathematical operations or reasoning words\n",
    "            if re.search(r'[+\\-*/=]|because|means|so|thus|therefore', line, re.IGNORECASE):\n",
    "                # Clean the line\n",
    "                clean_line = re.sub(r'<<.*?>>', '', line).strip()\n",
    "                if clean_line:\n",
    "                    if current_step:\n",
    "                        current_step += \" \" + clean_line\n",
    "                    else:\n",
    "                        current_step = clean_line\n",
    "    \n",
    "    # Add the last step if there is one\n",
    "    if current_step:\n",
    "        cleaned_steps.append(current_step)\n",
    "    \n",
    "    return cleaned_steps if cleaned_steps else [answer_text.strip()]\n",
    "\n",
    "def test_cot_steps():\n",
    "    test_cases = [\n",
    "        \"\"\"Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
    "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
    "The answer is 72.\"\"\",\n",
    "        \n",
    "        \"\"\"Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\n",
    "Working 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\n",
    "The answer is 10.\"\"\"\n",
    "    ]\n",
    "    \n",
    "    for case in test_cases:\n",
    "        print(\"Original:\")\n",
    "        print(case)\n",
    "        print(\"\\nExtracted Steps:\")\n",
    "        print(extract_cot_steps(case))\n",
    "        print(\"\\n---\\n\")\n",
    "\n",
    "# Safe device transfer function for M1 MPS\n",
    "def to_device(tensor_or_module):\n",
    "    \"\"\"Safely move tensors or modules to the selected device\"\"\"\n",
    "    if tensor_or_module is None:\n",
    "        return None\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                         \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    \n",
    "    try:\n",
    "        return tensor_or_module.to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not move to {device}: {e}\")\n",
    "        return tensor_or_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
      "The answer is 72.\n",
      "\n",
      "Extracted Steps:\n",
      "['Natalia sold 48/2 = 24 clips in May. Natalia sold 48+24 = 72 clips altogether in April and May.']\n",
      "\n",
      "---\n",
      "\n",
      "Original:\n",
      "Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\n",
      "Working 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\n",
      "The answer is 10.\n",
      "\n",
      "Extracted Steps:\n",
      "['Weng earns 12/60 = $0.2 per minute. Working 50 minutes, she earned 0.2 x 50 = $10.']\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_cot_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSM8KDataset(Dataset):\n",
    "    def __init__(self, split=\"train\", tokenizer=None, max_length=768, max_samples=None):\n",
    "        self.data = load_dataset(\"gsm8k\", \"main\")[split]\n",
    "        if max_samples:\n",
    "            self.data = self.data.select(range(min(max_samples, len(self.data))))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.processed_data = self.preprocess_data()\n",
    "        \n",
    "    def preprocess_data(self):\n",
    "        processed = []\n",
    "        for item in tqdm(self.data, desc=\"Preprocessing data\"):\n",
    "            question = item[\"question\"]\n",
    "            answer_with_cot = item[\"answer\"]\n",
    "            \n",
    "            # Extract the CoT steps and the final answer\n",
    "            final_answer = extract_final_answer(answer_with_cot)\n",
    "            cot_steps = extract_cot_steps(answer_with_cot)\n",
    "            \n",
    "            # Reformat the CoT steps in a more structured way\n",
    "            formatted_cot = \"\"\n",
    "            for i, step in enumerate(cot_steps, 1):\n",
    "                formatted_cot += f\"Step {i}: {step}\\n\"\n",
    "            formatted_cot += f\"Final Answer: {final_answer}\"\n",
    "            \n",
    "            # Format question with improved CoT prompt\n",
    "            formatted_question = f\"{question}\\n\\n{COT_PROMPT}\"\n",
    "            \n",
    "            processed.append({\n",
    "                \"question\": question,\n",
    "                \"formatted_question\": formatted_question,\n",
    "                \"formatted_cot\": formatted_cot,\n",
    "                \"cot_steps\": cot_steps,\n",
    "                \"final_answer\": final_answer,\n",
    "                \"full_answer\": answer_with_cot\n",
    "            })\n",
    "        return processed\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.processed_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.processed_data[idx]\n",
    "        \n",
    "        if self.tokenizer:\n",
    "            input_text = item[\"formatted_question\"]\n",
    "            target_text = item[\"formatted_cot\"]\n",
    "            \n",
    "            try:\n",
    "                inputs = self.tokenizer(\n",
    "                    input_text,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=self.max_length // 3,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                targets = self.tokenizer(\n",
    "                    target_text,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=self.max_length * 2 // 3,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                return {\n",
    "                    \"input_ids\": inputs.input_ids.squeeze(),\n",
    "                    \"attention_mask\": inputs.attention_mask.squeeze(),\n",
    "                    \"labels\": targets.input_ids.squeeze(),\n",
    "                    \"raw_question\": item[\"question\"],\n",
    "                    \"raw_cot\": item[\"formatted_cot\"],\n",
    "                    \"raw_answer\": item[\"final_answer\"]\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"Error tokenizing item {idx}: {e}\")\n",
    "                # Return a fallback\n",
    "                dummy_tensor = torch.zeros(self.max_length, dtype=torch.long)\n",
    "                return {\n",
    "                    \"input_ids\": dummy_tensor,\n",
    "                    \"attention_mask\": dummy_tensor,\n",
    "                    \"labels\": dummy_tensor,\n",
    "                    \"raw_question\": item[\"question\"],\n",
    "                    \"raw_cot\": \"\",\n",
    "                    \"raw_answer\": \"\"\n",
    "                }\n",
    "        else:\n",
    "            return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectionModule(nn.Module):\n",
    "    def __init__(self, base_model_name=\"distilroberta-base\", device=None):\n",
    "        super(ReflectionModule, self).__init__()\n",
    "        \n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                                      \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        # Load base transformer model\n",
    "        self.encoder = AutoModel.from_pretrained(base_model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "        \n",
    "        # Define evaluation heads\n",
    "        self.coherence_head = nn.Sequential(\n",
    "            nn.Linear(self.encoder.config.hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.progress_head = nn.Sequential(\n",
    "            nn.Linear(self.encoder.config.hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.consistency_head = nn.Sequential(\n",
    "            nn.Linear(self.encoder.config.hidden_size * 2, 256),  # Takes concatenated embeddings\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.math_patterns = {\n",
    "            'equation': r'(\\d+\\s*[\\+\\-\\*\\/]\\s*\\d+\\s*=\\s*\\d+)',\n",
    "            'numerical_equality': r'(\\d+\\s*=\\s*\\d+)',\n",
    "            'variable_assignment': r'([a-zA-Z]\\s*=\\s*\\d+)',\n",
    "            'arithmetic_operation': r'(\\d+\\s*[\\+\\-\\*\\/]\\s*\\d+)'\n",
    "        }\n",
    "        \n",
    "        self.to(self.device)\n",
    "        \n",
    "    def _encode_text(self, text):\n",
    "        # Tokenize and encode text\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", \n",
    "                              padding=True, truncation=True, max_length=512).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.encoder(**inputs)\n",
    "            # Use [CLS] token embedding as the representation\n",
    "            return outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    def evaluate_step(self, step, question, previous_steps=None):\n",
    "        # Encode current step\n",
    "        step_embedding = self._encode_text(step)\n",
    "        \n",
    "        coherence_score = self.coherence_head(step_embedding).item()\n",
    "        \n",
    "        question_embedding = self._encode_text(question)\n",
    "        concat_embedding = torch.cat([question_embedding, step_embedding], dim=1)\n",
    "        progress_score = self.progress_head(step_embedding).item()\n",
    "        \n",
    "        consistency_score = 1.0\n",
    "        \n",
    "        if previous_steps and len(previous_steps) > 0:\n",
    "            prev_text = \" \".join(previous_steps)\n",
    "            prev_embedding = self._encode_text(prev_text)\n",
    "            \n",
    "            concat_embedding = torch.cat([prev_embedding, step_embedding], dim=1)\n",
    "            consistency_score = self.consistency_head(concat_embedding).item()\n",
    "        \n",
    "        math_validity = self._check_math_validity(step)\n",
    "        \n",
    "        process_reward = (coherence_score * 0.3 + \n",
    "                          consistency_score * 0.3 + \n",
    "                          progress_score * 0.3 +\n",
    "                          math_validity * 0.1)\n",
    "        \n",
    "        return {\n",
    "            \"coherence\": coherence_score,\n",
    "            \"consistency\": consistency_score,\n",
    "            \"progress\": progress_score,\n",
    "            \"math_validity\": math_validity,\n",
    "            \"process_reward\": process_reward\n",
    "        }\n",
    "    \n",
    "    def _check_math_validity(self, step):\n",
    "        \"\"\"Check if mathematical expressions in the step are valid\"\"\"\n",
    "        expressions = []\n",
    "        for pattern_name, pattern in self.math_patterns.items():\n",
    "            matches = re.findall(pattern, step)\n",
    "            expressions.extend(matches)\n",
    "        \n",
    "        if not expressions:\n",
    "            return 0.5  # Default value if not valid\n",
    "        \n",
    "        valid_count = 0\n",
    "        for expr in expressions:\n",
    "            try:\n",
    "                eval_expr = expr.replace(\"=\", \"==\")\n",
    "                if eval(eval_expr):\n",
    "                    valid_count += 1\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return valid_count / len(expressions) if expressions else 0.5\n",
    "    \n",
    "    def refine_step(self, question, current_step, previous_steps=None):\n",
    "        \"\"\"Use the reflection module to refine a step\"\"\"\n",
    "        eval_results = self.evaluate_step(current_step, question, previous_steps)\n",
    "        \n",
    "        # threshold\n",
    "        if eval_results[\"process_reward\"] > 0.8:\n",
    "            return current_step\n",
    "        \n",
    "        fixed_step = current_step\n",
    "        \n",
    "        if eval_results[\"math_validity\"] < 0.7:\n",
    "            fixed_step = self._fix_math_expressions(fixed_step)\n",
    "        \n",
    "        if previous_steps and eval_results[\"consistency\"] < 0.7:\n",
    "            # Extract any variables or values from previous steps\n",
    "            context = self._extract_context_from_steps(previous_steps)\n",
    "            fixed_step = self._ensure_consistency(fixed_step, context)\n",
    "        \n",
    "        return fixed_step\n",
    "    \n",
    "    def _fix_math_expressions(self, step):\n",
    "        \"\"\"Attempt to fix mathematical expressions in a step\"\"\"\n",
    "        # Implement simple fixes for common math errors, think of more\n",
    "        \n",
    "        step = re.sub(r'(\\d+)([+\\-*/])(\\d+)', r'\\1 \\2 \\3', step)\n",
    "        \n",
    "        # Fix incorrect equals signs (= vs ==)\n",
    "        step = re.sub(r'(\\d+)\\s*==\\s*(\\d+)', r'\\1 = \\2', step)\n",
    "        \n",
    "        return step\n",
    "    \n",
    "    def _extract_context_from_steps(self, steps):\n",
    "        \"\"\"Extract variables and their values from previous steps\"\"\"\n",
    "        context = {}\n",
    "        for step in steps:\n",
    "            var_matches = re.findall(r'([a-zA-Z])\\s*=\\s*(\\d+(?:\\.\\d+)?)', step)\n",
    "            for var, value in var_matches:\n",
    "                context[var] = float(value)\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def _ensure_consistency(self, step, context):\n",
    "        \"\"\"Ensure step is consistent with the extracted context\"\"\"\n",
    "        for var, value in context.items():\n",
    "            pattern = rf'\\b{var}\\b(?!\\s*=)'\n",
    "            step = re.sub(pattern, str(value), step)\n",
    "        \n",
    "        return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalModule:\n",
    "    \"\"\"\n",
    "    Maintains a memory bank of high-quality exemplar CoT sequences and retrieves similar exemplars for any input prompt.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.memory_bank = []\n",
    "        self.embeddings = None\n",
    "        self.index = None\n",
    "        \n",
    "    def add_exemplar(self, question, cot_steps, final_answer, quality_score=1.0):\n",
    "        \"\"\"Add a new exemplar to the memory bank\"\"\"\n",
    "        exemplar = {\n",
    "            \"question\": question,\n",
    "            \"cot_steps\": cot_steps,\n",
    "            \"final_answer\": final_answer,\n",
    "            \"quality_score\": quality_score\n",
    "        }\n",
    "        self.memory_bank.append(exemplar)\n",
    "        \n",
    "        # Rebuild index if we have enough examples\n",
    "        if len(self.memory_bank) % 100 == 0:\n",
    "            self._build_index()\n",
    "    \n",
    "    def add_batch_exemplars(self, exemplars):\n",
    "        \"\"\"Add multiple exemplars at once\"\"\"\n",
    "        self.memory_bank.extend(exemplars)\n",
    "        self._build_index()\n",
    "    \n",
    "    def _build_index(self):\n",
    "        \"\"\"Build a FAISS index for fast similarity search\"\"\"\n",
    "        if not self.memory_bank:\n",
    "            return\n",
    "        \n",
    "        # Create embeddings for all questions\n",
    "        questions = [item[\"question\"] for item in self.memory_bank]\n",
    "        self.embeddings = self.embedding_model.encode(questions)\n",
    "        \n",
    "        # Convert to numpy array with correct dtype\n",
    "        embeddings_np = np.array(self.embeddings).astype('float32')\n",
    "        \n",
    "        # Build FAISS index\n",
    "        dimension = embeddings_np.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dimension)\n",
    "        self.index.add(embeddings_np)\n",
    "        \n",
    "        print(f\"Built index with {len(self.memory_bank)} exemplars\")\n",
    "    \n",
    "    def retrieve_similar(self, question, top_k=5):\n",
    "        \"\"\"Retrieve most similar exemplars for a given question\"\"\"\n",
    "        if not self.memory_bank or self.index is None:\n",
    "            return []\n",
    "        \n",
    "        # Encode the question\n",
    "        question_embedding = self.embedding_model.encode([question])\n",
    "        question_embedding = np.array(question_embedding).astype('float32')\n",
    "        \n",
    "        # Search for similar questions\n",
    "        distances, indices = self.index.search(question_embedding, min(top_k, len(self.memory_bank)))\n",
    "        \n",
    "        # Return the corresponding exemplars\n",
    "        similar_exemplars = [self.memory_bank[idx] for idx in indices[0]]\n",
    "        \n",
    "        return similar_exemplars\n",
    "    \n",
    "    def save_memory_bank(self, file_path):\n",
    "        \"\"\"Save the memory bank to a file\"\"\"\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(self.memory_bank, f)\n",
    "    \n",
    "    def load_memory_bank(self, file_path):\n",
    "        \"\"\"Load the memory bank from a file\"\"\"\n",
    "        with open(file_path, 'r') as f:\n",
    "            self.memory_bank = json.load(f)\n",
    "        self._build_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoTLossCalculator:\n",
    "    \"\"\"\n",
    "    Calculates various loss functions for training the CoT generator\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, reflection_module=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.reflection_module = reflection_module\n",
    "        \n",
    "        # Weights for combining loss components\n",
    "        self.weights = {\n",
    "            \"nll_loss\": 1.0,\n",
    "            \"consistency_loss\": 0.3,\n",
    "            \"step_quality_loss\": 0.5,\n",
    "            \"answer_correctness_loss\": 1.0\n",
    "        }\n",
    "    \n",
    "    def calculate_nll_loss(self, logits, labels):\n",
    "        \"\"\"Standard negative log-likelihood loss\"\"\"\n",
    "        # Shift logits and labels for next token prediction\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        \n",
    "        # Calculate cross entropy loss\n",
    "        loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        nll_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        \n",
    "        return nll_loss\n",
    "    \n",
    "    def calculate_consistency_loss(self, generated_steps, question):\n",
    "        \"\"\"Loss based on consistency between steps\"\"\"\n",
    "        if not self.reflection_module or not generated_steps:\n",
    "            return torch.tensor(0.0)\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        for i in range(1, len(generated_steps)):\n",
    "            eval_result = self.reflection_module.evaluate_step(\n",
    "                generated_steps[i], question, generated_steps[:i]\n",
    "            )\n",
    "            step_loss = 1.0 - eval_result[\"consistency\"]\n",
    "            total_loss += step_loss\n",
    "        \n",
    "        # Average over all steps\n",
    "        return torch.tensor(total_loss / len(generated_steps) if generated_steps else 0.0)\n",
    "    \n",
    "    def calculate_step_quality_loss(self, generated_steps, question):\n",
    "        \"\"\"Loss based on the quality of each step\"\"\"\n",
    "        if not self.reflection_module or not generated_steps:\n",
    "            return torch.tensor(0.0)\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        for step in generated_steps:\n",
    "            eval_result = self.reflection_module.evaluate_step(step, question)\n",
    "            step_loss = 1.0 - eval_result[\"process_reward\"]\n",
    "            total_loss += step_loss\n",
    "        \n",
    "        # Average over all steps\n",
    "        return torch.tensor(total_loss / len(generated_steps) if generated_steps else 0.0)\n",
    "    \n",
    "    def calculate_answer_correctness_loss(self, predicted_answer, correct_answer):\n",
    "        \"\"\"Loss based on correctness of the final answer\"\"\"\n",
    "        if predicted_answer == correct_answer:\n",
    "            return torch.tensor(0.0)\n",
    "        else:\n",
    "            return torch.tensor(1.0)\n",
    "    \n",
    "    def calculate_combined_loss(self, outputs, labels, generated_steps=None, \n",
    "                               question=None, predicted_answer=None, correct_answer=None):\n",
    "        \"\"\"Combine multiple loss functions\"\"\"\n",
    "        # Calculate NLL loss\n",
    "        nll_loss = self.calculate_nll_loss(outputs.logits, labels)\n",
    "        losses = {\"nll_loss\": nll_loss}\n",
    "        \n",
    "        # Calculate consistency loss if data is available\n",
    "        if generated_steps and question:\n",
    "            consistency_loss = self.calculate_consistency_loss(generated_steps, question)\n",
    "            losses[\"consistency_loss\"] = consistency_loss\n",
    "        else:\n",
    "            losses[\"consistency_loss\"] = torch.tensor(0.0)\n",
    "        \n",
    "        # Calculate step quality loss if data is available\n",
    "        if generated_steps and question:\n",
    "            step_quality_loss = self.calculate_step_quality_loss(generated_steps, question)\n",
    "            losses[\"step_quality_loss\"] = step_quality_loss\n",
    "        else:\n",
    "            losses[\"step_quality_loss\"] = torch.tensor(0.0)\n",
    "        \n",
    "        # Calculate answer correctness loss if data is available\n",
    "        if predicted_answer is not None and correct_answer is not None:\n",
    "            answer_loss = self.calculate_answer_correctness_loss(predicted_answer, correct_answer)\n",
    "            losses[\"answer_correctness_loss\"] = answer_loss\n",
    "        else:\n",
    "            losses[\"answer_correctness_loss\"] = torch.tensor(0.0)\n",
    "        \n",
    "        # Combine all losses with their weights\n",
    "        combined_loss = sum(self.weights[key] * losses[key] for key in losses)\n",
    "        \n",
    "        return combined_loss, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedCoTGenerator(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \n",
    "                 local_dir=\"./models/tinyllama_cache\",\n",
    "                 max_steps=8,\n",
    "                 reflection_module=None,\n",
    "                 retrieval_module=None,\n",
    "                 device=None,\n",
    "                 load_in_8bit=False,\n",
    "                 hf_token=None):\n",
    "        super(EnhancedCoTGenerator, self).__init__()\n",
    "        \n",
    "        self.max_steps = max_steps\n",
    "        self.model_name = model_name\n",
    "        self.local_dir = local_dir\n",
    "        self.load_in_8bit = load_in_8bit\n",
    "        self.hf_token = hf_token\n",
    "        \n",
    "        # Initialize reflection and retrieval modules\n",
    "        self.reflection_module = reflection_module\n",
    "        self.retrieval_module = retrieval_module\n",
    "        \n",
    "        # Set device\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                                       \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = device\n",
    "        \n",
    "        # Create directory if needed\n",
    "        os.makedirs(self.local_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"Loading model {model_name}...\")\n",
    "        \n",
    "        # Authenticate with Hugging Face if token is provided\n",
    "        if self.hf_token:\n",
    "            try:\n",
    "                from huggingface_hub import login\n",
    "                login(token=self.hf_token)\n",
    "                print(\"Successfully logged in to Hugging Face!\")\n",
    "            except Exception as e:\n",
    "                print(f\"Authentication failed: {e}\")\n",
    "                print(\"Will try to continue, but may fail if model requires authentication.\")\n",
    "        \n",
    "        # Check for local model first\n",
    "        if os.path.exists(os.path.join(self.local_dir, \"pytorch_model.bin\")) and \\\n",
    "           os.path.exists(os.path.join(self.local_dir, \"tokenizer_config.json\")):\n",
    "            print(f\"Found existing model at {self.local_dir}. Loading locally...\")\n",
    "            self._load_local_model()\n",
    "        else:\n",
    "            print(f\"Model not found locally. Downloading {model_name}...\")\n",
    "            self._download_model()\n",
    "    \n",
    "    def _download_model(self):\n",
    "        try:\n",
    "            # Add token to kwargs if available\n",
    "            token_kwargs = {\"token\": self.hf_token} if self.hf_token else {}\n",
    "            \n",
    "            # Download tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_name,\n",
    "                cache_dir=self.local_dir,\n",
    "                use_fast=True,\n",
    "                **token_kwargs\n",
    "            )\n",
    "            \n",
    "            # Make sure padding token is set\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "                \n",
    "            print(f\"Tokenizer downloaded and saved to {self.local_dir}\")\n",
    "            \n",
    "            # Download model with appropriate settings for device\n",
    "            model_kwargs = {\n",
    "                \"cache_dir\": self.local_dir,\n",
    "                \"low_cpu_mem_usage\": True,\n",
    "                **token_kwargs  # Include token if available\n",
    "            }\n",
    "            \n",
    "            if self.load_in_8bit and self.device.type == \"cuda\":  # Only use 8-bit for CUDA\n",
    "                model_kwargs[\"load_in_8bit\"] = True\n",
    "                print(\"Loading model in 8-bit quantization\")\n",
    "            elif self.device.type == \"mps\" or self.device.type == \"cuda\":\n",
    "                model_kwargs[\"torch_dtype\"] = torch.float16\n",
    "                print(f\"Loading model in float16 on {self.device.type}\")\n",
    "            else:\n",
    "                print(\"Loading model in default precision on CPU\")\n",
    "            \n",
    "            try:\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    **model_kwargs\n",
    "                )\n",
    "                if not self.load_in_8bit:  # Only move to device if not 8-bit (8-bit handles device placement)\n",
    "                    self.model = self.model.to(self.device)\n",
    "                print(f\"Model downloaded and moved to {self.device}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model to device: {e}\")\n",
    "                print(\"Falling back to CPU with reduced precision\")\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    cache_dir=self.local_dir,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                    **token_kwargs\n",
    "                )\n",
    "                print(f\"Model downloaded (CPU version)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading model: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def _load_local_model(self):\n",
    "        try:\n",
    "            # Load tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.local_dir)\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            # Load model with appropriate settings for device\n",
    "            model_kwargs = {\n",
    "                \"low_cpu_mem_usage\": True,\n",
    "            }\n",
    "            \n",
    "            if self.load_in_8bit and self.device.type == \"cuda\":  # Only use 8-bit for CUDA\n",
    "                from transformers import BitsAndBytesConfig\n",
    "                model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(load_in_8bit=True)\n",
    "                print(\"Loading model in 8-bit quantization\")\n",
    "            elif self.device.type == \"mps\" or self.device.type == \"cuda\":\n",
    "                model_kwargs[\"torch_dtype\"] = torch.float16\n",
    "                print(f\"Loading model in float16 on {self.device.type}\")\n",
    "            else:\n",
    "                print(\"Loading model in default precision on CPU\")\n",
    "            \n",
    "            try:\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.local_dir, \n",
    "                    **model_kwargs\n",
    "                )\n",
    "                if not self.load_in_8bit:  # Only move to device if not 8-bit\n",
    "                    self.model = self.model.to(self.device)\n",
    "                print(f\"Model loaded from {self.local_dir} and moved to {self.device}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model to device: {e}\")\n",
    "                print(\"Falling back to CPU with reduced precision\")\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.local_dir,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    low_cpu_mem_usage=True\n",
    "                )\n",
    "                print(f\"Model loaded from {self.local_dir} (CPU version)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading local model: {e}\")\n",
    "            print(\"Will attempt to download from source...\")\n",
    "            self._download_model()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        \"\"\"Forward pass for training\"\"\"\n",
    "        return self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "    \n",
    "    def _enhance_prompt_with_examples(self, question):\n",
    "        \"\"\"Enhance the prompt with similar examples from retrieval module\"\"\"\n",
    "        if not self.retrieval_module:\n",
    "            return question\n",
    "        \n",
    "        # Retrieve similar examples\n",
    "        similar_examples = self.retrieval_module.retrieve_similar(question, top_k=2)\n",
    "        \n",
    "        if not similar_examples:\n",
    "            return question\n",
    "        \n",
    "        # Format examples as part of the prompt\n",
    "        enhanced_prompt = \"Here are some examples of how to solve similar math problems:\\n\\n\"\n",
    "        \n",
    "        for i, example in enumerate(similar_examples, 1):\n",
    "            enhanced_prompt += f\"Example {i}:\\n\"\n",
    "            enhanced_prompt += f\"Question: {example['question']}\\n\"\n",
    "            \n",
    "            # Format the chain of thought steps\n",
    "            for j, step in enumerate(example['cot_steps'], 1):\n",
    "                enhanced_prompt += f\"Step {j}: {step}\\n\"\n",
    "            \n",
    "            enhanced_prompt += f\"Final Answer: {example['final_answer']}\\n\\n\"\n",
    "        \n",
    "        # Add the original question\n",
    "        enhanced_prompt += f\"Now solve this problem:\\n{question}\"\n",
    "        \n",
    "        return enhanced_prompt\n",
    "    \n",
    "    def _refine_steps_with_reflection(self, steps, question, final_answer=None):\n",
    "        \"\"\"Refine generated steps using the reflection module\"\"\"\n",
    "        if not self.reflection_module:\n",
    "            return steps\n",
    "        \n",
    "        refined_steps = []\n",
    "        previous_steps = []\n",
    "        \n",
    "        for i, step in enumerate(steps):\n",
    "            # Refine each step based on previous steps and question\n",
    "            refined_step = self.reflection_module.refine_step(\n",
    "                question=question,\n",
    "                current_step=step,\n",
    "                previous_steps=previous_steps\n",
    "            )\n",
    "            \n",
    "            refined_steps.append(refined_step)\n",
    "            previous_steps.append(refined_step)\n",
    "        \n",
    "        return refined_steps\n",
    "    \n",
    "    def generate(self, question, use_reflection=True, use_retrieval=True):\n",
    "        \"\"\"Generate a complete chain of thought reasoning for a math problem\"\"\"\n",
    "        try:\n",
    "            # 1. Use retrieval module to enhance prompt if available\n",
    "            if use_retrieval and self.retrieval_module:\n",
    "                enhanced_question = self._enhance_prompt_with_examples(question)\n",
    "            else:\n",
    "                enhanced_question = question\n",
    "            \n",
    "            # 2. Create a structured prompt for TinyLlama model format\n",
    "            # TinyLlama chat format: <|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\n",
    "            prompt = f\"<|im_start|>user\\nPlease solve this math problem step by step.\\n\\n{enhanced_question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "            \n",
    "            # 3. Encode input\n",
    "            inputs = self.tokenizer(\n",
    "                prompt, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # 4. Generate initial output\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=inputs.input_ids,\n",
    "                    attention_mask=inputs.attention_mask,\n",
    "                    max_length=inputs.input_ids.shape[1] + 512,  # Allow reasonable length for math reasoning\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.95,\n",
    "                    top_k=40,\n",
    "                    repetition_penalty=1.1,\n",
    "                    num_return_sequences=1,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # 5. Decode output - only decode the new tokens, not the input prompt\n",
    "            input_length = inputs.input_ids.shape[1]\n",
    "            generated_ids = outputs[0][input_length:]\n",
    "            generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "            \n",
    "            # 6. Process the generated text to extract steps and final answer\n",
    "            cot_steps = self._extract_cot_steps(generated_text)\n",
    "            final_answer = self._extract_final_answer(generated_text)\n",
    "            \n",
    "            # 7. Use reflection module to refine steps if available\n",
    "            if use_reflection and self.reflection_module and cot_steps:\n",
    "                refined_steps = self._refine_steps_with_reflection(\n",
    "                    cot_steps, \n",
    "                    question, \n",
    "                    final_answer\n",
    "                )\n",
    "                cot_steps = refined_steps\n",
    "            \n",
    "            # 8. Format the output consistently\n",
    "            formatted_output = \"\"\n",
    "            for i, step in enumerate(cot_steps, 1):\n",
    "                formatted_output += f\"Step {i}: {step}\\n\"\n",
    "            \n",
    "            if final_answer:\n",
    "                formatted_output += f\"Final Answer: {final_answer}\"\n",
    "            else:\n",
    "                # If no final answer was extracted, regenerate it from steps\n",
    "                regenerated_answer = self._regenerate_final_answer(cot_steps, question)\n",
    "                formatted_output += f\"Final Answer: {regenerated_answer}\"\n",
    "                final_answer = regenerated_answer\n",
    "            \n",
    "            # 9. Create result dictionary\n",
    "            result = {\n",
    "                \"question\": question,\n",
    "                \"cot_steps\": cot_steps,\n",
    "                \"final_answer\": final_answer,\n",
    "                \"full_output\": formatted_output\n",
    "            }\n",
    "            \n",
    "            # 10. Store in retrieval module if available\n",
    "            if self.retrieval_module:\n",
    "                # Add to retrieval module\n",
    "                self.retrieval_module.add_exemplar(\n",
    "                    question=question,\n",
    "                    cot_steps=cot_steps,\n",
    "                    final_answer=final_answer,\n",
    "                    quality_score=1.0  # Default score\n",
    "                )\n",
    "                \n",
    "            return result\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating solution: {e}\")\n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"cot_steps\": [\"Error generating steps\"],\n",
    "                \"final_answer\": \"Error\",\n",
    "                \"full_output\": f\"Error: {str(e)}\"\n",
    "            }\n",
    "    \n",
    "    def _extract_cot_steps(self, text):\n",
    "        \"\"\"Extract chain of thought steps from generated text\"\"\"\n",
    "        # Pattern matching for steps\n",
    "        step_pattern = r\"Step\\s*\\d+:?\\s*(.*?)(?=Step\\s*\\d+:|Final Answer:|$)\"\n",
    "        steps = re.findall(step_pattern, text, re.DOTALL)\n",
    "        \n",
    "        # If no steps are found, try to extract reasoning paragraphs\n",
    "        if not steps:\n",
    "            # Split into lines and look for reasoning\n",
    "            lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "            # Filter out short lines and final answer\n",
    "            steps = [line for line in lines if len(line) > 10 and not line.startswith(\"Final Answer\")]\n",
    "        \n",
    "        # Clean up each step\n",
    "        steps = [step.strip() for step in steps if step.strip()]\n",
    "        \n",
    "        return steps\n",
    "    \n",
    "    def _extract_final_answer(self, text):\n",
    "        \"\"\"Extract final answer from generated text\"\"\"\n",
    "        # Look for explicit \"Final Answer:\" pattern\n",
    "        answer_pattern = r\"Final Answer:?\\s*(.*?)(?=<|$)\"\n",
    "        matches = re.findall(answer_pattern, text, re.DOTALL)\n",
    "        \n",
    "        if matches:\n",
    "            return matches[0].strip()\n",
    "        \n",
    "        # If no explicit final answer, look at the last numerical value\n",
    "        numbers = re.findall(r\"\\d+(?:,\\d+)*(?:\\.\\d+)?\", text)\n",
    "        if numbers:\n",
    "            return numbers[-1].strip()\n",
    "        \n",
    "        # If still no answer, take the last sentence\n",
    "        sentences = text.split('.')\n",
    "        if sentences:\n",
    "            return sentences[-1].strip()\n",
    "        \n",
    "        return \"\"\n",
    "    \n",
    "    def _regenerate_final_answer(self, steps, question):\n",
    "        \"\"\"Regenerate final answer if extraction failed\"\"\"\n",
    "        try:\n",
    "            # Combine steps into a context\n",
    "            context = \"\\n\".join(steps)\n",
    "            \n",
    "            # Create a prompt to regenerate the answer - adjusted for TinyLlama\n",
    "            prompt = f\"<|im_start|>user\\nBased on these steps, what is the final numerical answer to this math problem? Only provide the numerical value.\\n\\nQuestion: {question}\\n\\nSteps:\\n{context}<|im_end|>\\n<|im_start|>assistant\\nThe final answer is \"\n",
    "            \n",
    "            # Encode input\n",
    "            inputs = self.tokenizer(\n",
    "                prompt, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Generate output\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=inputs.input_ids,\n",
    "                    attention_mask=inputs.attention_mask,\n",
    "                    max_length=inputs.input_ids.shape[1] + 50,  # Just enough for the answer\n",
    "                    do_sample=False,  # Deterministic for final answer\n",
    "                    num_return_sequences=1,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode output - only the new tokens\n",
    "            input_length = inputs.input_ids.shape[1]\n",
    "            generated_ids = outputs[0][input_length:]\n",
    "            answer_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "            \n",
    "            # Extract numerical answer\n",
    "            numbers = re.findall(r\"\\d+(?:,\\d+)*(?:\\.\\d+)?\", answer_text)\n",
    "            if numbers:\n",
    "                return numbers[-1].strip().replace(',', '')\n",
    "            \n",
    "            return answer_text.strip()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error regenerating final answer: {e}\")\n",
    "            return \"Error\"\n",
    "    \n",
    "    def train_with_examples(self, train_dataset, val_dataset=None, \n",
    "                            epochs=3, batch_size=2, learning_rate=5e-5):\n",
    "        \"\"\"Train the model with examples - simplified for TinyLlama on M1 Mac\"\"\"\n",
    "        # 1. Setup optimizer with parameter efficient fine-tuning\n",
    "        from peft import get_peft_model, LoraConfig, TaskType\n",
    "        \n",
    "        # Using LoRA for efficient fine-tuning\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            inference_mode=False,\n",
    "            r=4,  # Smaller rank for M1 Mac\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.1,\n",
    "            # Target appropriate modules for TinyLlama\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "        )\n",
    "        \n",
    "        # Convert to PEFT model\n",
    "        self.model = get_peft_model(self.model, peft_config)\n",
    "        \n",
    "        # Only optimize the LoRA parameters to save memory\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # 2. Create dataloaders with smaller batch size for M1 Mac\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        if val_dataset:\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # 3. Training loop\n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            for batch_idx, batch in enumerate(tqdm(train_loader, \n",
    "                                        total=len(train_loader),\n",
    "                                        desc=f\"Epoch {epoch+1}/{epochs} Training\")):\n",
    "                # Move batch to device\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                labels = batch[\"labels\"].to(self.device)\n",
    "                \n",
    "                # Prepare inputs for causal LM training\n",
    "                inputs = {\n",
    "                    \"input_ids\": input_ids,\n",
    "                    \"attention_mask\": attention_mask,\n",
    "                    \"labels\": labels\n",
    "                }\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(**inputs)\n",
    "                loss = outputs.loss\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                # Log every 10 batches - more frequent for visibility\n",
    "                if (batch_idx + 1) % 10 == 0:\n",
    "                    avg_loss = train_loss / (batch_idx + 1)\n",
    "                    print(f\"Batch {batch_idx+1}/{len(train_loader)}, Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            # Calculate average training loss\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "            \n",
    "            # Validation phase\n",
    "            if val_dataset:\n",
    "                self.model.eval()\n",
    "                val_loss = 0.0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                        input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                        attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                        labels = batch[\"labels\"].to(self.device)\n",
    "                        \n",
    "                        outputs = self.model(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            labels=labels\n",
    "                        )\n",
    "                        \n",
    "                        loss = outputs.loss\n",
    "                        val_loss += loss.item()\n",
    "                \n",
    "                avg_val_loss = val_loss / len(val_loader)\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Val Loss: {avg_val_loss:.4f}\")\n",
    "            \n",
    "            # Save model after each epoch\n",
    "            self.save(f\"{self.local_dir}_epoch_{epoch+1}\")\n",
    "    \n",
    "    def save(self, path=None):\n",
    "        \"\"\"Save the model and tokenizer\"\"\"\n",
    "        save_path = path if path else self.local_dir\n",
    "        try:\n",
    "            # For PEFT models, save the adapter\n",
    "            if hasattr(self.model, \"save_pretrained\") and hasattr(self.model, \"config\") and hasattr(self.model.config, \"peft_config_id\"):\n",
    "                self.model.save_pretrained(save_path)\n",
    "                print(f\"PEFT adapter saved to {save_path}\")\n",
    "            else:\n",
    "                # Regular save for non-PEFT models\n",
    "                self.model.save_pretrained(save_path)\n",
    "            \n",
    "            self.tokenizer.save_pretrained(save_path)\n",
    "            print(f\"Model and tokenizer saved to {save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {e}\")\n",
    "    \n",
    "    def load(self, path=None):\n",
    "        \"\"\"Load the model and tokenizer\"\"\"\n",
    "        load_path = path if path else self.local_dir\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(load_path)\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "                \n",
    "            # Check if this is a PEFT adapter\n",
    "            from peft import PeftModel, PeftConfig\n",
    "            import os\n",
    "            \n",
    "            # Check if it's a PEFT model by looking for adapter_config.json\n",
    "            if os.path.exists(os.path.join(load_path, \"adapter_config.json\")):\n",
    "                # Need to load base model first, then adapter\n",
    "                peft_config = PeftConfig.from_pretrained(load_path)\n",
    "                base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    peft_config.base_model_name_or_path,\n",
    "                    torch_dtype=torch.float16 if self.device.type != \"cpu\" else torch.float32,\n",
    "                    low_cpu_mem_usage=True\n",
    "                )\n",
    "                self.model = PeftModel.from_pretrained(base_model, load_path)\n",
    "                print(f\"PEFT model loaded from {load_path}\")\n",
    "            else:\n",
    "                # Regular model load\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    load_path,\n",
    "                    torch_dtype=torch.float16 if self.device.type != \"cpu\" else torch.float32,\n",
    "                    low_cpu_mem_usage=True\n",
    "                )\n",
    "            \n",
    "            self.model = self.model.to(self.device)\n",
    "            print(f\"Model loaded from {load_path} and moved to {self.device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our modules\n",
    "def initialize_modules():\n",
    "    # 1. Create Reflection Module\n",
    "    reflection_module = ReflectionModule(base_model_name=\"distilroberta-base\")\n",
    "    \n",
    "    # 2. Create Retrieval Module\n",
    "    retrieval_module = RetrievalModule(embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # 3. Create Enhanced CoT Generator\n",
    "    cot_generator = EnhancedCoTGenerator(\n",
    "        reflection_module=reflection_module,\n",
    "        retrieval_module=retrieval_module,\n",
    "        load_in_8bit=False  # Reduced precision for memory efficiency\n",
    "    )\n",
    "    \n",
    "    return cot_generator, reflection_module, retrieval_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example problems for testing\n",
    "test_problems = [\n",
    "    \"Natalia sold clips in May. If she sold 48/2 clips in May, how many clips did she sell?\",\n",
    "    \"Weng earns money per minute. If she earns 12/60 dollars per minute and works 50 minutes, how much does she earn?\",\n",
    "    \"Betty wants to save $100. Her grandparents gave her $30. She already has $50. How much more does she need to save?\",\n",
    "    \"Maila wants to read a 120-page book. She read 12 pages today and will read half the remaining pages tomorrow. How many pages will she read tomorrow?\"\n",
    "]\n",
    "\n",
    "# Function to test the CoT generator\n",
    "def test_cot_generator(cot_generator, problems):\n",
    "    results = []\n",
    "    \n",
    "    for i, problem in enumerate(problems, 1):\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Problem {i}: {problem}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        result = cot_generator.generate(problem)\n",
    "        \n",
    "        print(\"Chain of Thought Steps:\")\n",
    "        for j, step in enumerate(result[\"cot_steps\"], 1):\n",
    "            print(f\"{j}. {step}\")\n",
    "        print(f\"Final Answer: {result['final_answer']}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing modules...\n",
      "Loading model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "Model not found locally. Downloading TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "Tokenizer downloaded and saved to ./models/tinyllama_cache\n",
      "Loading model in float16 on mps\n",
      "Model downloaded and moved to mps\n",
      "Pre-seeding retrieval module with examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing data: 100%|██████████| 50/50 [00:00<00:00, 7049.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing CoT generator...\n",
      "==================================================\n",
      "Problem 1: Natalia sold clips in May. If she sold 48/2 clips in May, how many clips did she sell?\n",
      "==================================================\n",
      "Chain of Thought Steps:\n",
      "1. Sure, here's the solution:\n",
      "2. - Natalia sold clips in May\n",
      "3. - She sold 48 clips in May\n",
      "4. - This means that she sold a total of 48/2 clips in May\n",
      "5. - Therefore, she sold 24 clips in total\n",
      "Final Answer: 24\n",
      "==================================================\n",
      "==================================================\n",
      "Problem 2: Weng earns money per minute. If she earns 12/60 dollars per minute and works 50 minutes, how much does she earn?\n",
      "==================================================\n",
      "Chain of Thought Steps:\n",
      "1. Convert the percentage to a fraction. The percentage \"per minute\" is represented by the term \"12/60\", so we need to divide it by 60 to get a fraction. So, we have:\n",
      "- Percentage: 12/60 (in decimal form)\n",
      "- Fractional part: 1/60 (in decimal form)\n",
      "- Fraction: 1/60 (in decimal form) / 1 (in decimal form) = 1/60\n",
      "2. Multiply the result by 50 minutes. The time duration of one work hour is 8 hours (or 48 minutes), so we multiply the resulting fraction by 50 minutes to find the amount earned in total:\n",
      "- Total number of minutes: 50 * 8 (the time duration of an hour)\n",
      "- Resulting fraction: 1 / 60 * 50 = 39 / 60\n",
      "- Earned amount in total: 39 / 60 x 50 minutes = $3.90\n",
      "3. Divide the earned amount by 12 to find the minimum wage. The minimum wage for an employee with a 12/60 job is calculated as follows:\n",
      "- Employer's profit: $3.90\n",
      "- Minimum wage: $7.25\n",
      "- Work hours: 50\n",
      "- Total profit: $3.90 + $7.25 x 50 = $11.25\n",
      "\n",
      "Therefore, the minimum wage for Weng is $11.25 per hour.\n",
      "Final Answer: 11.25\n",
      "==================================================\n",
      "==================================================\n",
      "Problem 3: Betty wants to save $100. Her grandparents gave her $30. She already has $50. How much more does she need to save?\n",
      "==================================================\n",
      "Chain of Thought Steps:\n",
      "1. Divide the $50 saved by the $30 given by Betty's grandparents. This is equivalent to dividing $50 by $30 (which equals $1.50). The result is $1.50.\n",
      "2. Multiply $1.50 by $100, which equals $1500. This is the amount of money that Betty needs to save to reach the $100 goal.\n",
      "3. Subtract $1500 from $100 to find the amount of money left over after saving $100. This is equivalent to subtracting $1500 from $100 (which equals $490). The result is $490.\n",
      "\n",
      "Therefore, Betty needs to save $490 more.\n",
      "\n",
      "Note that the above solution assumes that Betty's grandparents' gift was equal to $30 in the first place. However, if they had actually given her $50 or $100 instead, then their original $30 would have been divided differently and the same steps would apply.\n",
      "Final Answer: 30\n",
      "==================================================\n",
      "==================================================\n",
      "Problem 4: Maila wants to read a 120-page book. She read 12 pages today and will read half the remaining pages tomorrow. How many pages will she read tomorrow?\n",
      "==================================================\n",
      "Chain of Thought Steps:\n",
      "1. Please write the answer as an integer (i.e., no decimal places or negative numbers).\n",
      "Final Answer: 60\n",
      "==================================================\n",
      "Done! Results saved to cot_results.json\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing modules...\")\n",
    "cot_generator, reflection_module, retrieval_module = initialize_modules()\n",
    "\n",
    "# Pre-seed the retrieval module with some examples\n",
    "print(\"Pre-seeding retrieval module with examples...\")\n",
    "gsm8k_dataset = GSM8KDataset(split=\"train\", max_samples=50)\n",
    "\n",
    "# Add a few examples to the retrieval module\n",
    "for i, item in enumerate(gsm8k_dataset.processed_data[:10]):\n",
    "    retrieval_module.add_exemplar(\n",
    "        question=item[\"question\"],\n",
    "        cot_steps=item[\"cot_steps\"],\n",
    "        final_answer=item[\"final_answer\"],\n",
    "        quality_score=1.0  # Assuming these are good examples\n",
    "    )\n",
    "\n",
    "print(\"Testing CoT generator...\")\n",
    "results = test_cot_generator(cot_generator, test_problems)\n",
    "\n",
    "# Save results\n",
    "with open(\"cot_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Done! Results saved to cot_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30920,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
